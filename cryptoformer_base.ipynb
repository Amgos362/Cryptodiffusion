{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 0: 데이터 구간 [0:25000]\n",
      "                            open         high          low        close  \\\n",
      "2017-10-06 03:00:00    4923000.0    4985000.0    4923000.0    4971000.0   \n",
      "2017-10-06 04:00:00    4972000.0    4989000.0    4941000.0    4951000.0   \n",
      "2017-10-06 05:00:00    4953000.0    4990000.0    4917000.0    4949000.0   \n",
      "2017-10-06 06:00:00    4948000.0    4975000.0    4928000.0    4964000.0   \n",
      "2017-10-06 07:00:00    4965000.0    4985000.0    4926000.0    4984000.0   \n",
      "...                          ...          ...          ...          ...   \n",
      "2024-12-18 12:00:00  153867008.0  153870000.0  153000000.0  153452000.0   \n",
      "2024-12-18 13:00:00  153451008.0  153452000.0  150348000.0  152200000.0   \n",
      "2024-12-18 14:00:00  152200000.0  152800000.0  151382000.0  151994000.0   \n",
      "2024-12-18 15:00:00  151904992.0  152500000.0  151524000.0  152184992.0   \n",
      "2024-12-18 16:00:00  152186000.0  152696992.0  151999008.0  152559008.0   \n",
      "\n",
      "                          volume         value  William_R           ATR  \\\n",
      "2017-10-06 03:00:00     0.123978  6.153118e+05 -12.087913  5.638502e+04   \n",
      "2017-10-06 04:00:00     0.128259  6.363279e+05 -23.076923  5.578609e+04   \n",
      "2017-10-06 05:00:00     0.113699  5.627034e+05 -24.175825  5.701566e+04   \n",
      "2017-10-06 06:00:00     0.490839  2.433323e+06 -16.022099  5.630025e+04   \n",
      "2017-10-06 07:00:00     0.133811  6.635750e+05  -5.844156  5.649309e+04   \n",
      "...                          ...           ...        ...           ...   \n",
      "2024-12-18 12:00:00   366.390350  5.620806e+10 -89.232971  1.111935e+06   \n",
      "2024-12-18 13:00:00  1059.528442  1.607647e+11 -72.872421  1.254225e+06   \n",
      "2024-12-18 14:00:00   356.374054  5.420075e+10 -70.877563  1.265924e+06   \n",
      "2024-12-18 15:00:00   167.901810  2.553601e+10 -67.498230  1.245215e+06   \n",
      "2024-12-18 16:00:00    53.888309  8.214325e+09 -60.524906  1.206128e+06   \n",
      "\n",
      "                               OBV   Z_Score  ...  BB_close_lower_pct_Bin_118  \\\n",
      "2017-10-06 03:00:00     204.461853  1.398576  ...                       False   \n",
      "2017-10-06 04:00:00     204.333588  0.887563  ...                       False   \n",
      "2017-10-06 05:00:00     204.219894  0.783738  ...                       False   \n",
      "2017-10-06 06:00:00     204.710724  0.994589  ...                       False   \n",
      "2017-10-06 07:00:00     204.844543  1.249369  ...                       False   \n",
      "...                            ...       ...  ...                         ...   \n",
      "2024-12-18 12:00:00  140060.500000 -2.307751  ...                       False   \n",
      "2024-12-18 13:00:00  139000.968750 -2.817141  ...                       False   \n",
      "2024-12-18 14:00:00  138644.593750 -2.377464  ...                       False   \n",
      "2024-12-18 15:00:00  138812.500000 -1.881165  ...                       False   \n",
      "2024-12-18 16:00:00  138866.390625 -1.428235  ...                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_119  BB_close_lower_pct_Bin_120  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_121  BB_close_lower_pct_Bin_122  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_123  BB_close_lower_pct_Bin_124  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_125  BB_close_lower_pct_Bin_126  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_127  \n",
      "2017-10-06 03:00:00                        True  \n",
      "2017-10-06 04:00:00                        True  \n",
      "2017-10-06 05:00:00                        True  \n",
      "2017-10-06 06:00:00                        True  \n",
      "2017-10-06 07:00:00                        True  \n",
      "...                                         ...  \n",
      "2024-12-18 12:00:00                       False  \n",
      "2024-12-18 13:00:00                       False  \n",
      "2024-12-18 14:00:00                       False  \n",
      "2024-12-18 15:00:00                       False  \n",
      "2024-12-18 16:00:00                       False  \n",
      "\n",
      "[63034 rows x 4162 columns]\n",
      "Experiment 0: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.7260, Train Acc: 0.5015 | Val Loss: 0.6938, Val Acc: 0.4891\n",
      "Epoch 2/10 | Train Loss: 0.6965, Train Acc: 0.5092 | Val Loss: 0.6960, Val Acc: 0.5162\n",
      "Epoch 3/10 | Train Loss: 0.6837, Train Acc: 0.5634 | Val Loss: 0.6925, Val Acc: 0.5121\n",
      "Epoch 4/10 | Train Loss: 0.6666, Train Acc: 0.5987 | Val Loss: 0.6966, Val Acc: 0.5170\n",
      "Epoch 5/10 | Train Loss: 0.6469, Train Acc: 0.6303 | Val Loss: 0.7146, Val Acc: 0.5311\n",
      "Epoch 6/10 | Train Loss: 0.6390, Train Acc: 0.6435 | Val Loss: 0.7567, Val Acc: 0.5170\n",
      "Epoch 7/10 | Train Loss: 0.6093, Train Acc: 0.6771 | Val Loss: 0.7968, Val Acc: 0.5323\n",
      "Epoch 8/10 | Train Loss: 0.5956, Train Acc: 0.6917 | Val Loss: 0.8065, Val Acc: 0.5166\n",
      "Epoch 9/10 | Train Loss: 0.5836, Train Acc: 0.7034 | Val Loss: 0.8026, Val Acc: 0.4984\n",
      "Epoch 10/10 | Train Loss: 0.5613, Train Acc: 0.7255 | Val Loss: 0.8600, Val Acc: 0.5133\n",
      "Saved model for experiment 0.\n",
      "Experiment 0: Final Validation Accuracy: 0.5133\n",
      "Experiment 0: Test Accuracy: 0.5291\n",
      "Experiment 1: 데이터 구간 [2500:27500]\n",
      "                            open         high          low        close  \\\n",
      "2017-10-06 03:00:00    4923000.0    4985000.0    4923000.0    4971000.0   \n",
      "2017-10-06 04:00:00    4972000.0    4989000.0    4941000.0    4951000.0   \n",
      "2017-10-06 05:00:00    4953000.0    4990000.0    4917000.0    4949000.0   \n",
      "2017-10-06 06:00:00    4948000.0    4975000.0    4928000.0    4964000.0   \n",
      "2017-10-06 07:00:00    4965000.0    4985000.0    4926000.0    4984000.0   \n",
      "...                          ...          ...          ...          ...   \n",
      "2024-12-18 12:00:00  153867008.0  153870000.0  153000000.0  153452000.0   \n",
      "2024-12-18 13:00:00  153451008.0  153452000.0  150348000.0  152200000.0   \n",
      "2024-12-18 14:00:00  152200000.0  152800000.0  151382000.0  151994000.0   \n",
      "2024-12-18 15:00:00  151904992.0  152500000.0  151524000.0  152184992.0   \n",
      "2024-12-18 16:00:00  152186000.0  152696992.0  151999008.0  152559008.0   \n",
      "\n",
      "                          volume         value  William_R           ATR  \\\n",
      "2017-10-06 03:00:00     0.123978  6.153118e+05 -12.087913  5.638502e+04   \n",
      "2017-10-06 04:00:00     0.128259  6.363279e+05 -23.076923  5.578609e+04   \n",
      "2017-10-06 05:00:00     0.113699  5.627034e+05 -24.175825  5.701566e+04   \n",
      "2017-10-06 06:00:00     0.490839  2.433323e+06 -16.022099  5.630025e+04   \n",
      "2017-10-06 07:00:00     0.133811  6.635750e+05  -5.844156  5.649309e+04   \n",
      "...                          ...           ...        ...           ...   \n",
      "2024-12-18 12:00:00   366.390350  5.620806e+10 -89.232971  1.111935e+06   \n",
      "2024-12-18 13:00:00  1059.528442  1.607647e+11 -72.872421  1.254225e+06   \n",
      "2024-12-18 14:00:00   356.374054  5.420075e+10 -70.877563  1.265924e+06   \n",
      "2024-12-18 15:00:00   167.901810  2.553601e+10 -67.498230  1.245215e+06   \n",
      "2024-12-18 16:00:00    53.888309  8.214325e+09 -60.524906  1.206128e+06   \n",
      "\n",
      "                               OBV   Z_Score  ...  BB_close_lower_pct_Bin_118  \\\n",
      "2017-10-06 03:00:00     204.461853  1.398576  ...                       False   \n",
      "2017-10-06 04:00:00     204.333588  0.887563  ...                       False   \n",
      "2017-10-06 05:00:00     204.219894  0.783738  ...                       False   \n",
      "2017-10-06 06:00:00     204.710724  0.994589  ...                       False   \n",
      "2017-10-06 07:00:00     204.844543  1.249369  ...                       False   \n",
      "...                            ...       ...  ...                         ...   \n",
      "2024-12-18 12:00:00  140060.500000 -2.307751  ...                       False   \n",
      "2024-12-18 13:00:00  139000.968750 -2.817141  ...                       False   \n",
      "2024-12-18 14:00:00  138644.593750 -2.377464  ...                       False   \n",
      "2024-12-18 15:00:00  138812.500000 -1.881165  ...                       False   \n",
      "2024-12-18 16:00:00  138866.390625 -1.428235  ...                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_119  BB_close_lower_pct_Bin_120  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_121  BB_close_lower_pct_Bin_122  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_123  BB_close_lower_pct_Bin_124  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_125  BB_close_lower_pct_Bin_126  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_127  \n",
      "2017-10-06 03:00:00                        True  \n",
      "2017-10-06 04:00:00                        True  \n",
      "2017-10-06 05:00:00                        True  \n",
      "2017-10-06 06:00:00                        True  \n",
      "2017-10-06 07:00:00                        True  \n",
      "...                                         ...  \n",
      "2024-12-18 12:00:00                       False  \n",
      "2024-12-18 13:00:00                       False  \n",
      "2024-12-18 14:00:00                       False  \n",
      "2024-12-18 15:00:00                       False  \n",
      "2024-12-18 16:00:00                       False  \n",
      "\n",
      "[63034 rows x 4162 columns]\n",
      "Loaded model from experiment 0 for fine-tuning.\n",
      "Experiment 1: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.6123, Train Acc: 0.6763 | Val Loss: 0.7642, Val Acc: 0.5323\n",
      "Epoch 2/10 | Train Loss: 0.5959, Train Acc: 0.6944 | Val Loss: 0.7660, Val Acc: 0.5359\n",
      "Epoch 3/10 | Train Loss: 0.5751, Train Acc: 0.7094 | Val Loss: 0.8217, Val Acc: 0.5214\n",
      "Epoch 4/10 | Train Loss: 0.5512, Train Acc: 0.7298 | Val Loss: 0.8471, Val Acc: 0.5162\n",
      "Epoch 5/10 | Train Loss: 0.4940, Train Acc: 0.7728 | Val Loss: 0.9979, Val Acc: 0.5125\n",
      "Epoch 6/10 | Train Loss: 0.4627, Train Acc: 0.7938 | Val Loss: 0.8984, Val Acc: 0.5016\n",
      "Epoch 7/10 | Train Loss: 0.4356, Train Acc: 0.8108 | Val Loss: 1.0866, Val Acc: 0.5299\n",
      "Epoch 8/10 | Train Loss: 0.3887, Train Acc: 0.8395 | Val Loss: 1.2394, Val Acc: 0.5299\n",
      "Epoch 9/10 | Train Loss: 0.3629, Train Acc: 0.8553 | Val Loss: 1.1947, Val Acc: 0.5097\n",
      "Epoch 10/10 | Train Loss: 0.3427, Train Acc: 0.8658 | Val Loss: 1.2744, Val Acc: 0.5065\n",
      "Saved model for experiment 1.\n",
      "Experiment 1: Final Validation Accuracy: 0.5065\n",
      "Experiment 1: Test Accuracy: 0.5061\n",
      "Experiment 2: 데이터 구간 [5000:30000]\n",
      "                            open         high          low        close  \\\n",
      "2017-10-06 03:00:00    4923000.0    4985000.0    4923000.0    4971000.0   \n",
      "2017-10-06 04:00:00    4972000.0    4989000.0    4941000.0    4951000.0   \n",
      "2017-10-06 05:00:00    4953000.0    4990000.0    4917000.0    4949000.0   \n",
      "2017-10-06 06:00:00    4948000.0    4975000.0    4928000.0    4964000.0   \n",
      "2017-10-06 07:00:00    4965000.0    4985000.0    4926000.0    4984000.0   \n",
      "...                          ...          ...          ...          ...   \n",
      "2024-12-18 12:00:00  153867008.0  153870000.0  153000000.0  153452000.0   \n",
      "2024-12-18 13:00:00  153451008.0  153452000.0  150348000.0  152200000.0   \n",
      "2024-12-18 14:00:00  152200000.0  152800000.0  151382000.0  151994000.0   \n",
      "2024-12-18 15:00:00  151904992.0  152500000.0  151524000.0  152184992.0   \n",
      "2024-12-18 16:00:00  152186000.0  152696992.0  151999008.0  152559008.0   \n",
      "\n",
      "                          volume         value  William_R           ATR  \\\n",
      "2017-10-06 03:00:00     0.123978  6.153118e+05 -12.087913  5.638502e+04   \n",
      "2017-10-06 04:00:00     0.128259  6.363279e+05 -23.076923  5.578609e+04   \n",
      "2017-10-06 05:00:00     0.113699  5.627034e+05 -24.175825  5.701566e+04   \n",
      "2017-10-06 06:00:00     0.490839  2.433323e+06 -16.022099  5.630025e+04   \n",
      "2017-10-06 07:00:00     0.133811  6.635750e+05  -5.844156  5.649309e+04   \n",
      "...                          ...           ...        ...           ...   \n",
      "2024-12-18 12:00:00   366.390350  5.620806e+10 -89.232971  1.111935e+06   \n",
      "2024-12-18 13:00:00  1059.528442  1.607647e+11 -72.872421  1.254225e+06   \n",
      "2024-12-18 14:00:00   356.374054  5.420075e+10 -70.877563  1.265924e+06   \n",
      "2024-12-18 15:00:00   167.901810  2.553601e+10 -67.498230  1.245215e+06   \n",
      "2024-12-18 16:00:00    53.888309  8.214325e+09 -60.524906  1.206128e+06   \n",
      "\n",
      "                               OBV   Z_Score  ...  BB_close_lower_pct_Bin_118  \\\n",
      "2017-10-06 03:00:00     204.461853  1.398576  ...                       False   \n",
      "2017-10-06 04:00:00     204.333588  0.887563  ...                       False   \n",
      "2017-10-06 05:00:00     204.219894  0.783738  ...                       False   \n",
      "2017-10-06 06:00:00     204.710724  0.994589  ...                       False   \n",
      "2017-10-06 07:00:00     204.844543  1.249369  ...                       False   \n",
      "...                            ...       ...  ...                         ...   \n",
      "2024-12-18 12:00:00  140060.500000 -2.307751  ...                       False   \n",
      "2024-12-18 13:00:00  139000.968750 -2.817141  ...                       False   \n",
      "2024-12-18 14:00:00  138644.593750 -2.377464  ...                       False   \n",
      "2024-12-18 15:00:00  138812.500000 -1.881165  ...                       False   \n",
      "2024-12-18 16:00:00  138866.390625 -1.428235  ...                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_119  BB_close_lower_pct_Bin_120  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_121  BB_close_lower_pct_Bin_122  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_123  BB_close_lower_pct_Bin_124  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_125  BB_close_lower_pct_Bin_126  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_127  \n",
      "2017-10-06 03:00:00                        True  \n",
      "2017-10-06 04:00:00                        True  \n",
      "2017-10-06 05:00:00                        True  \n",
      "2017-10-06 06:00:00                        True  \n",
      "2017-10-06 07:00:00                        True  \n",
      "...                                         ...  \n",
      "2024-12-18 12:00:00                       False  \n",
      "2024-12-18 13:00:00                       False  \n",
      "2024-12-18 14:00:00                       False  \n",
      "2024-12-18 15:00:00                       False  \n",
      "2024-12-18 16:00:00                       False  \n",
      "\n",
      "[63034 rows x 4162 columns]\n",
      "Loaded model from experiment 1 for fine-tuning.\n",
      "Experiment 2: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.5120, Train Acc: 0.7684 | Val Loss: 0.8869, Val Acc: 0.5109\n",
      "Epoch 2/10 | Train Loss: 0.4881, Train Acc: 0.7841 | Val Loss: 0.9159, Val Acc: 0.5166\n",
      "Epoch 3/10 | Train Loss: 0.4512, Train Acc: 0.8042 | Val Loss: 0.9831, Val Acc: 0.4972\n",
      "Epoch 4/10 | Train Loss: 0.4198, Train Acc: 0.8218 | Val Loss: 1.0614, Val Acc: 0.5081\n",
      "Epoch 5/10 | Train Loss: 0.3410, Train Acc: 0.8668 | Val Loss: 1.1872, Val Acc: 0.5125\n",
      "Epoch 6/10 | Train Loss: 0.2970, Train Acc: 0.8911 | Val Loss: 1.4321, Val Acc: 0.5032\n",
      "Epoch 7/10 | Train Loss: 0.2728, Train Acc: 0.9017 | Val Loss: 1.5087, Val Acc: 0.5044\n",
      "Epoch 8/10 | Train Loss: 0.2140, Train Acc: 0.9280 | Val Loss: 1.6033, Val Acc: 0.5093\n",
      "Epoch 9/10 | Train Loss: 0.1930, Train Acc: 0.9345 | Val Loss: 1.8835, Val Acc: 0.5073\n",
      "Epoch 10/10 | Train Loss: 0.1753, Train Acc: 0.9412 | Val Loss: 2.0673, Val Acc: 0.5008\n",
      "Saved model for experiment 2.\n",
      "Experiment 2: Final Validation Accuracy: 0.5008\n",
      "Experiment 2: Test Accuracy: 0.5089\n",
      "Experiment 3: 데이터 구간 [7500:32500]\n",
      "                            open         high          low        close  \\\n",
      "2017-10-06 03:00:00    4923000.0    4985000.0    4923000.0    4971000.0   \n",
      "2017-10-06 04:00:00    4972000.0    4989000.0    4941000.0    4951000.0   \n",
      "2017-10-06 05:00:00    4953000.0    4990000.0    4917000.0    4949000.0   \n",
      "2017-10-06 06:00:00    4948000.0    4975000.0    4928000.0    4964000.0   \n",
      "2017-10-06 07:00:00    4965000.0    4985000.0    4926000.0    4984000.0   \n",
      "...                          ...          ...          ...          ...   \n",
      "2024-12-18 12:00:00  153867008.0  153870000.0  153000000.0  153452000.0   \n",
      "2024-12-18 13:00:00  153451008.0  153452000.0  150348000.0  152200000.0   \n",
      "2024-12-18 14:00:00  152200000.0  152800000.0  151382000.0  151994000.0   \n",
      "2024-12-18 15:00:00  151904992.0  152500000.0  151524000.0  152184992.0   \n",
      "2024-12-18 16:00:00  152186000.0  152696992.0  151999008.0  152559008.0   \n",
      "\n",
      "                          volume         value  William_R           ATR  \\\n",
      "2017-10-06 03:00:00     0.123978  6.153118e+05 -12.087913  5.638502e+04   \n",
      "2017-10-06 04:00:00     0.128259  6.363279e+05 -23.076923  5.578609e+04   \n",
      "2017-10-06 05:00:00     0.113699  5.627034e+05 -24.175825  5.701566e+04   \n",
      "2017-10-06 06:00:00     0.490839  2.433323e+06 -16.022099  5.630025e+04   \n",
      "2017-10-06 07:00:00     0.133811  6.635750e+05  -5.844156  5.649309e+04   \n",
      "...                          ...           ...        ...           ...   \n",
      "2024-12-18 12:00:00   366.390350  5.620806e+10 -89.232971  1.111935e+06   \n",
      "2024-12-18 13:00:00  1059.528442  1.607647e+11 -72.872421  1.254225e+06   \n",
      "2024-12-18 14:00:00   356.374054  5.420075e+10 -70.877563  1.265924e+06   \n",
      "2024-12-18 15:00:00   167.901810  2.553601e+10 -67.498230  1.245215e+06   \n",
      "2024-12-18 16:00:00    53.888309  8.214325e+09 -60.524906  1.206128e+06   \n",
      "\n",
      "                               OBV   Z_Score  ...  BB_close_lower_pct_Bin_118  \\\n",
      "2017-10-06 03:00:00     204.461853  1.398576  ...                       False   \n",
      "2017-10-06 04:00:00     204.333588  0.887563  ...                       False   \n",
      "2017-10-06 05:00:00     204.219894  0.783738  ...                       False   \n",
      "2017-10-06 06:00:00     204.710724  0.994589  ...                       False   \n",
      "2017-10-06 07:00:00     204.844543  1.249369  ...                       False   \n",
      "...                            ...       ...  ...                         ...   \n",
      "2024-12-18 12:00:00  140060.500000 -2.307751  ...                       False   \n",
      "2024-12-18 13:00:00  139000.968750 -2.817141  ...                       False   \n",
      "2024-12-18 14:00:00  138644.593750 -2.377464  ...                       False   \n",
      "2024-12-18 15:00:00  138812.500000 -1.881165  ...                       False   \n",
      "2024-12-18 16:00:00  138866.390625 -1.428235  ...                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_119  BB_close_lower_pct_Bin_120  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_121  BB_close_lower_pct_Bin_122  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_123  BB_close_lower_pct_Bin_124  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_125  BB_close_lower_pct_Bin_126  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_127  \n",
      "2017-10-06 03:00:00                        True  \n",
      "2017-10-06 04:00:00                        True  \n",
      "2017-10-06 05:00:00                        True  \n",
      "2017-10-06 06:00:00                        True  \n",
      "2017-10-06 07:00:00                        True  \n",
      "...                                         ...  \n",
      "2024-12-18 12:00:00                       False  \n",
      "2024-12-18 13:00:00                       False  \n",
      "2024-12-18 14:00:00                       False  \n",
      "2024-12-18 15:00:00                       False  \n",
      "2024-12-18 16:00:00                       False  \n",
      "\n",
      "[63034 rows x 4162 columns]\n",
      "Loaded model from experiment 2 for fine-tuning.\n",
      "Experiment 3: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.4162, Train Acc: 0.8330 | Val Loss: 1.0067, Val Acc: 0.5069\n",
      "Epoch 2/10 | Train Loss: 0.3833, Train Acc: 0.8491 | Val Loss: 1.1409, Val Acc: 0.5117\n",
      "Epoch 3/10 | Train Loss: 0.3528, Train Acc: 0.8588 | Val Loss: 1.2301, Val Acc: 0.5093\n",
      "Epoch 4/10 | Train Loss: 0.3084, Train Acc: 0.8806 | Val Loss: 1.5718, Val Acc: 0.4976\n",
      "Epoch 5/10 | Train Loss: 0.2149, Train Acc: 0.9203 | Val Loss: 1.7805, Val Acc: 0.4960\n",
      "Epoch 6/10 | Train Loss: 0.1688, Train Acc: 0.9418 | Val Loss: 1.9491, Val Acc: 0.5065\n",
      "Epoch 7/10 | Train Loss: 0.1426, Train Acc: 0.9529 | Val Loss: 2.2023, Val Acc: 0.4976\n",
      "Epoch 8/10 | Train Loss: 0.0838, Train Acc: 0.9740 | Val Loss: 3.0592, Val Acc: 0.5053\n",
      "Epoch 9/10 | Train Loss: 0.0633, Train Acc: 0.9825 | Val Loss: 3.0569, Val Acc: 0.4984\n",
      "Epoch 10/10 | Train Loss: 0.0495, Train Acc: 0.9869 | Val Loss: 3.3977, Val Acc: 0.4988\n",
      "Saved model for experiment 3.\n",
      "Experiment 3: Final Validation Accuracy: 0.4988\n",
      "Experiment 3: Test Accuracy: 0.5129\n",
      "Experiment 4: 데이터 구간 [10000:35000]\n",
      "                            open         high          low        close  \\\n",
      "2017-10-06 03:00:00    4923000.0    4985000.0    4923000.0    4971000.0   \n",
      "2017-10-06 04:00:00    4972000.0    4989000.0    4941000.0    4951000.0   \n",
      "2017-10-06 05:00:00    4953000.0    4990000.0    4917000.0    4949000.0   \n",
      "2017-10-06 06:00:00    4948000.0    4975000.0    4928000.0    4964000.0   \n",
      "2017-10-06 07:00:00    4965000.0    4985000.0    4926000.0    4984000.0   \n",
      "...                          ...          ...          ...          ...   \n",
      "2024-12-18 12:00:00  153867008.0  153870000.0  153000000.0  153452000.0   \n",
      "2024-12-18 13:00:00  153451008.0  153452000.0  150348000.0  152200000.0   \n",
      "2024-12-18 14:00:00  152200000.0  152800000.0  151382000.0  151994000.0   \n",
      "2024-12-18 15:00:00  151904992.0  152500000.0  151524000.0  152184992.0   \n",
      "2024-12-18 16:00:00  152186000.0  152696992.0  151999008.0  152559008.0   \n",
      "\n",
      "                          volume         value  William_R           ATR  \\\n",
      "2017-10-06 03:00:00     0.123978  6.153118e+05 -12.087913  5.638502e+04   \n",
      "2017-10-06 04:00:00     0.128259  6.363279e+05 -23.076923  5.578609e+04   \n",
      "2017-10-06 05:00:00     0.113699  5.627034e+05 -24.175825  5.701566e+04   \n",
      "2017-10-06 06:00:00     0.490839  2.433323e+06 -16.022099  5.630025e+04   \n",
      "2017-10-06 07:00:00     0.133811  6.635750e+05  -5.844156  5.649309e+04   \n",
      "...                          ...           ...        ...           ...   \n",
      "2024-12-18 12:00:00   366.390350  5.620806e+10 -89.232971  1.111935e+06   \n",
      "2024-12-18 13:00:00  1059.528442  1.607647e+11 -72.872421  1.254225e+06   \n",
      "2024-12-18 14:00:00   356.374054  5.420075e+10 -70.877563  1.265924e+06   \n",
      "2024-12-18 15:00:00   167.901810  2.553601e+10 -67.498230  1.245215e+06   \n",
      "2024-12-18 16:00:00    53.888309  8.214325e+09 -60.524906  1.206128e+06   \n",
      "\n",
      "                               OBV   Z_Score  ...  BB_close_lower_pct_Bin_118  \\\n",
      "2017-10-06 03:00:00     204.461853  1.398576  ...                       False   \n",
      "2017-10-06 04:00:00     204.333588  0.887563  ...                       False   \n",
      "2017-10-06 05:00:00     204.219894  0.783738  ...                       False   \n",
      "2017-10-06 06:00:00     204.710724  0.994589  ...                       False   \n",
      "2017-10-06 07:00:00     204.844543  1.249369  ...                       False   \n",
      "...                            ...       ...  ...                         ...   \n",
      "2024-12-18 12:00:00  140060.500000 -2.307751  ...                       False   \n",
      "2024-12-18 13:00:00  139000.968750 -2.817141  ...                       False   \n",
      "2024-12-18 14:00:00  138644.593750 -2.377464  ...                       False   \n",
      "2024-12-18 15:00:00  138812.500000 -1.881165  ...                       False   \n",
      "2024-12-18 16:00:00  138866.390625 -1.428235  ...                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_119  BB_close_lower_pct_Bin_120  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_121  BB_close_lower_pct_Bin_122  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_123  BB_close_lower_pct_Bin_124  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_125  BB_close_lower_pct_Bin_126  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_127  \n",
      "2017-10-06 03:00:00                        True  \n",
      "2017-10-06 04:00:00                        True  \n",
      "2017-10-06 05:00:00                        True  \n",
      "2017-10-06 06:00:00                        True  \n",
      "2017-10-06 07:00:00                        True  \n",
      "...                                         ...  \n",
      "2024-12-18 12:00:00                       False  \n",
      "2024-12-18 13:00:00                       False  \n",
      "2024-12-18 14:00:00                       False  \n",
      "2024-12-18 15:00:00                       False  \n",
      "2024-12-18 16:00:00                       False  \n",
      "\n",
      "[63034 rows x 4162 columns]\n",
      "Loaded model from experiment 3 for fine-tuning.\n",
      "Experiment 4: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.3482, Train Acc: 0.8701 | Val Loss: 1.0787, Val Acc: 0.5008\n",
      "Epoch 2/10 | Train Loss: 0.2755, Train Acc: 0.8927 | Val Loss: 1.2517, Val Acc: 0.5081\n",
      "Epoch 3/10 | Train Loss: 0.2262, Train Acc: 0.9120 | Val Loss: 1.4976, Val Acc: 0.5121\n",
      "Epoch 4/10 | Train Loss: 0.2045, Train Acc: 0.9240 | Val Loss: 1.7117, Val Acc: 0.5089\n",
      "Epoch 5/10 | Train Loss: 0.1233, Train Acc: 0.9574 | Val Loss: 1.6256, Val Acc: 0.5097\n",
      "Epoch 6/10 | Train Loss: 0.0862, Train Acc: 0.9730 | Val Loss: 2.5272, Val Acc: 0.5129\n",
      "Epoch 7/10 | Train Loss: 0.0671, Train Acc: 0.9793 | Val Loss: 2.9885, Val Acc: 0.5133\n",
      "Epoch 8/10 | Train Loss: 0.0278, Train Acc: 0.9926 | Val Loss: 3.6468, Val Acc: 0.5141\n",
      "Epoch 9/10 | Train Loss: 0.0133, Train Acc: 0.9967 | Val Loss: 4.1521, Val Acc: 0.5117\n",
      "Epoch 10/10 | Train Loss: 0.0101, Train Acc: 0.9977 | Val Loss: 4.3364, Val Acc: 0.5182\n",
      "Saved model for experiment 4.\n",
      "Experiment 4: Final Validation Accuracy: 0.5182\n",
      "Experiment 4: Test Accuracy: 0.5036\n",
      "Experiment 5: 데이터 구간 [12500:37500]\n",
      "                            open         high          low        close  \\\n",
      "2017-10-06 03:00:00    4923000.0    4985000.0    4923000.0    4971000.0   \n",
      "2017-10-06 04:00:00    4972000.0    4989000.0    4941000.0    4951000.0   \n",
      "2017-10-06 05:00:00    4953000.0    4990000.0    4917000.0    4949000.0   \n",
      "2017-10-06 06:00:00    4948000.0    4975000.0    4928000.0    4964000.0   \n",
      "2017-10-06 07:00:00    4965000.0    4985000.0    4926000.0    4984000.0   \n",
      "...                          ...          ...          ...          ...   \n",
      "2024-12-18 12:00:00  153867008.0  153870000.0  153000000.0  153452000.0   \n",
      "2024-12-18 13:00:00  153451008.0  153452000.0  150348000.0  152200000.0   \n",
      "2024-12-18 14:00:00  152200000.0  152800000.0  151382000.0  151994000.0   \n",
      "2024-12-18 15:00:00  151904992.0  152500000.0  151524000.0  152184992.0   \n",
      "2024-12-18 16:00:00  152186000.0  152696992.0  151999008.0  152559008.0   \n",
      "\n",
      "                          volume         value  William_R           ATR  \\\n",
      "2017-10-06 03:00:00     0.123978  6.153118e+05 -12.087913  5.638502e+04   \n",
      "2017-10-06 04:00:00     0.128259  6.363279e+05 -23.076923  5.578609e+04   \n",
      "2017-10-06 05:00:00     0.113699  5.627034e+05 -24.175825  5.701566e+04   \n",
      "2017-10-06 06:00:00     0.490839  2.433323e+06 -16.022099  5.630025e+04   \n",
      "2017-10-06 07:00:00     0.133811  6.635750e+05  -5.844156  5.649309e+04   \n",
      "...                          ...           ...        ...           ...   \n",
      "2024-12-18 12:00:00   366.390350  5.620806e+10 -89.232971  1.111935e+06   \n",
      "2024-12-18 13:00:00  1059.528442  1.607647e+11 -72.872421  1.254225e+06   \n",
      "2024-12-18 14:00:00   356.374054  5.420075e+10 -70.877563  1.265924e+06   \n",
      "2024-12-18 15:00:00   167.901810  2.553601e+10 -67.498230  1.245215e+06   \n",
      "2024-12-18 16:00:00    53.888309  8.214325e+09 -60.524906  1.206128e+06   \n",
      "\n",
      "                               OBV   Z_Score  ...  BB_close_lower_pct_Bin_118  \\\n",
      "2017-10-06 03:00:00     204.461853  1.398576  ...                       False   \n",
      "2017-10-06 04:00:00     204.333588  0.887563  ...                       False   \n",
      "2017-10-06 05:00:00     204.219894  0.783738  ...                       False   \n",
      "2017-10-06 06:00:00     204.710724  0.994589  ...                       False   \n",
      "2017-10-06 07:00:00     204.844543  1.249369  ...                       False   \n",
      "...                            ...       ...  ...                         ...   \n",
      "2024-12-18 12:00:00  140060.500000 -2.307751  ...                       False   \n",
      "2024-12-18 13:00:00  139000.968750 -2.817141  ...                       False   \n",
      "2024-12-18 14:00:00  138644.593750 -2.377464  ...                       False   \n",
      "2024-12-18 15:00:00  138812.500000 -1.881165  ...                       False   \n",
      "2024-12-18 16:00:00  138866.390625 -1.428235  ...                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_119  BB_close_lower_pct_Bin_120  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_121  BB_close_lower_pct_Bin_122  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_123  BB_close_lower_pct_Bin_124  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_125  BB_close_lower_pct_Bin_126  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_127  \n",
      "2017-10-06 03:00:00                        True  \n",
      "2017-10-06 04:00:00                        True  \n",
      "2017-10-06 05:00:00                        True  \n",
      "2017-10-06 06:00:00                        True  \n",
      "2017-10-06 07:00:00                        True  \n",
      "...                                         ...  \n",
      "2024-12-18 12:00:00                       False  \n",
      "2024-12-18 13:00:00                       False  \n",
      "2024-12-18 14:00:00                       False  \n",
      "2024-12-18 15:00:00                       False  \n",
      "2024-12-18 16:00:00                       False  \n",
      "\n",
      "[63034 rows x 4162 columns]\n",
      "Loaded model from experiment 4 for fine-tuning.\n",
      "Experiment 5: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.2892, Train Acc: 0.8952 | Val Loss: 1.3493, Val Acc: 0.5065\n",
      "Epoch 2/10 | Train Loss: 0.2303, Train Acc: 0.9093 | Val Loss: 1.4064, Val Acc: 0.5166\n",
      "Epoch 3/10 | Train Loss: 0.1980, Train Acc: 0.9211 | Val Loss: 2.0359, Val Acc: 0.5162\n",
      "Epoch 4/10 | Train Loss: 0.1765, Train Acc: 0.9327 | Val Loss: 1.8674, Val Acc: 0.4992\n",
      "Epoch 5/10 | Train Loss: 0.1018, Train Acc: 0.9650 | Val Loss: 2.2574, Val Acc: 0.5117\n",
      "Epoch 6/10 | Train Loss: 0.0720, Train Acc: 0.9779 | Val Loss: 3.2645, Val Acc: 0.5299\n",
      "Epoch 7/10 | Train Loss: 0.0524, Train Acc: 0.9851 | Val Loss: 3.4498, Val Acc: 0.5238\n",
      "Epoch 8/10 | Train Loss: 0.0212, Train Acc: 0.9943 | Val Loss: 3.8973, Val Acc: 0.5222\n",
      "Epoch 9/10 | Train Loss: 0.0060, Train Acc: 0.9986 | Val Loss: 4.3413, Val Acc: 0.5145\n",
      "Epoch 10/10 | Train Loss: 0.0057, Train Acc: 0.9989 | Val Loss: 4.4645, Val Acc: 0.5202\n",
      "Saved model for experiment 5.\n",
      "Experiment 5: Final Validation Accuracy: 0.5202\n",
      "Experiment 5: Test Accuracy: 0.5016\n",
      "Experiment 6: 데이터 구간 [15000:40000]\n",
      "                            open         high          low        close  \\\n",
      "2017-10-06 03:00:00    4923000.0    4985000.0    4923000.0    4971000.0   \n",
      "2017-10-06 04:00:00    4972000.0    4989000.0    4941000.0    4951000.0   \n",
      "2017-10-06 05:00:00    4953000.0    4990000.0    4917000.0    4949000.0   \n",
      "2017-10-06 06:00:00    4948000.0    4975000.0    4928000.0    4964000.0   \n",
      "2017-10-06 07:00:00    4965000.0    4985000.0    4926000.0    4984000.0   \n",
      "...                          ...          ...          ...          ...   \n",
      "2024-12-18 12:00:00  153867008.0  153870000.0  153000000.0  153452000.0   \n",
      "2024-12-18 13:00:00  153451008.0  153452000.0  150348000.0  152200000.0   \n",
      "2024-12-18 14:00:00  152200000.0  152800000.0  151382000.0  151994000.0   \n",
      "2024-12-18 15:00:00  151904992.0  152500000.0  151524000.0  152184992.0   \n",
      "2024-12-18 16:00:00  152186000.0  152696992.0  151999008.0  152559008.0   \n",
      "\n",
      "                          volume         value  William_R           ATR  \\\n",
      "2017-10-06 03:00:00     0.123978  6.153118e+05 -12.087913  5.638502e+04   \n",
      "2017-10-06 04:00:00     0.128259  6.363279e+05 -23.076923  5.578609e+04   \n",
      "2017-10-06 05:00:00     0.113699  5.627034e+05 -24.175825  5.701566e+04   \n",
      "2017-10-06 06:00:00     0.490839  2.433323e+06 -16.022099  5.630025e+04   \n",
      "2017-10-06 07:00:00     0.133811  6.635750e+05  -5.844156  5.649309e+04   \n",
      "...                          ...           ...        ...           ...   \n",
      "2024-12-18 12:00:00   366.390350  5.620806e+10 -89.232971  1.111935e+06   \n",
      "2024-12-18 13:00:00  1059.528442  1.607647e+11 -72.872421  1.254225e+06   \n",
      "2024-12-18 14:00:00   356.374054  5.420075e+10 -70.877563  1.265924e+06   \n",
      "2024-12-18 15:00:00   167.901810  2.553601e+10 -67.498230  1.245215e+06   \n",
      "2024-12-18 16:00:00    53.888309  8.214325e+09 -60.524906  1.206128e+06   \n",
      "\n",
      "                               OBV   Z_Score  ...  BB_close_lower_pct_Bin_118  \\\n",
      "2017-10-06 03:00:00     204.461853  1.398576  ...                       False   \n",
      "2017-10-06 04:00:00     204.333588  0.887563  ...                       False   \n",
      "2017-10-06 05:00:00     204.219894  0.783738  ...                       False   \n",
      "2017-10-06 06:00:00     204.710724  0.994589  ...                       False   \n",
      "2017-10-06 07:00:00     204.844543  1.249369  ...                       False   \n",
      "...                            ...       ...  ...                         ...   \n",
      "2024-12-18 12:00:00  140060.500000 -2.307751  ...                       False   \n",
      "2024-12-18 13:00:00  139000.968750 -2.817141  ...                       False   \n",
      "2024-12-18 14:00:00  138644.593750 -2.377464  ...                       False   \n",
      "2024-12-18 15:00:00  138812.500000 -1.881165  ...                       False   \n",
      "2024-12-18 16:00:00  138866.390625 -1.428235  ...                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_119  BB_close_lower_pct_Bin_120  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_121  BB_close_lower_pct_Bin_122  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_123  BB_close_lower_pct_Bin_124  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_125  BB_close_lower_pct_Bin_126  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_127  \n",
      "2017-10-06 03:00:00                        True  \n",
      "2017-10-06 04:00:00                        True  \n",
      "2017-10-06 05:00:00                        True  \n",
      "2017-10-06 06:00:00                        True  \n",
      "2017-10-06 07:00:00                        True  \n",
      "...                                         ...  \n",
      "2024-12-18 12:00:00                       False  \n",
      "2024-12-18 13:00:00                       False  \n",
      "2024-12-18 14:00:00                       False  \n",
      "2024-12-18 15:00:00                       False  \n",
      "2024-12-18 16:00:00                       False  \n",
      "\n",
      "[63034 rows x 4162 columns]\n",
      "Loaded model from experiment 5 for fine-tuning.\n",
      "Experiment 6: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.2919, Train Acc: 0.9008 | Val Loss: 1.3479, Val Acc: 0.5158\n",
      "Epoch 2/10 | Train Loss: 0.2343, Train Acc: 0.9154 | Val Loss: 1.6928, Val Acc: 0.5137\n",
      "Epoch 3/10 | Train Loss: 0.1838, Train Acc: 0.9370 | Val Loss: 1.8498, Val Acc: 0.5174\n",
      "Epoch 4/10 | Train Loss: 0.1596, Train Acc: 0.9460 | Val Loss: 2.2218, Val Acc: 0.5170\n",
      "Epoch 5/10 | Train Loss: 0.0944, Train Acc: 0.9715 | Val Loss: 2.6561, Val Acc: 0.5032\n",
      "Epoch 6/10 | Train Loss: 0.0582, Train Acc: 0.9841 | Val Loss: 3.1527, Val Acc: 0.5000\n",
      "Epoch 7/10 | Train Loss: 0.0413, Train Acc: 0.9892 | Val Loss: 3.3230, Val Acc: 0.5117\n",
      "Epoch 8/10 | Train Loss: 0.0158, Train Acc: 0.9958 | Val Loss: 3.9409, Val Acc: 0.5113\n",
      "Epoch 9/10 | Train Loss: 0.0063, Train Acc: 0.9985 | Val Loss: 4.3579, Val Acc: 0.5069\n",
      "Epoch 10/10 | Train Loss: 0.0061, Train Acc: 0.9985 | Val Loss: 4.5114, Val Acc: 0.5032\n",
      "Saved model for experiment 6.\n",
      "Experiment 6: Final Validation Accuracy: 0.5032\n",
      "Experiment 6: Test Accuracy: 0.4911\n",
      "Experiment 7: 데이터 구간 [17500:42500]\n",
      "                            open         high          low        close  \\\n",
      "2017-10-06 03:00:00    4923000.0    4985000.0    4923000.0    4971000.0   \n",
      "2017-10-06 04:00:00    4972000.0    4989000.0    4941000.0    4951000.0   \n",
      "2017-10-06 05:00:00    4953000.0    4990000.0    4917000.0    4949000.0   \n",
      "2017-10-06 06:00:00    4948000.0    4975000.0    4928000.0    4964000.0   \n",
      "2017-10-06 07:00:00    4965000.0    4985000.0    4926000.0    4984000.0   \n",
      "...                          ...          ...          ...          ...   \n",
      "2024-12-18 12:00:00  153867008.0  153870000.0  153000000.0  153452000.0   \n",
      "2024-12-18 13:00:00  153451008.0  153452000.0  150348000.0  152200000.0   \n",
      "2024-12-18 14:00:00  152200000.0  152800000.0  151382000.0  151994000.0   \n",
      "2024-12-18 15:00:00  151904992.0  152500000.0  151524000.0  152184992.0   \n",
      "2024-12-18 16:00:00  152186000.0  152696992.0  151999008.0  152559008.0   \n",
      "\n",
      "                          volume         value  William_R           ATR  \\\n",
      "2017-10-06 03:00:00     0.123978  6.153118e+05 -12.087913  5.638502e+04   \n",
      "2017-10-06 04:00:00     0.128259  6.363279e+05 -23.076923  5.578609e+04   \n",
      "2017-10-06 05:00:00     0.113699  5.627034e+05 -24.175825  5.701566e+04   \n",
      "2017-10-06 06:00:00     0.490839  2.433323e+06 -16.022099  5.630025e+04   \n",
      "2017-10-06 07:00:00     0.133811  6.635750e+05  -5.844156  5.649309e+04   \n",
      "...                          ...           ...        ...           ...   \n",
      "2024-12-18 12:00:00   366.390350  5.620806e+10 -89.232971  1.111935e+06   \n",
      "2024-12-18 13:00:00  1059.528442  1.607647e+11 -72.872421  1.254225e+06   \n",
      "2024-12-18 14:00:00   356.374054  5.420075e+10 -70.877563  1.265924e+06   \n",
      "2024-12-18 15:00:00   167.901810  2.553601e+10 -67.498230  1.245215e+06   \n",
      "2024-12-18 16:00:00    53.888309  8.214325e+09 -60.524906  1.206128e+06   \n",
      "\n",
      "                               OBV   Z_Score  ...  BB_close_lower_pct_Bin_118  \\\n",
      "2017-10-06 03:00:00     204.461853  1.398576  ...                       False   \n",
      "2017-10-06 04:00:00     204.333588  0.887563  ...                       False   \n",
      "2017-10-06 05:00:00     204.219894  0.783738  ...                       False   \n",
      "2017-10-06 06:00:00     204.710724  0.994589  ...                       False   \n",
      "2017-10-06 07:00:00     204.844543  1.249369  ...                       False   \n",
      "...                            ...       ...  ...                         ...   \n",
      "2024-12-18 12:00:00  140060.500000 -2.307751  ...                       False   \n",
      "2024-12-18 13:00:00  139000.968750 -2.817141  ...                       False   \n",
      "2024-12-18 14:00:00  138644.593750 -2.377464  ...                       False   \n",
      "2024-12-18 15:00:00  138812.500000 -1.881165  ...                       False   \n",
      "2024-12-18 16:00:00  138866.390625 -1.428235  ...                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_119  BB_close_lower_pct_Bin_120  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_121  BB_close_lower_pct_Bin_122  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_123  BB_close_lower_pct_Bin_124  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_125  BB_close_lower_pct_Bin_126  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_127  \n",
      "2017-10-06 03:00:00                        True  \n",
      "2017-10-06 04:00:00                        True  \n",
      "2017-10-06 05:00:00                        True  \n",
      "2017-10-06 06:00:00                        True  \n",
      "2017-10-06 07:00:00                        True  \n",
      "...                                         ...  \n",
      "2024-12-18 12:00:00                       False  \n",
      "2024-12-18 13:00:00                       False  \n",
      "2024-12-18 14:00:00                       False  \n",
      "2024-12-18 15:00:00                       False  \n",
      "2024-12-18 16:00:00                       False  \n",
      "\n",
      "[63034 rows x 4162 columns]\n",
      "Loaded model from experiment 6 for fine-tuning.\n",
      "Experiment 7: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.2871, Train Acc: 0.9024 | Val Loss: 1.6744, Val Acc: 0.4855\n",
      "Epoch 2/10 | Train Loss: 0.2119, Train Acc: 0.9211 | Val Loss: 1.9377, Val Acc: 0.5053\n",
      "Epoch 3/10 | Train Loss: 0.1758, Train Acc: 0.9327 | Val Loss: 2.0805, Val Acc: 0.5048\n",
      "Epoch 4/10 | Train Loss: 0.1573, Train Acc: 0.9426 | Val Loss: 2.2688, Val Acc: 0.4956\n",
      "Epoch 5/10 | Train Loss: 0.0814, Train Acc: 0.9733 | Val Loss: 3.2215, Val Acc: 0.5012\n",
      "Epoch 6/10 | Train Loss: 0.0502, Train Acc: 0.9849 | Val Loss: 3.7046, Val Acc: 0.5036\n",
      "Epoch 7/10 | Train Loss: 0.0364, Train Acc: 0.9909 | Val Loss: 3.5704, Val Acc: 0.5065\n",
      "Epoch 8/10 | Train Loss: 0.0112, Train Acc: 0.9974 | Val Loss: 4.0047, Val Acc: 0.4988\n",
      "Epoch 9/10 | Train Loss: 0.0066, Train Acc: 0.9988 | Val Loss: 4.1931, Val Acc: 0.5016\n",
      "Epoch 10/10 | Train Loss: 0.0036, Train Acc: 0.9994 | Val Loss: 4.2868, Val Acc: 0.5044\n",
      "Saved model for experiment 7.\n",
      "Experiment 7: Final Validation Accuracy: 0.5044\n",
      "Experiment 7: Test Accuracy: 0.5040\n",
      "Experiment 8: 데이터 구간 [20000:45000]\n",
      "                            open         high          low        close  \\\n",
      "2017-10-06 03:00:00    4923000.0    4985000.0    4923000.0    4971000.0   \n",
      "2017-10-06 04:00:00    4972000.0    4989000.0    4941000.0    4951000.0   \n",
      "2017-10-06 05:00:00    4953000.0    4990000.0    4917000.0    4949000.0   \n",
      "2017-10-06 06:00:00    4948000.0    4975000.0    4928000.0    4964000.0   \n",
      "2017-10-06 07:00:00    4965000.0    4985000.0    4926000.0    4984000.0   \n",
      "...                          ...          ...          ...          ...   \n",
      "2024-12-18 12:00:00  153867008.0  153870000.0  153000000.0  153452000.0   \n",
      "2024-12-18 13:00:00  153451008.0  153452000.0  150348000.0  152200000.0   \n",
      "2024-12-18 14:00:00  152200000.0  152800000.0  151382000.0  151994000.0   \n",
      "2024-12-18 15:00:00  151904992.0  152500000.0  151524000.0  152184992.0   \n",
      "2024-12-18 16:00:00  152186000.0  152696992.0  151999008.0  152559008.0   \n",
      "\n",
      "                          volume         value  William_R           ATR  \\\n",
      "2017-10-06 03:00:00     0.123978  6.153118e+05 -12.087913  5.638502e+04   \n",
      "2017-10-06 04:00:00     0.128259  6.363279e+05 -23.076923  5.578609e+04   \n",
      "2017-10-06 05:00:00     0.113699  5.627034e+05 -24.175825  5.701566e+04   \n",
      "2017-10-06 06:00:00     0.490839  2.433323e+06 -16.022099  5.630025e+04   \n",
      "2017-10-06 07:00:00     0.133811  6.635750e+05  -5.844156  5.649309e+04   \n",
      "...                          ...           ...        ...           ...   \n",
      "2024-12-18 12:00:00   366.390350  5.620806e+10 -89.232971  1.111935e+06   \n",
      "2024-12-18 13:00:00  1059.528442  1.607647e+11 -72.872421  1.254225e+06   \n",
      "2024-12-18 14:00:00   356.374054  5.420075e+10 -70.877563  1.265924e+06   \n",
      "2024-12-18 15:00:00   167.901810  2.553601e+10 -67.498230  1.245215e+06   \n",
      "2024-12-18 16:00:00    53.888309  8.214325e+09 -60.524906  1.206128e+06   \n",
      "\n",
      "                               OBV   Z_Score  ...  BB_close_lower_pct_Bin_118  \\\n",
      "2017-10-06 03:00:00     204.461853  1.398576  ...                       False   \n",
      "2017-10-06 04:00:00     204.333588  0.887563  ...                       False   \n",
      "2017-10-06 05:00:00     204.219894  0.783738  ...                       False   \n",
      "2017-10-06 06:00:00     204.710724  0.994589  ...                       False   \n",
      "2017-10-06 07:00:00     204.844543  1.249369  ...                       False   \n",
      "...                            ...       ...  ...                         ...   \n",
      "2024-12-18 12:00:00  140060.500000 -2.307751  ...                       False   \n",
      "2024-12-18 13:00:00  139000.968750 -2.817141  ...                       False   \n",
      "2024-12-18 14:00:00  138644.593750 -2.377464  ...                       False   \n",
      "2024-12-18 15:00:00  138812.500000 -1.881165  ...                       False   \n",
      "2024-12-18 16:00:00  138866.390625 -1.428235  ...                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_119  BB_close_lower_pct_Bin_120  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_121  BB_close_lower_pct_Bin_122  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_123  BB_close_lower_pct_Bin_124  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_125  BB_close_lower_pct_Bin_126  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_127  \n",
      "2017-10-06 03:00:00                        True  \n",
      "2017-10-06 04:00:00                        True  \n",
      "2017-10-06 05:00:00                        True  \n",
      "2017-10-06 06:00:00                        True  \n",
      "2017-10-06 07:00:00                        True  \n",
      "...                                         ...  \n",
      "2024-12-18 12:00:00                       False  \n",
      "2024-12-18 13:00:00                       False  \n",
      "2024-12-18 14:00:00                       False  \n",
      "2024-12-18 15:00:00                       False  \n",
      "2024-12-18 16:00:00                       False  \n",
      "\n",
      "[63034 rows x 4162 columns]\n",
      "Loaded model from experiment 7 for fine-tuning.\n",
      "Experiment 8: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.3017, Train Acc: 0.9003 | Val Loss: 1.7336, Val Acc: 0.5109\n",
      "Epoch 2/10 | Train Loss: 0.2310, Train Acc: 0.9194 | Val Loss: 2.0691, Val Acc: 0.5020\n",
      "Epoch 3/10 | Train Loss: 0.1815, Train Acc: 0.9356 | Val Loss: 1.8416, Val Acc: 0.5093\n",
      "Epoch 4/10 | Train Loss: 0.1617, Train Acc: 0.9428 | Val Loss: 1.9279, Val Acc: 0.5089\n",
      "Epoch 5/10 | Train Loss: 0.0973, Train Acc: 0.9697 | Val Loss: 2.9549, Val Acc: 0.5077\n",
      "Epoch 6/10 | Train Loss: 0.0637, Train Acc: 0.9804 | Val Loss: 2.8901, Val Acc: 0.5210\n",
      "Epoch 7/10 | Train Loss: 0.0491, Train Acc: 0.9862 | Val Loss: 3.4318, Val Acc: 0.5048\n",
      "Epoch 8/10 | Train Loss: 0.0230, Train Acc: 0.9943 | Val Loss: 3.4291, Val Acc: 0.5137\n",
      "Epoch 9/10 | Train Loss: 0.0114, Train Acc: 0.9973 | Val Loss: 3.7579, Val Acc: 0.5109\n",
      "Epoch 10/10 | Train Loss: 0.0075, Train Acc: 0.9982 | Val Loss: 3.7917, Val Acc: 0.5149\n",
      "Saved model for experiment 8.\n",
      "Experiment 8: Final Validation Accuracy: 0.5149\n",
      "Experiment 8: Test Accuracy: 0.4988\n",
      "Experiment 9: 데이터 구간 [22500:47500]\n",
      "                            open         high          low        close  \\\n",
      "2017-10-06 03:00:00    4923000.0    4985000.0    4923000.0    4971000.0   \n",
      "2017-10-06 04:00:00    4972000.0    4989000.0    4941000.0    4951000.0   \n",
      "2017-10-06 05:00:00    4953000.0    4990000.0    4917000.0    4949000.0   \n",
      "2017-10-06 06:00:00    4948000.0    4975000.0    4928000.0    4964000.0   \n",
      "2017-10-06 07:00:00    4965000.0    4985000.0    4926000.0    4984000.0   \n",
      "...                          ...          ...          ...          ...   \n",
      "2024-12-18 12:00:00  153867008.0  153870000.0  153000000.0  153452000.0   \n",
      "2024-12-18 13:00:00  153451008.0  153452000.0  150348000.0  152200000.0   \n",
      "2024-12-18 14:00:00  152200000.0  152800000.0  151382000.0  151994000.0   \n",
      "2024-12-18 15:00:00  151904992.0  152500000.0  151524000.0  152184992.0   \n",
      "2024-12-18 16:00:00  152186000.0  152696992.0  151999008.0  152559008.0   \n",
      "\n",
      "                          volume         value  William_R           ATR  \\\n",
      "2017-10-06 03:00:00     0.123978  6.153118e+05 -12.087913  5.638502e+04   \n",
      "2017-10-06 04:00:00     0.128259  6.363279e+05 -23.076923  5.578609e+04   \n",
      "2017-10-06 05:00:00     0.113699  5.627034e+05 -24.175825  5.701566e+04   \n",
      "2017-10-06 06:00:00     0.490839  2.433323e+06 -16.022099  5.630025e+04   \n",
      "2017-10-06 07:00:00     0.133811  6.635750e+05  -5.844156  5.649309e+04   \n",
      "...                          ...           ...        ...           ...   \n",
      "2024-12-18 12:00:00   366.390350  5.620806e+10 -89.232971  1.111935e+06   \n",
      "2024-12-18 13:00:00  1059.528442  1.607647e+11 -72.872421  1.254225e+06   \n",
      "2024-12-18 14:00:00   356.374054  5.420075e+10 -70.877563  1.265924e+06   \n",
      "2024-12-18 15:00:00   167.901810  2.553601e+10 -67.498230  1.245215e+06   \n",
      "2024-12-18 16:00:00    53.888309  8.214325e+09 -60.524906  1.206128e+06   \n",
      "\n",
      "                               OBV   Z_Score  ...  BB_close_lower_pct_Bin_118  \\\n",
      "2017-10-06 03:00:00     204.461853  1.398576  ...                       False   \n",
      "2017-10-06 04:00:00     204.333588  0.887563  ...                       False   \n",
      "2017-10-06 05:00:00     204.219894  0.783738  ...                       False   \n",
      "2017-10-06 06:00:00     204.710724  0.994589  ...                       False   \n",
      "2017-10-06 07:00:00     204.844543  1.249369  ...                       False   \n",
      "...                            ...       ...  ...                         ...   \n",
      "2024-12-18 12:00:00  140060.500000 -2.307751  ...                       False   \n",
      "2024-12-18 13:00:00  139000.968750 -2.817141  ...                       False   \n",
      "2024-12-18 14:00:00  138644.593750 -2.377464  ...                       False   \n",
      "2024-12-18 15:00:00  138812.500000 -1.881165  ...                       False   \n",
      "2024-12-18 16:00:00  138866.390625 -1.428235  ...                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_119  BB_close_lower_pct_Bin_120  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_121  BB_close_lower_pct_Bin_122  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_123  BB_close_lower_pct_Bin_124  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_125  BB_close_lower_pct_Bin_126  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_127  \n",
      "2017-10-06 03:00:00                        True  \n",
      "2017-10-06 04:00:00                        True  \n",
      "2017-10-06 05:00:00                        True  \n",
      "2017-10-06 06:00:00                        True  \n",
      "2017-10-06 07:00:00                        True  \n",
      "...                                         ...  \n",
      "2024-12-18 12:00:00                       False  \n",
      "2024-12-18 13:00:00                       False  \n",
      "2024-12-18 14:00:00                       False  \n",
      "2024-12-18 15:00:00                       False  \n",
      "2024-12-18 16:00:00                       False  \n",
      "\n",
      "[63034 rows x 4162 columns]\n",
      "Loaded model from experiment 8 for fine-tuning.\n",
      "Experiment 9: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.2731, Train Acc: 0.9059 | Val Loss: 1.8086, Val Acc: 0.5162\n",
      "Epoch 2/10 | Train Loss: 0.2181, Train Acc: 0.9224 | Val Loss: 1.7901, Val Acc: 0.5137\n",
      "Epoch 3/10 | Train Loss: 0.1721, Train Acc: 0.9385 | Val Loss: 2.3084, Val Acc: 0.5024\n",
      "Epoch 4/10 | Train Loss: 0.1567, Train Acc: 0.9469 | Val Loss: 1.7541, Val Acc: 0.5061\n",
      "Epoch 5/10 | Train Loss: 0.1333, Train Acc: 0.9534 | Val Loss: 2.1015, Val Acc: 0.5048\n",
      "Epoch 6/10 | Train Loss: 0.1180, Train Acc: 0.9621 | Val Loss: 2.3035, Val Acc: 0.5032\n",
      "Epoch 7/10 | Train Loss: 0.1083, Train Acc: 0.9667 | Val Loss: 2.1382, Val Acc: 0.5081\n",
      "Epoch 8/10 | Train Loss: 0.0514, Train Acc: 0.9851 | Val Loss: 3.0880, Val Acc: 0.5069\n",
      "Epoch 9/10 | Train Loss: 0.0260, Train Acc: 0.9931 | Val Loss: 3.3027, Val Acc: 0.5097\n",
      "Epoch 10/10 | Train Loss: 0.0161, Train Acc: 0.9965 | Val Loss: 3.7295, Val Acc: 0.5004\n",
      "Saved model for experiment 9.\n",
      "Experiment 9: Final Validation Accuracy: 0.5004\n",
      "Experiment 9: Test Accuracy: 0.5073\n",
      "Experiment 10: 데이터 구간 [25000:50000]\n",
      "                            open         high          low        close  \\\n",
      "2017-10-06 03:00:00    4923000.0    4985000.0    4923000.0    4971000.0   \n",
      "2017-10-06 04:00:00    4972000.0    4989000.0    4941000.0    4951000.0   \n",
      "2017-10-06 05:00:00    4953000.0    4990000.0    4917000.0    4949000.0   \n",
      "2017-10-06 06:00:00    4948000.0    4975000.0    4928000.0    4964000.0   \n",
      "2017-10-06 07:00:00    4965000.0    4985000.0    4926000.0    4984000.0   \n",
      "...                          ...          ...          ...          ...   \n",
      "2024-12-18 12:00:00  153867008.0  153870000.0  153000000.0  153452000.0   \n",
      "2024-12-18 13:00:00  153451008.0  153452000.0  150348000.0  152200000.0   \n",
      "2024-12-18 14:00:00  152200000.0  152800000.0  151382000.0  151994000.0   \n",
      "2024-12-18 15:00:00  151904992.0  152500000.0  151524000.0  152184992.0   \n",
      "2024-12-18 16:00:00  152186000.0  152696992.0  151999008.0  152559008.0   \n",
      "\n",
      "                          volume         value  William_R           ATR  \\\n",
      "2017-10-06 03:00:00     0.123978  6.153118e+05 -12.087913  5.638502e+04   \n",
      "2017-10-06 04:00:00     0.128259  6.363279e+05 -23.076923  5.578609e+04   \n",
      "2017-10-06 05:00:00     0.113699  5.627034e+05 -24.175825  5.701566e+04   \n",
      "2017-10-06 06:00:00     0.490839  2.433323e+06 -16.022099  5.630025e+04   \n",
      "2017-10-06 07:00:00     0.133811  6.635750e+05  -5.844156  5.649309e+04   \n",
      "...                          ...           ...        ...           ...   \n",
      "2024-12-18 12:00:00   366.390350  5.620806e+10 -89.232971  1.111935e+06   \n",
      "2024-12-18 13:00:00  1059.528442  1.607647e+11 -72.872421  1.254225e+06   \n",
      "2024-12-18 14:00:00   356.374054  5.420075e+10 -70.877563  1.265924e+06   \n",
      "2024-12-18 15:00:00   167.901810  2.553601e+10 -67.498230  1.245215e+06   \n",
      "2024-12-18 16:00:00    53.888309  8.214325e+09 -60.524906  1.206128e+06   \n",
      "\n",
      "                               OBV   Z_Score  ...  BB_close_lower_pct_Bin_118  \\\n",
      "2017-10-06 03:00:00     204.461853  1.398576  ...                       False   \n",
      "2017-10-06 04:00:00     204.333588  0.887563  ...                       False   \n",
      "2017-10-06 05:00:00     204.219894  0.783738  ...                       False   \n",
      "2017-10-06 06:00:00     204.710724  0.994589  ...                       False   \n",
      "2017-10-06 07:00:00     204.844543  1.249369  ...                       False   \n",
      "...                            ...       ...  ...                         ...   \n",
      "2024-12-18 12:00:00  140060.500000 -2.307751  ...                       False   \n",
      "2024-12-18 13:00:00  139000.968750 -2.817141  ...                       False   \n",
      "2024-12-18 14:00:00  138644.593750 -2.377464  ...                       False   \n",
      "2024-12-18 15:00:00  138812.500000 -1.881165  ...                       False   \n",
      "2024-12-18 16:00:00  138866.390625 -1.428235  ...                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_119  BB_close_lower_pct_Bin_120  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_121  BB_close_lower_pct_Bin_122  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_123  BB_close_lower_pct_Bin_124  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_125  BB_close_lower_pct_Bin_126  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_127  \n",
      "2017-10-06 03:00:00                        True  \n",
      "2017-10-06 04:00:00                        True  \n",
      "2017-10-06 05:00:00                        True  \n",
      "2017-10-06 06:00:00                        True  \n",
      "2017-10-06 07:00:00                        True  \n",
      "...                                         ...  \n",
      "2024-12-18 12:00:00                       False  \n",
      "2024-12-18 13:00:00                       False  \n",
      "2024-12-18 14:00:00                       False  \n",
      "2024-12-18 15:00:00                       False  \n",
      "2024-12-18 16:00:00                       False  \n",
      "\n",
      "[63034 rows x 4162 columns]\n",
      "Loaded model from experiment 9 for fine-tuning.\n",
      "Experiment 10: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.2585, Train Acc: 0.9144 | Val Loss: 2.0238, Val Acc: 0.4964\n",
      "Epoch 2/10 | Train Loss: 0.1842, Train Acc: 0.9337 | Val Loss: 1.8821, Val Acc: 0.5044\n",
      "Epoch 3/10 | Train Loss: 0.1550, Train Acc: 0.9458 | Val Loss: 2.2241, Val Acc: 0.4980\n",
      "Epoch 4/10 | Train Loss: 0.1369, Train Acc: 0.9547 | Val Loss: 2.7700, Val Acc: 0.5057\n",
      "Epoch 5/10 | Train Loss: 0.1228, Train Acc: 0.9604 | Val Loss: 2.4993, Val Acc: 0.5028\n",
      "Epoch 6/10 | Train Loss: 0.0580, Train Acc: 0.9831 | Val Loss: 3.0853, Val Acc: 0.4984\n",
      "Epoch 7/10 | Train Loss: 0.0329, Train Acc: 0.9906 | Val Loss: 3.3720, Val Acc: 0.4984\n",
      "Epoch 8/10 | Train Loss: 0.0224, Train Acc: 0.9949 | Val Loss: 3.3663, Val Acc: 0.4992\n",
      "Epoch 9/10 | Train Loss: 0.0099, Train Acc: 0.9979 | Val Loss: 3.6284, Val Acc: 0.5016\n",
      "Epoch 10/10 | Train Loss: 0.0059, Train Acc: 0.9990 | Val Loss: 3.7026, Val Acc: 0.5028\n",
      "Saved model for experiment 10.\n",
      "Experiment 10: Final Validation Accuracy: 0.5028\n",
      "Experiment 10: Test Accuracy: 0.5004\n",
      "Experiment 11: 데이터 구간 [27500:52500]\n",
      "                            open         high          low        close  \\\n",
      "2017-10-06 03:00:00    4923000.0    4985000.0    4923000.0    4971000.0   \n",
      "2017-10-06 04:00:00    4972000.0    4989000.0    4941000.0    4951000.0   \n",
      "2017-10-06 05:00:00    4953000.0    4990000.0    4917000.0    4949000.0   \n",
      "2017-10-06 06:00:00    4948000.0    4975000.0    4928000.0    4964000.0   \n",
      "2017-10-06 07:00:00    4965000.0    4985000.0    4926000.0    4984000.0   \n",
      "...                          ...          ...          ...          ...   \n",
      "2024-12-18 12:00:00  153867008.0  153870000.0  153000000.0  153452000.0   \n",
      "2024-12-18 13:00:00  153451008.0  153452000.0  150348000.0  152200000.0   \n",
      "2024-12-18 14:00:00  152200000.0  152800000.0  151382000.0  151994000.0   \n",
      "2024-12-18 15:00:00  151904992.0  152500000.0  151524000.0  152184992.0   \n",
      "2024-12-18 16:00:00  152186000.0  152696992.0  151999008.0  152559008.0   \n",
      "\n",
      "                          volume         value  William_R           ATR  \\\n",
      "2017-10-06 03:00:00     0.123978  6.153118e+05 -12.087913  5.638502e+04   \n",
      "2017-10-06 04:00:00     0.128259  6.363279e+05 -23.076923  5.578609e+04   \n",
      "2017-10-06 05:00:00     0.113699  5.627034e+05 -24.175825  5.701566e+04   \n",
      "2017-10-06 06:00:00     0.490839  2.433323e+06 -16.022099  5.630025e+04   \n",
      "2017-10-06 07:00:00     0.133811  6.635750e+05  -5.844156  5.649309e+04   \n",
      "...                          ...           ...        ...           ...   \n",
      "2024-12-18 12:00:00   366.390350  5.620806e+10 -89.232971  1.111935e+06   \n",
      "2024-12-18 13:00:00  1059.528442  1.607647e+11 -72.872421  1.254225e+06   \n",
      "2024-12-18 14:00:00   356.374054  5.420075e+10 -70.877563  1.265924e+06   \n",
      "2024-12-18 15:00:00   167.901810  2.553601e+10 -67.498230  1.245215e+06   \n",
      "2024-12-18 16:00:00    53.888309  8.214325e+09 -60.524906  1.206128e+06   \n",
      "\n",
      "                               OBV   Z_Score  ...  BB_close_lower_pct_Bin_118  \\\n",
      "2017-10-06 03:00:00     204.461853  1.398576  ...                       False   \n",
      "2017-10-06 04:00:00     204.333588  0.887563  ...                       False   \n",
      "2017-10-06 05:00:00     204.219894  0.783738  ...                       False   \n",
      "2017-10-06 06:00:00     204.710724  0.994589  ...                       False   \n",
      "2017-10-06 07:00:00     204.844543  1.249369  ...                       False   \n",
      "...                            ...       ...  ...                         ...   \n",
      "2024-12-18 12:00:00  140060.500000 -2.307751  ...                       False   \n",
      "2024-12-18 13:00:00  139000.968750 -2.817141  ...                       False   \n",
      "2024-12-18 14:00:00  138644.593750 -2.377464  ...                       False   \n",
      "2024-12-18 15:00:00  138812.500000 -1.881165  ...                       False   \n",
      "2024-12-18 16:00:00  138866.390625 -1.428235  ...                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_119  BB_close_lower_pct_Bin_120  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_121  BB_close_lower_pct_Bin_122  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_123  BB_close_lower_pct_Bin_124  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_125  BB_close_lower_pct_Bin_126  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_127  \n",
      "2017-10-06 03:00:00                        True  \n",
      "2017-10-06 04:00:00                        True  \n",
      "2017-10-06 05:00:00                        True  \n",
      "2017-10-06 06:00:00                        True  \n",
      "2017-10-06 07:00:00                        True  \n",
      "...                                         ...  \n",
      "2024-12-18 12:00:00                       False  \n",
      "2024-12-18 13:00:00                       False  \n",
      "2024-12-18 14:00:00                       False  \n",
      "2024-12-18 15:00:00                       False  \n",
      "2024-12-18 16:00:00                       False  \n",
      "\n",
      "[63034 rows x 4162 columns]\n",
      "Loaded model from experiment 10 for fine-tuning.\n",
      "Experiment 11: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.2293, Train Acc: 0.9164 | Val Loss: 2.0988, Val Acc: 0.5081\n",
      "Epoch 2/10 | Train Loss: 0.1773, Train Acc: 0.9285 | Val Loss: 2.2182, Val Acc: 0.4992\n",
      "Epoch 3/10 | Train Loss: 0.1523, Train Acc: 0.9415 | Val Loss: 2.1355, Val Acc: 0.5036\n",
      "Epoch 4/10 | Train Loss: 0.1331, Train Acc: 0.9517 | Val Loss: 2.3064, Val Acc: 0.5048\n",
      "Epoch 5/10 | Train Loss: 0.0741, Train Acc: 0.9749 | Val Loss: 3.1970, Val Acc: 0.5044\n",
      "Epoch 6/10 | Train Loss: 0.0491, Train Acc: 0.9857 | Val Loss: 3.1743, Val Acc: 0.5089\n",
      "Epoch 7/10 | Train Loss: 0.0292, Train Acc: 0.9925 | Val Loss: 3.4707, Val Acc: 0.5012\n",
      "Epoch 8/10 | Train Loss: 0.0121, Train Acc: 0.9972 | Val Loss: 3.8107, Val Acc: 0.5012\n",
      "Epoch 9/10 | Train Loss: 0.0033, Train Acc: 0.9994 | Val Loss: 4.1986, Val Acc: 0.4952\n",
      "Epoch 10/10 | Train Loss: 0.0041, Train Acc: 0.9993 | Val Loss: 4.2134, Val Acc: 0.4980\n",
      "Saved model for experiment 11.\n",
      "Experiment 11: Final Validation Accuracy: 0.4980\n",
      "Experiment 11: Test Accuracy: 0.5073\n",
      "Experiment 12: 데이터 구간 [30000:55000]\n",
      "                            open         high          low        close  \\\n",
      "2017-10-06 03:00:00    4923000.0    4985000.0    4923000.0    4971000.0   \n",
      "2017-10-06 04:00:00    4972000.0    4989000.0    4941000.0    4951000.0   \n",
      "2017-10-06 05:00:00    4953000.0    4990000.0    4917000.0    4949000.0   \n",
      "2017-10-06 06:00:00    4948000.0    4975000.0    4928000.0    4964000.0   \n",
      "2017-10-06 07:00:00    4965000.0    4985000.0    4926000.0    4984000.0   \n",
      "...                          ...          ...          ...          ...   \n",
      "2024-12-18 12:00:00  153867008.0  153870000.0  153000000.0  153452000.0   \n",
      "2024-12-18 13:00:00  153451008.0  153452000.0  150348000.0  152200000.0   \n",
      "2024-12-18 14:00:00  152200000.0  152800000.0  151382000.0  151994000.0   \n",
      "2024-12-18 15:00:00  151904992.0  152500000.0  151524000.0  152184992.0   \n",
      "2024-12-18 16:00:00  152186000.0  152696992.0  151999008.0  152559008.0   \n",
      "\n",
      "                          volume         value  William_R           ATR  \\\n",
      "2017-10-06 03:00:00     0.123978  6.153118e+05 -12.087913  5.638502e+04   \n",
      "2017-10-06 04:00:00     0.128259  6.363279e+05 -23.076923  5.578609e+04   \n",
      "2017-10-06 05:00:00     0.113699  5.627034e+05 -24.175825  5.701566e+04   \n",
      "2017-10-06 06:00:00     0.490839  2.433323e+06 -16.022099  5.630025e+04   \n",
      "2017-10-06 07:00:00     0.133811  6.635750e+05  -5.844156  5.649309e+04   \n",
      "...                          ...           ...        ...           ...   \n",
      "2024-12-18 12:00:00   366.390350  5.620806e+10 -89.232971  1.111935e+06   \n",
      "2024-12-18 13:00:00  1059.528442  1.607647e+11 -72.872421  1.254225e+06   \n",
      "2024-12-18 14:00:00   356.374054  5.420075e+10 -70.877563  1.265924e+06   \n",
      "2024-12-18 15:00:00   167.901810  2.553601e+10 -67.498230  1.245215e+06   \n",
      "2024-12-18 16:00:00    53.888309  8.214325e+09 -60.524906  1.206128e+06   \n",
      "\n",
      "                               OBV   Z_Score  ...  BB_close_lower_pct_Bin_118  \\\n",
      "2017-10-06 03:00:00     204.461853  1.398576  ...                       False   \n",
      "2017-10-06 04:00:00     204.333588  0.887563  ...                       False   \n",
      "2017-10-06 05:00:00     204.219894  0.783738  ...                       False   \n",
      "2017-10-06 06:00:00     204.710724  0.994589  ...                       False   \n",
      "2017-10-06 07:00:00     204.844543  1.249369  ...                       False   \n",
      "...                            ...       ...  ...                         ...   \n",
      "2024-12-18 12:00:00  140060.500000 -2.307751  ...                       False   \n",
      "2024-12-18 13:00:00  139000.968750 -2.817141  ...                       False   \n",
      "2024-12-18 14:00:00  138644.593750 -2.377464  ...                       False   \n",
      "2024-12-18 15:00:00  138812.500000 -1.881165  ...                       False   \n",
      "2024-12-18 16:00:00  138866.390625 -1.428235  ...                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_119  BB_close_lower_pct_Bin_120  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_121  BB_close_lower_pct_Bin_122  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_123  BB_close_lower_pct_Bin_124  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_125  BB_close_lower_pct_Bin_126  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_127  \n",
      "2017-10-06 03:00:00                        True  \n",
      "2017-10-06 04:00:00                        True  \n",
      "2017-10-06 05:00:00                        True  \n",
      "2017-10-06 06:00:00                        True  \n",
      "2017-10-06 07:00:00                        True  \n",
      "...                                         ...  \n",
      "2024-12-18 12:00:00                       False  \n",
      "2024-12-18 13:00:00                       False  \n",
      "2024-12-18 14:00:00                       False  \n",
      "2024-12-18 15:00:00                       False  \n",
      "2024-12-18 16:00:00                       False  \n",
      "\n",
      "[63034 rows x 4162 columns]\n",
      "Loaded model from experiment 11 for fine-tuning.\n",
      "Experiment 12: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.2717, Train Acc: 0.9132 | Val Loss: 1.2281, Val Acc: 0.5174\n",
      "Epoch 2/10 | Train Loss: 0.1970, Train Acc: 0.9348 | Val Loss: 2.1495, Val Acc: 0.5008\n",
      "Epoch 3/10 | Train Loss: 0.1541, Train Acc: 0.9494 | Val Loss: 2.1157, Val Acc: 0.5105\n",
      "Epoch 4/10 | Train Loss: 0.1277, Train Acc: 0.9607 | Val Loss: 2.0969, Val Acc: 0.5024\n",
      "Epoch 5/10 | Train Loss: 0.0659, Train Acc: 0.9816 | Val Loss: 3.0103, Val Acc: 0.5028\n",
      "Epoch 6/10 | Train Loss: 0.0367, Train Acc: 0.9918 | Val Loss: 3.3847, Val Acc: 0.5093\n",
      "Epoch 7/10 | Train Loss: 0.0264, Train Acc: 0.9945 | Val Loss: 3.5159, Val Acc: 0.5040\n",
      "Epoch 8/10 | Train Loss: 0.0098, Train Acc: 0.9979 | Val Loss: 3.9279, Val Acc: 0.5073\n",
      "Epoch 9/10 | Train Loss: 0.0056, Train Acc: 0.9991 | Val Loss: 3.8916, Val Acc: 0.5113\n",
      "Epoch 10/10 | Train Loss: 0.0070, Train Acc: 0.9989 | Val Loss: 3.5352, Val Acc: 0.5129\n",
      "Saved model for experiment 12.\n",
      "Experiment 12: Final Validation Accuracy: 0.5129\n",
      "Experiment 12: Test Accuracy: 0.4891\n",
      "Experiment 13: 데이터 구간 [32500:57500]\n",
      "                            open         high          low        close  \\\n",
      "2017-10-06 03:00:00    4923000.0    4985000.0    4923000.0    4971000.0   \n",
      "2017-10-06 04:00:00    4972000.0    4989000.0    4941000.0    4951000.0   \n",
      "2017-10-06 05:00:00    4953000.0    4990000.0    4917000.0    4949000.0   \n",
      "2017-10-06 06:00:00    4948000.0    4975000.0    4928000.0    4964000.0   \n",
      "2017-10-06 07:00:00    4965000.0    4985000.0    4926000.0    4984000.0   \n",
      "...                          ...          ...          ...          ...   \n",
      "2024-12-18 12:00:00  153867008.0  153870000.0  153000000.0  153452000.0   \n",
      "2024-12-18 13:00:00  153451008.0  153452000.0  150348000.0  152200000.0   \n",
      "2024-12-18 14:00:00  152200000.0  152800000.0  151382000.0  151994000.0   \n",
      "2024-12-18 15:00:00  151904992.0  152500000.0  151524000.0  152184992.0   \n",
      "2024-12-18 16:00:00  152186000.0  152696992.0  151999008.0  152559008.0   \n",
      "\n",
      "                          volume         value  William_R           ATR  \\\n",
      "2017-10-06 03:00:00     0.123978  6.153118e+05 -12.087913  5.638502e+04   \n",
      "2017-10-06 04:00:00     0.128259  6.363279e+05 -23.076923  5.578609e+04   \n",
      "2017-10-06 05:00:00     0.113699  5.627034e+05 -24.175825  5.701566e+04   \n",
      "2017-10-06 06:00:00     0.490839  2.433323e+06 -16.022099  5.630025e+04   \n",
      "2017-10-06 07:00:00     0.133811  6.635750e+05  -5.844156  5.649309e+04   \n",
      "...                          ...           ...        ...           ...   \n",
      "2024-12-18 12:00:00   366.390350  5.620806e+10 -89.232971  1.111935e+06   \n",
      "2024-12-18 13:00:00  1059.528442  1.607647e+11 -72.872421  1.254225e+06   \n",
      "2024-12-18 14:00:00   356.374054  5.420075e+10 -70.877563  1.265924e+06   \n",
      "2024-12-18 15:00:00   167.901810  2.553601e+10 -67.498230  1.245215e+06   \n",
      "2024-12-18 16:00:00    53.888309  8.214325e+09 -60.524906  1.206128e+06   \n",
      "\n",
      "                               OBV   Z_Score  ...  BB_close_lower_pct_Bin_118  \\\n",
      "2017-10-06 03:00:00     204.461853  1.398576  ...                       False   \n",
      "2017-10-06 04:00:00     204.333588  0.887563  ...                       False   \n",
      "2017-10-06 05:00:00     204.219894  0.783738  ...                       False   \n",
      "2017-10-06 06:00:00     204.710724  0.994589  ...                       False   \n",
      "2017-10-06 07:00:00     204.844543  1.249369  ...                       False   \n",
      "...                            ...       ...  ...                         ...   \n",
      "2024-12-18 12:00:00  140060.500000 -2.307751  ...                       False   \n",
      "2024-12-18 13:00:00  139000.968750 -2.817141  ...                       False   \n",
      "2024-12-18 14:00:00  138644.593750 -2.377464  ...                       False   \n",
      "2024-12-18 15:00:00  138812.500000 -1.881165  ...                       False   \n",
      "2024-12-18 16:00:00  138866.390625 -1.428235  ...                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_119  BB_close_lower_pct_Bin_120  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_121  BB_close_lower_pct_Bin_122  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_123  BB_close_lower_pct_Bin_124  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_125  BB_close_lower_pct_Bin_126  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_127  \n",
      "2017-10-06 03:00:00                        True  \n",
      "2017-10-06 04:00:00                        True  \n",
      "2017-10-06 05:00:00                        True  \n",
      "2017-10-06 06:00:00                        True  \n",
      "2017-10-06 07:00:00                        True  \n",
      "...                                         ...  \n",
      "2024-12-18 12:00:00                       False  \n",
      "2024-12-18 13:00:00                       False  \n",
      "2024-12-18 14:00:00                       False  \n",
      "2024-12-18 15:00:00                       False  \n",
      "2024-12-18 16:00:00                       False  \n",
      "\n",
      "[63034 rows x 4162 columns]\n",
      "Loaded model from experiment 12 for fine-tuning.\n",
      "Experiment 13: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.2557, Train Acc: 0.9141 | Val Loss: 1.4121, Val Acc: 0.4923\n",
      "Epoch 2/10 | Train Loss: 0.1870, Train Acc: 0.9374 | Val Loss: 2.0687, Val Acc: 0.4992\n",
      "Epoch 3/10 | Train Loss: 0.1496, Train Acc: 0.9518 | Val Loss: 2.2977, Val Acc: 0.4935\n",
      "Epoch 4/10 | Train Loss: 0.1222, Train Acc: 0.9629 | Val Loss: 1.7865, Val Acc: 0.4988\n",
      "Epoch 5/10 | Train Loss: 0.0637, Train Acc: 0.9829 | Val Loss: 3.1081, Val Acc: 0.5020\n",
      "Epoch 6/10 | Train Loss: 0.0353, Train Acc: 0.9914 | Val Loss: 3.4690, Val Acc: 0.4984\n",
      "Epoch 7/10 | Train Loss: 0.0226, Train Acc: 0.9946 | Val Loss: 3.3609, Val Acc: 0.4875\n",
      "Epoch 8/10 | Train Loss: 0.0086, Train Acc: 0.9981 | Val Loss: 3.8803, Val Acc: 0.4899\n",
      "Epoch 9/10 | Train Loss: 0.0024, Train Acc: 0.9995 | Val Loss: 4.0033, Val Acc: 0.4826\n",
      "Epoch 10/10 | Train Loss: 0.0017, Train Acc: 0.9996 | Val Loss: 4.3116, Val Acc: 0.4887\n",
      "Saved model for experiment 13.\n",
      "Experiment 13: Final Validation Accuracy: 0.4887\n",
      "Experiment 13: Test Accuracy: 0.5117\n",
      "Experiment 14: 데이터 구간 [35000:60000]\n",
      "                            open         high          low        close  \\\n",
      "2017-10-06 03:00:00    4923000.0    4985000.0    4923000.0    4971000.0   \n",
      "2017-10-06 04:00:00    4972000.0    4989000.0    4941000.0    4951000.0   \n",
      "2017-10-06 05:00:00    4953000.0    4990000.0    4917000.0    4949000.0   \n",
      "2017-10-06 06:00:00    4948000.0    4975000.0    4928000.0    4964000.0   \n",
      "2017-10-06 07:00:00    4965000.0    4985000.0    4926000.0    4984000.0   \n",
      "...                          ...          ...          ...          ...   \n",
      "2024-12-18 12:00:00  153867008.0  153870000.0  153000000.0  153452000.0   \n",
      "2024-12-18 13:00:00  153451008.0  153452000.0  150348000.0  152200000.0   \n",
      "2024-12-18 14:00:00  152200000.0  152800000.0  151382000.0  151994000.0   \n",
      "2024-12-18 15:00:00  151904992.0  152500000.0  151524000.0  152184992.0   \n",
      "2024-12-18 16:00:00  152186000.0  152696992.0  151999008.0  152559008.0   \n",
      "\n",
      "                          volume         value  William_R           ATR  \\\n",
      "2017-10-06 03:00:00     0.123978  6.153118e+05 -12.087913  5.638502e+04   \n",
      "2017-10-06 04:00:00     0.128259  6.363279e+05 -23.076923  5.578609e+04   \n",
      "2017-10-06 05:00:00     0.113699  5.627034e+05 -24.175825  5.701566e+04   \n",
      "2017-10-06 06:00:00     0.490839  2.433323e+06 -16.022099  5.630025e+04   \n",
      "2017-10-06 07:00:00     0.133811  6.635750e+05  -5.844156  5.649309e+04   \n",
      "...                          ...           ...        ...           ...   \n",
      "2024-12-18 12:00:00   366.390350  5.620806e+10 -89.232971  1.111935e+06   \n",
      "2024-12-18 13:00:00  1059.528442  1.607647e+11 -72.872421  1.254225e+06   \n",
      "2024-12-18 14:00:00   356.374054  5.420075e+10 -70.877563  1.265924e+06   \n",
      "2024-12-18 15:00:00   167.901810  2.553601e+10 -67.498230  1.245215e+06   \n",
      "2024-12-18 16:00:00    53.888309  8.214325e+09 -60.524906  1.206128e+06   \n",
      "\n",
      "                               OBV   Z_Score  ...  BB_close_lower_pct_Bin_118  \\\n",
      "2017-10-06 03:00:00     204.461853  1.398576  ...                       False   \n",
      "2017-10-06 04:00:00     204.333588  0.887563  ...                       False   \n",
      "2017-10-06 05:00:00     204.219894  0.783738  ...                       False   \n",
      "2017-10-06 06:00:00     204.710724  0.994589  ...                       False   \n",
      "2017-10-06 07:00:00     204.844543  1.249369  ...                       False   \n",
      "...                            ...       ...  ...                         ...   \n",
      "2024-12-18 12:00:00  140060.500000 -2.307751  ...                       False   \n",
      "2024-12-18 13:00:00  139000.968750 -2.817141  ...                       False   \n",
      "2024-12-18 14:00:00  138644.593750 -2.377464  ...                       False   \n",
      "2024-12-18 15:00:00  138812.500000 -1.881165  ...                       False   \n",
      "2024-12-18 16:00:00  138866.390625 -1.428235  ...                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_119  BB_close_lower_pct_Bin_120  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_121  BB_close_lower_pct_Bin_122  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_123  BB_close_lower_pct_Bin_124  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_125  BB_close_lower_pct_Bin_126  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_127  \n",
      "2017-10-06 03:00:00                        True  \n",
      "2017-10-06 04:00:00                        True  \n",
      "2017-10-06 05:00:00                        True  \n",
      "2017-10-06 06:00:00                        True  \n",
      "2017-10-06 07:00:00                        True  \n",
      "...                                         ...  \n",
      "2024-12-18 12:00:00                       False  \n",
      "2024-12-18 13:00:00                       False  \n",
      "2024-12-18 14:00:00                       False  \n",
      "2024-12-18 15:00:00                       False  \n",
      "2024-12-18 16:00:00                       False  \n",
      "\n",
      "[63034 rows x 4162 columns]\n",
      "Loaded model from experiment 13 for fine-tuning.\n",
      "Experiment 14: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.2713, Train Acc: 0.9172 | Val Loss: 1.3128, Val Acc: 0.5198\n",
      "Epoch 2/10 | Train Loss: 0.2061, Train Acc: 0.9368 | Val Loss: 1.5328, Val Acc: 0.5141\n",
      "Epoch 3/10 | Train Loss: 0.1503, Train Acc: 0.9524 | Val Loss: 2.0918, Val Acc: 0.5097\n",
      "Epoch 4/10 | Train Loss: 0.1228, Train Acc: 0.9632 | Val Loss: 1.9606, Val Acc: 0.5044\n",
      "Epoch 5/10 | Train Loss: 0.0609, Train Acc: 0.9846 | Val Loss: 3.1661, Val Acc: 0.5089\n",
      "Epoch 6/10 | Train Loss: 0.0465, Train Acc: 0.9895 | Val Loss: 2.6007, Val Acc: 0.5113\n",
      "Epoch 7/10 | Train Loss: 0.0300, Train Acc: 0.9938 | Val Loss: 3.1066, Val Acc: 0.5048\n",
      "Epoch 8/10 | Train Loss: 0.0123, Train Acc: 0.9979 | Val Loss: 3.4497, Val Acc: 0.5121\n",
      "Epoch 9/10 | Train Loss: 0.0102, Train Acc: 0.9983 | Val Loss: 3.5151, Val Acc: 0.5057\n",
      "Epoch 10/10 | Train Loss: 0.0080, Train Acc: 0.9988 | Val Loss: 3.6060, Val Acc: 0.5061\n",
      "Saved model for experiment 14.\n",
      "Experiment 14: Final Validation Accuracy: 0.5061\n",
      "Experiment 14: Test Accuracy: 0.5162\n",
      "Experiment 15: 데이터 구간 [37500:62500]\n",
      "                            open         high          low        close  \\\n",
      "2017-10-06 03:00:00    4923000.0    4985000.0    4923000.0    4971000.0   \n",
      "2017-10-06 04:00:00    4972000.0    4989000.0    4941000.0    4951000.0   \n",
      "2017-10-06 05:00:00    4953000.0    4990000.0    4917000.0    4949000.0   \n",
      "2017-10-06 06:00:00    4948000.0    4975000.0    4928000.0    4964000.0   \n",
      "2017-10-06 07:00:00    4965000.0    4985000.0    4926000.0    4984000.0   \n",
      "...                          ...          ...          ...          ...   \n",
      "2024-12-18 12:00:00  153867008.0  153870000.0  153000000.0  153452000.0   \n",
      "2024-12-18 13:00:00  153451008.0  153452000.0  150348000.0  152200000.0   \n",
      "2024-12-18 14:00:00  152200000.0  152800000.0  151382000.0  151994000.0   \n",
      "2024-12-18 15:00:00  151904992.0  152500000.0  151524000.0  152184992.0   \n",
      "2024-12-18 16:00:00  152186000.0  152696992.0  151999008.0  152559008.0   \n",
      "\n",
      "                          volume         value  William_R           ATR  \\\n",
      "2017-10-06 03:00:00     0.123978  6.153118e+05 -12.087913  5.638502e+04   \n",
      "2017-10-06 04:00:00     0.128259  6.363279e+05 -23.076923  5.578609e+04   \n",
      "2017-10-06 05:00:00     0.113699  5.627034e+05 -24.175825  5.701566e+04   \n",
      "2017-10-06 06:00:00     0.490839  2.433323e+06 -16.022099  5.630025e+04   \n",
      "2017-10-06 07:00:00     0.133811  6.635750e+05  -5.844156  5.649309e+04   \n",
      "...                          ...           ...        ...           ...   \n",
      "2024-12-18 12:00:00   366.390350  5.620806e+10 -89.232971  1.111935e+06   \n",
      "2024-12-18 13:00:00  1059.528442  1.607647e+11 -72.872421  1.254225e+06   \n",
      "2024-12-18 14:00:00   356.374054  5.420075e+10 -70.877563  1.265924e+06   \n",
      "2024-12-18 15:00:00   167.901810  2.553601e+10 -67.498230  1.245215e+06   \n",
      "2024-12-18 16:00:00    53.888309  8.214325e+09 -60.524906  1.206128e+06   \n",
      "\n",
      "                               OBV   Z_Score  ...  BB_close_lower_pct_Bin_118  \\\n",
      "2017-10-06 03:00:00     204.461853  1.398576  ...                       False   \n",
      "2017-10-06 04:00:00     204.333588  0.887563  ...                       False   \n",
      "2017-10-06 05:00:00     204.219894  0.783738  ...                       False   \n",
      "2017-10-06 06:00:00     204.710724  0.994589  ...                       False   \n",
      "2017-10-06 07:00:00     204.844543  1.249369  ...                       False   \n",
      "...                            ...       ...  ...                         ...   \n",
      "2024-12-18 12:00:00  140060.500000 -2.307751  ...                       False   \n",
      "2024-12-18 13:00:00  139000.968750 -2.817141  ...                       False   \n",
      "2024-12-18 14:00:00  138644.593750 -2.377464  ...                       False   \n",
      "2024-12-18 15:00:00  138812.500000 -1.881165  ...                       False   \n",
      "2024-12-18 16:00:00  138866.390625 -1.428235  ...                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_119  BB_close_lower_pct_Bin_120  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_121  BB_close_lower_pct_Bin_122  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_123  BB_close_lower_pct_Bin_124  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_125  BB_close_lower_pct_Bin_126  \\\n",
      "2017-10-06 03:00:00                       False                       False   \n",
      "2017-10-06 04:00:00                       False                       False   \n",
      "2017-10-06 05:00:00                       False                       False   \n",
      "2017-10-06 06:00:00                       False                       False   \n",
      "2017-10-06 07:00:00                       False                       False   \n",
      "...                                         ...                         ...   \n",
      "2024-12-18 12:00:00                       False                       False   \n",
      "2024-12-18 13:00:00                       False                       False   \n",
      "2024-12-18 14:00:00                       False                       False   \n",
      "2024-12-18 15:00:00                       False                       False   \n",
      "2024-12-18 16:00:00                       False                       False   \n",
      "\n",
      "                     BB_close_lower_pct_Bin_127  \n",
      "2017-10-06 03:00:00                        True  \n",
      "2017-10-06 04:00:00                        True  \n",
      "2017-10-06 05:00:00                        True  \n",
      "2017-10-06 06:00:00                        True  \n",
      "2017-10-06 07:00:00                        True  \n",
      "...                                         ...  \n",
      "2024-12-18 12:00:00                       False  \n",
      "2024-12-18 13:00:00                       False  \n",
      "2024-12-18 14:00:00                       False  \n",
      "2024-12-18 15:00:00                       False  \n",
      "2024-12-18 16:00:00                       False  \n",
      "\n",
      "[63034 rows x 4162 columns]\n",
      "Loaded model from experiment 14 for fine-tuning.\n",
      "Experiment 15: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.2463, Train Acc: 0.9165 | Val Loss: 1.9224, Val Acc: 0.4903\n",
      "Epoch 2/10 | Train Loss: 0.1736, Train Acc: 0.9381 | Val Loss: 2.4500, Val Acc: 0.4964\n",
      "Epoch 3/10 | Train Loss: 0.1407, Train Acc: 0.9547 | Val Loss: 2.3631, Val Acc: 0.5105\n",
      "Epoch 4/10 | Train Loss: 0.1120, Train Acc: 0.9675 | Val Loss: 2.3358, Val Acc: 0.5044\n",
      "Epoch 5/10 | Train Loss: 0.0535, Train Acc: 0.9854 | Val Loss: 2.6492, Val Acc: 0.5089\n",
      "Epoch 6/10 | Train Loss: 0.0288, Train Acc: 0.9938 | Val Loss: 2.8420, Val Acc: 0.5101\n",
      "Epoch 7/10 | Train Loss: 0.0188, Train Acc: 0.9960 | Val Loss: 3.2584, Val Acc: 0.5081\n",
      "Epoch 8/10 | Train Loss: 0.0113, Train Acc: 0.9978 | Val Loss: 3.6948, Val Acc: 0.5125\n",
      "Epoch 9/10 | Train Loss: 0.0036, Train Acc: 0.9995 | Val Loss: 3.6955, Val Acc: 0.5145\n",
      "Epoch 10/10 | Train Loss: 0.0026, Train Acc: 0.9997 | Val Loss: 3.7702, Val Acc: 0.5024\n",
      "Saved model for experiment 15.\n",
      "Experiment 15: Final Validation Accuracy: 0.5024\n",
      "Experiment 15: Test Accuracy: 0.5109\n",
      "\n",
      "Final Average Validation Accuracy: 0.5057\n",
      "Final Average Test Accuracy: 0.5062\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta as ta\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from math import sqrt\n",
    "\n",
    "####################################\n",
    "# 1. 기술적 지표 계산 함수 (변경 없음)\n",
    "####################################\n",
    "def calculate_indicators(data):\n",
    "    data['William_R'] = ta.willr(data['high'], data['low'], data['close'])\n",
    "    data['ATR'] = ta.atr(data['high'], data['low'], data['close'])\n",
    "    data['OBV'] = ta.obv(data['close'], data['volume'])\n",
    "    data['Z_Score'] = (data['close'] - data['close'].rolling(window=20).mean()) / data['close'].rolling(window=20).std()\n",
    "    data['Entropy'] = ta.entropy(data['close'], length=14)\n",
    "    data['SMA_5'] = data['close'].rolling(window=5).mean()\n",
    "    data['SMA_10'] = data['close'].rolling(window=10).mean()\n",
    "    data['SMA_20'] = data['close'].rolling(window=20).mean()\n",
    "    data['SMA_60'] = data['close'].rolling(window=60).mean()\n",
    "    data['SMA_120'] = data['close'].rolling(window=120).mean()\n",
    "    data['SMA_250'] = data['close'].rolling(window=250).mean()\n",
    "    data['RSI'] = ta.rsi(data['close'])\n",
    "    bb = ta.bbands(data['close'])\n",
    "    data['BB_Upper'], data['BB_Middle'], data['BB_Lower'] = bb.iloc[:, 0], bb.iloc[:, 1], bb.iloc[:, 2]\n",
    "    macd = ta.macd(data['close'])\n",
    "    data['MACD'] = macd.iloc[:, 0]\n",
    "    data['Stochastic'] = ta.stoch(data['high'], data['low'], data['close']).iloc[:, 0]\n",
    "    return data.dropna()\n",
    "\n",
    "####################################\n",
    "# 1-2. 추가 feature 계산 (가격 차이)\n",
    "####################################\n",
    "def calculate_price_differences(data):\n",
    "    data['close_open'] = data['close'] - data['open']\n",
    "    data['high_low'] = data['high'] - data['low']\n",
    "    data['high_open'] = data['high'] - data['open']\n",
    "    data['high_close'] = data['high'] - data['close']\n",
    "    data['open_low'] = data['open'] - data['low']\n",
    "    data['close_low'] = data['close'] - data['low']\n",
    "    return data\n",
    "\n",
    "####################################\n",
    "# 2. Datetime Feature One-Hot Encoding (각 feature 128차원)\n",
    "####################################\n",
    "def encode_datetime_features_onehot(data, dim=128):\n",
    "    if 'datetime' not in data.columns:\n",
    "        data['datetime'] = pd.to_datetime(data.index)\n",
    "    \n",
    "    data['hour_of_day'] = data['datetime'].dt.hour\n",
    "    data['day_of_week'] = data['datetime'].dt.dayofweek\n",
    "    data['week_of_month'] = (data['datetime'].dt.day - 1) // 7 + 1\n",
    "    data['month'] = data['datetime'].dt.month\n",
    "\n",
    "    def onehot_with_fixed_dim(series, prefix, dim):\n",
    "        dummies = pd.get_dummies(series, prefix=prefix)\n",
    "        expected_cols = [f\"{prefix}_{i}\" for i in range(dim)]\n",
    "        dummies = dummies.reindex(columns=expected_cols, fill_value=0)\n",
    "        return dummies\n",
    "\n",
    "    hour_one_hot = onehot_with_fixed_dim(data['hour_of_day'], 'Hour', dim)\n",
    "    day_one_hot = onehot_with_fixed_dim(data['day_of_week'], 'Day', dim)\n",
    "    week_one_hot = onehot_with_fixed_dim(data['week_of_month'], 'Week', dim)\n",
    "    month_one_hot = onehot_with_fixed_dim(data['month'], 'Month', dim)\n",
    "    \n",
    "    data = pd.concat([data, hour_one_hot, day_one_hot, week_one_hot, month_one_hot], axis=1)\n",
    "    return data\n",
    "\n",
    "####################################\n",
    "# 3. Rolling MinMax Scaling (분모 0 방지)\n",
    "####################################\n",
    "def rolling_minmax_scale(series, window=24):\n",
    "    roll_min = series.rolling(window=window, min_periods=window).min()\n",
    "    roll_max = series.rolling(window=window, min_periods=window).max()\n",
    "    # 기본 minmax scaling 계산 (분모에 아주 작은 epsilon 추가)\n",
    "    scaled = (series - roll_min) / ((roll_max - roll_min) + 1e-8)\n",
    "    # 무한대나 -무한대 값을 NaN으로 대체하고, NaN은 최대값 1.0으로 대체\n",
    "    scaled = scaled.replace([np.inf, -np.inf], np.nan)\n",
    "    scaled = scaled.fillna(1.0)\n",
    "    # 혹시 1보다 큰 값이 있다면 최대값 1.0으로 클리핑\n",
    "    scaled = scaled.clip(upper=1.0)\n",
    "    return scaled\n",
    "\n",
    "\n",
    "####################################\n",
    "# 4. Binning 후 One-Hot 인코딩 (각 feature를 128차원으로)\n",
    "####################################\n",
    "def bin_and_encode(data, features, bins=128, drop_original=True):\n",
    "    for feature in features:\n",
    "        data[f'{feature}_Bin'] = pd.cut(data[feature], bins=bins, labels=False)\n",
    "        one_hot = pd.get_dummies(data[f'{feature}_Bin'], prefix=f'{feature}_Bin')\n",
    "        expected_columns = [f'{feature}_Bin_{i}' for i in range(bins)]\n",
    "        one_hot = one_hot.reindex(columns=expected_columns, fill_value=0)\n",
    "        data = pd.concat([data, one_hot], axis=1)\n",
    "        if drop_original:\n",
    "            data.drop(columns=[f'{feature}_Bin'], inplace=True)\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    data[numeric_cols] = data[numeric_cols].astype(np.float32)\n",
    "    return data\n",
    "\n",
    "####################################\n",
    "# 5. 데이터 로드 및 전처리\n",
    "####################################\n",
    "data = pd.read_csv(\"BTC_upbit_KRW_min60.csv\", index_col=0)\n",
    "data.columns = ['open', 'high', 'low', 'close', 'volume', 'value']\n",
    "data.index = pd.to_datetime(data.index)\n",
    "\n",
    "# 기본 지표 계산 및 추가 feature 생성\n",
    "data = calculate_indicators(data)\n",
    "data = calculate_price_differences(data)\n",
    "data = encode_datetime_features_onehot(data, dim=128)\n",
    "\n",
    "# 타깃: close 값을 그대로 사용 (continuous)\n",
    "data['close_target'] = data['close']\n",
    "\n",
    "####################################\n",
    "# [Bollinger Bands 관련 새로운 feature 계산]\n",
    "####################################\n",
    "data['BB_diff'] = data['BB_Upper'] - data['BB_Lower']\n",
    "data['BB_close_upper'] = data['close'] - data['BB_Upper']\n",
    "data['BB_close_lower'] = data['close'] - data['BB_Lower']\n",
    "\n",
    "####################################\n",
    "# [인코딩 대상 feature 목록 업데이트]\n",
    "####################################\n",
    "# 원래 feature들을 대상으로 전일 대비 상승률(= pct_change()*100)을 계산\n",
    "features_to_bin = ['open', 'high', 'low', 'volume', 'value', 'William_R',\n",
    "                   'ATR', 'OBV', 'Z_Score', 'Entropy', 'SMA_5', 'SMA_10', \n",
    "                   'SMA_20', 'SMA_60', 'SMA_120', 'SMA_250', 'RSI', 'MACD', 'Stochastic',\n",
    "                   'close_open', 'high_low', 'high_open', 'high_close', 'open_low', 'close_low',\n",
    "                   'BB_diff', 'BB_close_upper', 'BB_close_lower']\n",
    "\n",
    "# 각 feature에 대해 전일 대비 상승률을 계산하고 rolling minmax scaling 적용\n",
    "for feature in features_to_bin:\n",
    "    col_pct = feature + '_pct'\n",
    "    data[col_pct] = data[feature].pct_change() * 100\n",
    "    data[col_pct] = rolling_minmax_scale(data[col_pct], window=24)\n",
    "    \n",
    "# NaN 제거 (pct_change 및 rolling scaling으로 인한)\n",
    "data = data.dropna()\n",
    "\n",
    "# pct_change 결과에 대해 binning과 one-hot 인코딩 수행\n",
    "features_pct = [f + '_pct' for f in features_to_bin]\n",
    "data = bin_and_encode(data, features_pct, bins=128, drop_original=True)\n",
    "\n",
    "# datetime one-hot encoding된 컬럼 추출\n",
    "datetime_onehot_features = [col for col in data.columns if col.startswith('Hour_') or \n",
    "                              col.startswith('Day_') or col.startswith('Week_') or \n",
    "                              col.startswith('Month_')]\n",
    "\n",
    "# 최종 입력 데이터 구성: 각 지표의 상승률에 대한 one-hot 벡터와 datetime one-hot 벡터 결합\n",
    "final_input_columns = []\n",
    "for feature in features_pct:\n",
    "    final_input_columns.extend([f'{feature}_Bin_{i}' for i in range(128)])\n",
    "final_input_columns.extend(datetime_onehot_features)\n",
    "\n",
    "final_target_column = ['close_target']\n",
    "\n",
    "data_input = data[final_input_columns]\n",
    "data_target = data[final_target_column]\n",
    "\n",
    "####################################\n",
    "# 6-2. Dataset 정의 (입력과 타깃을 별도로 사용)\n",
    "####################################\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, input_data, target_data, lookback=24):\n",
    "        self.input_data = input_data.values\n",
    "        self.target_data = target_data.values  # shape: (N, 1)\n",
    "        self.lookback = lookback\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_data) - self.lookback\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.input_data[idx: idx + self.lookback, :]\n",
    "        y = self.target_data[idx + self.lookback, 0]\n",
    "        y_prev = self.target_data[idx + self.lookback - 1, 0]\n",
    "        y_target = 1 if y > y_prev else 0\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y_target, dtype=torch.long)\n",
    "\n",
    "####################################\n",
    "# 7. Transformer Encoder 직접 구현\n",
    "####################################\n",
    "# 7-1. Multi-Head Self-Attention\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == embed_dim, \"embed_dim은 num_heads로 나누어떨어져야 합니다.\"\n",
    "        \n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key   = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out   = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / sqrt(self.head_dim)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, V)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "\n",
    "# 7-2. Feed-Forward Network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ffn_dim):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, ffn_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(ffn_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "# 7-3. Transformer Encoder Layer (Self-Attention + FFN + Residual + LayerNorm)\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = FeedForward(embed_dim, ffn_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attn_out = self.self_attn(x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_out)\n",
    "        return x\n",
    "\n",
    "# 7-4. Encoder-Only Transformer 직접 구현 (lookback=24이므로, max_seq_len=24)\n",
    "class EncoderOnlyTransformerCustom(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=512, num_heads=8, num_layers=6, ffn_dim=2048, num_classes=2, max_seq_len=24):\n",
    "        super(EncoderOnlyTransformerCustom, self).__init__()\n",
    "        self.token_embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embedding_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(embedding_dim, num_heads, ffn_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x = self.token_embedding(x)\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        x = x + self.position_embedding(positions)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x[:, -1, :]\n",
    "        return self.fc(x)\n",
    "\n",
    "####################################\n",
    "# 8. 학습 및 평가 루프 (Fine-tuning 및 Validation Accuracy 출력)\n",
    "####################################\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return total_loss / len(data_loader), correct / total\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, lr, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_state = None\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, device)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = model.state_dict()\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(data, num_experiments=16, lookback=24, num_epochs=10):\n",
    "    # 최종 입력은 각 지표의 상승률(one-hot 인코딩된)과 datetime one-hot 인코딩 결합\n",
    "    input_cols = []\n",
    "    for feature in features_pct:\n",
    "        input_cols.extend([f'{feature}_Bin_{i}' for i in range(128)])\n",
    "    input_cols.extend(datetime_onehot_features)\n",
    "    target_cols = ['close_target']\n",
    "    \n",
    "    data_input = data[input_cols]\n",
    "    data_target = data[target_cols]\n",
    "    \n",
    "    # 여기서 모든 컬럼을 numeric 타입으로 변환\n",
    "    data_input = data_input.apply(pd.to_numeric)\n",
    "    data_input = data_input.astype(np.float32)\n",
    "    data_target = data_target.apply(pd.to_numeric)\n",
    "    data_target = data_target.astype(np.float32)\n",
    "    \n",
    "    step_size = 2500  # 데이터 구간 이동 단위\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    val_acc_list = []\n",
    "    test_acc_list = []\n",
    "    data_input.to_csv(\"onehotenc.csv\")\n",
    "    \n",
    "    for exp in range(num_experiments):\n",
    "        train_start = exp * step_size\n",
    "        train_end = train_start + step_size * 8\n",
    "        val_end = train_end + step_size\n",
    "        test_end = val_end + step_size\n",
    "        if test_end > len(data_input):\n",
    "            break\n",
    "        print(f\"Experiment {exp}: 데이터 구간 [{train_start}:{test_end}]\")\n",
    "        print(data)\n",
    "        \n",
    "        train_input = data_input.iloc[train_start:train_end]\n",
    "        train_target = data_target.iloc[train_start:train_end]\n",
    "        val_input = data_input.iloc[train_end:val_end]\n",
    "        val_target = data_target.iloc[train_end:val_end]\n",
    "        test_input = data_input.iloc[val_end:test_end]\n",
    "        test_target = data_target.iloc[val_end:test_end]\n",
    "        \n",
    "        train_dataset = TimeSeriesDataset(train_input, train_target, lookback=lookback)\n",
    "        val_dataset = TimeSeriesDataset(val_input, val_target, lookback=lookback)\n",
    "        test_dataset = TimeSeriesDataset(test_input, test_target, lookback=lookback)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        lr = 1e-4\n",
    "        input_dim = data_input.shape[1]\n",
    "        model = EncoderOnlyTransformerCustom(input_dim=input_dim, embedding_dim=512, num_heads=8, \n",
    "                                               num_layers=6, ffn_dim=2048, num_classes=2, max_seq_len=lookback).to(device)\n",
    "        model_path = f\"model_experiment_{exp}.pth\"\n",
    "        if exp > 0:\n",
    "            try:\n",
    "                model.load_state_dict(torch.load(f\"model_experiment_{exp - 1}.pth\"))\n",
    "                print(f\"Loaded model from experiment {exp - 1} for fine-tuning.\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Model file for experiment {exp - 1} not found. Starting fresh training.\")\n",
    "        \n",
    "        print(f\"Experiment {exp}: Training with lr={lr} (Fine-Tuning)\")\n",
    "        model = train_model(model, train_loader, val_loader, num_epochs, lr, device)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Saved model for experiment {exp}.\")\n",
    "        \n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, device)\n",
    "        val_acc_list.append(val_acc)\n",
    "        print(f\"Experiment {exp}: Final Validation Accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "        test_loss, test_acc = evaluate_model(model, test_loader, device)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(f\"Experiment {exp}: Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    if len(val_acc_list) > 0:\n",
    "        avg_val_acc = sum(val_acc_list) / len(val_acc_list)\n",
    "        avg_test_acc = sum(test_acc_list) / len(test_acc_list)\n",
    "        print(f\"\\nFinal Average Validation Accuracy: {avg_val_acc:.4f}\")\n",
    "        print(f\"Final Average Test Accuracy: {avg_test_acc:.4f}\")\n",
    "    else:\n",
    "        print(\"실험이 한 번도 실행되지 않았습니다.\")\n",
    "\n",
    "\n",
    "# # features_pct 리스트: 각 원본 feature에 대해 '_pct'가 붙은 컬럼명\n",
    "# features_pct = [f + '_pct' for f in features_to_bin]\n",
    "\n",
    "train_and_evaluate(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 0: 데이터 구간 [0:25000]\n",
      "Experiment 0: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.7201, Train Acc: 0.5113 | Val Loss: 0.6965, Val Acc: 0.5032\n",
      "Epoch 2/10 | Train Loss: 0.6936, Train Acc: 0.5292 | Val Loss: 0.7069, Val Acc: 0.4972\n",
      "Epoch 3/10 | Train Loss: 0.6721, Train Acc: 0.5922 | Val Loss: 0.7048, Val Acc: 0.5065\n",
      "Epoch 4/10 | Train Loss: 0.6554, Train Acc: 0.6141 | Val Loss: 0.7429, Val Acc: 0.5028\n",
      "Epoch 5/10 | Train Loss: 0.6323, Train Acc: 0.6503 | Val Loss: 0.7491, Val Acc: 0.4919\n",
      "Epoch 6/10 | Train Loss: 0.6159, Train Acc: 0.6690 | Val Loss: 0.8047, Val Acc: 0.4968\n",
      "Epoch 7/10 | Train Loss: 0.6073, Train Acc: 0.6763 | Val Loss: 0.7616, Val Acc: 0.5016\n",
      "Epoch 8/10 | Train Loss: 0.5880, Train Acc: 0.6982 | Val Loss: 0.7961, Val Acc: 0.4879\n",
      "Epoch 9/10 | Train Loss: 0.5811, Train Acc: 0.7051 | Val Loss: 0.8206, Val Acc: 0.5016\n",
      "Epoch 10/10 | Train Loss: 0.5767, Train Acc: 0.7109 | Val Loss: 0.7620, Val Acc: 0.4952\n",
      "Saved model for experiment 0.\n",
      "Experiment 0: Final Validation Accuracy: 0.4952\n",
      "Experiment 0: Test Accuracy: 0.5186\n",
      "Experiment 1: 데이터 구간 [2500:27500]\n",
      "Loaded model from experiment 0 for fine-tuning.\n",
      "Experiment 1: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6209, Train Acc: 0.6613 | Val Loss: 0.7246, Val Acc: 0.5133\n",
      "Epoch 2/10 | Train Loss: 0.6118, Train Acc: 0.6693 | Val Loss: 0.7773, Val Acc: 0.5145\n",
      "Epoch 3/10 | Train Loss: 0.5972, Train Acc: 0.6841 | Val Loss: 0.8184, Val Acc: 0.5194\n",
      "Epoch 4/10 | Train Loss: 0.5885, Train Acc: 0.6957 | Val Loss: 0.7400, Val Acc: 0.5218\n",
      "Epoch 5/10 | Train Loss: 0.5545, Train Acc: 0.7232 | Val Loss: 0.7376, Val Acc: 0.5190\n",
      "Epoch 6/10 | Train Loss: 0.5362, Train Acc: 0.7365 | Val Loss: 0.7302, Val Acc: 0.5218\n",
      "Epoch 7/10 | Train Loss: 0.5137, Train Acc: 0.7522 | Val Loss: 0.7540, Val Acc: 0.5061\n",
      "Epoch 8/10 | Train Loss: 0.4670, Train Acc: 0.7828 | Val Loss: 0.8860, Val Acc: 0.5299\n",
      "Epoch 9/10 | Train Loss: 0.4431, Train Acc: 0.7997 | Val Loss: 0.8556, Val Acc: 0.5186\n",
      "Epoch 10/10 | Train Loss: 0.4217, Train Acc: 0.8135 | Val Loss: 0.8165, Val Acc: 0.5182\n",
      "Saved model for experiment 1.\n",
      "Experiment 1: Final Validation Accuracy: 0.5182\n",
      "Experiment 1: Test Accuracy: 0.5020\n",
      "Experiment 2: 데이터 구간 [5000:30000]\n",
      "Loaded model from experiment 1 for fine-tuning.\n",
      "Experiment 2: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.5418, Train Acc: 0.7217 | Val Loss: 0.7324, Val Acc: 0.4976\n",
      "Epoch 2/10 | Train Loss: 0.5159, Train Acc: 0.7374 | Val Loss: 0.7572, Val Acc: 0.4867\n",
      "Epoch 3/10 | Train Loss: 0.4968, Train Acc: 0.7469 | Val Loss: 0.8031, Val Acc: 0.4935\n",
      "Epoch 4/10 | Train Loss: 0.4775, Train Acc: 0.7629 | Val Loss: 0.8061, Val Acc: 0.4980\n",
      "Epoch 5/10 | Train Loss: 0.4031, Train Acc: 0.8099 | Val Loss: 0.8930, Val Acc: 0.4742\n",
      "Epoch 6/10 | Train Loss: 0.3652, Train Acc: 0.8300 | Val Loss: 1.0363, Val Acc: 0.4964\n",
      "Epoch 7/10 | Train Loss: 0.3382, Train Acc: 0.8472 | Val Loss: 1.2218, Val Acc: 0.4899\n",
      "Epoch 8/10 | Train Loss: 0.2725, Train Acc: 0.8846 | Val Loss: 1.2778, Val Acc: 0.4838\n",
      "Epoch 9/10 | Train Loss: 0.2460, Train Acc: 0.8976 | Val Loss: 1.4368, Val Acc: 0.4790\n",
      "Epoch 10/10 | Train Loss: 0.2247, Train Acc: 0.9097 | Val Loss: 1.5595, Val Acc: 0.4891\n",
      "Saved model for experiment 2.\n",
      "Experiment 2: Final Validation Accuracy: 0.4891\n",
      "Experiment 2: Test Accuracy: 0.5085\n",
      "Experiment 3: 데이터 구간 [7500:32500]\n",
      "Loaded model from experiment 2 for fine-tuning.\n",
      "Experiment 3: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.4463, Train Acc: 0.7841 | Val Loss: 1.1233, Val Acc: 0.5089\n",
      "Epoch 2/10 | Train Loss: 0.4154, Train Acc: 0.8018 | Val Loss: 1.0363, Val Acc: 0.5032\n",
      "Epoch 3/10 | Train Loss: 0.3944, Train Acc: 0.8135 | Val Loss: 0.9849, Val Acc: 0.4947\n",
      "Epoch 4/10 | Train Loss: 0.3728, Train Acc: 0.8270 | Val Loss: 0.9848, Val Acc: 0.5012\n",
      "Epoch 5/10 | Train Loss: 0.3511, Train Acc: 0.8399 | Val Loss: 1.0694, Val Acc: 0.4984\n",
      "Epoch 6/10 | Train Loss: 0.3314, Train Acc: 0.8524 | Val Loss: 1.1822, Val Acc: 0.4964\n",
      "Epoch 7/10 | Train Loss: 0.3016, Train Acc: 0.8659 | Val Loss: 1.1447, Val Acc: 0.5121\n",
      "Epoch 8/10 | Train Loss: 0.2156, Train Acc: 0.9111 | Val Loss: 1.3306, Val Acc: 0.5020\n",
      "Epoch 9/10 | Train Loss: 0.1842, Train Acc: 0.9253 | Val Loss: 1.7352, Val Acc: 0.4972\n",
      "Epoch 10/10 | Train Loss: 0.1665, Train Acc: 0.9372 | Val Loss: 2.1251, Val Acc: 0.5085\n",
      "Saved model for experiment 3.\n",
      "Experiment 3: Final Validation Accuracy: 0.5085\n",
      "Experiment 3: Test Accuracy: 0.5129\n",
      "Experiment 4: 데이터 구간 [10000:35000]\n",
      "Loaded model from experiment 3 for fine-tuning.\n",
      "Experiment 4: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.3293, Train Acc: 0.8433 | Val Loss: 0.7110, Val Acc: 0.4943\n",
      "Epoch 2/10 | Train Loss: 0.3025, Train Acc: 0.8540 | Val Loss: 0.7184, Val Acc: 0.4822\n",
      "Epoch 3/10 | Train Loss: 0.2852, Train Acc: 0.8602 | Val Loss: 0.7287, Val Acc: 0.5040\n",
      "Epoch 4/10 | Train Loss: 0.2678, Train Acc: 0.8702 | Val Loss: 0.7260, Val Acc: 0.4867\n",
      "Epoch 5/10 | Train Loss: 0.2014, Train Acc: 0.9045 | Val Loss: 0.7880, Val Acc: 0.4834\n",
      "Epoch 6/10 | Train Loss: 0.1797, Train Acc: 0.9124 | Val Loss: 0.7852, Val Acc: 0.4919\n",
      "Epoch 7/10 | Train Loss: 0.1591, Train Acc: 0.9231 | Val Loss: 0.8206, Val Acc: 0.4826\n",
      "Epoch 8/10 | Train Loss: 0.1163, Train Acc: 0.9425 | Val Loss: 0.9584, Val Acc: 0.4834\n",
      "Epoch 9/10 | Train Loss: 0.1051, Train Acc: 0.9496 | Val Loss: 1.3824, Val Acc: 0.4810\n",
      "Epoch 10/10 | Train Loss: 0.0898, Train Acc: 0.9597 | Val Loss: 1.5709, Val Acc: 0.4891\n",
      "Saved model for experiment 4.\n",
      "Experiment 4: Final Validation Accuracy: 0.4891\n",
      "Experiment 4: Test Accuracy: 0.4972\n",
      "Experiment 5: 데이터 구간 [12500:37500]\n",
      "Loaded model from experiment 4 for fine-tuning.\n",
      "Experiment 5: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.2928, Train Acc: 0.8536 | Val Loss: 0.7130, Val Acc: 0.4964\n",
      "Epoch 2/10 | Train Loss: 0.2824, Train Acc: 0.8582 | Val Loss: 0.7498, Val Acc: 0.5198\n",
      "Epoch 3/10 | Train Loss: 0.2680, Train Acc: 0.8684 | Val Loss: 0.7369, Val Acc: 0.4976\n",
      "Epoch 4/10 | Train Loss: 0.2565, Train Acc: 0.8727 | Val Loss: 0.7474, Val Acc: 0.5057\n",
      "Epoch 5/10 | Train Loss: 0.1896, Train Acc: 0.9077 | Val Loss: 0.8343, Val Acc: 0.5061\n",
      "Epoch 6/10 | Train Loss: 0.1577, Train Acc: 0.9248 | Val Loss: 1.0728, Val Acc: 0.5044\n",
      "Epoch 7/10 | Train Loss: 0.1341, Train Acc: 0.9386 | Val Loss: 1.1260, Val Acc: 0.5133\n",
      "Epoch 8/10 | Train Loss: 0.0880, Train Acc: 0.9624 | Val Loss: 1.5914, Val Acc: 0.5097\n",
      "Epoch 9/10 | Train Loss: 0.0717, Train Acc: 0.9725 | Val Loss: 1.8953, Val Acc: 0.5089\n",
      "Epoch 10/10 | Train Loss: 0.0508, Train Acc: 0.9814 | Val Loss: 2.3656, Val Acc: 0.5214\n",
      "Saved model for experiment 5.\n",
      "Experiment 5: Final Validation Accuracy: 0.5214\n",
      "Experiment 5: Test Accuracy: 0.4996\n",
      "Experiment 6: 데이터 구간 [15000:40000]\n",
      "Loaded model from experiment 5 for fine-tuning.\n",
      "Experiment 6: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.2712, Train Acc: 0.8733 | Val Loss: 1.5021, Val Acc: 0.5053\n",
      "Epoch 2/10 | Train Loss: 0.2492, Train Acc: 0.8857 | Val Loss: 1.4605, Val Acc: 0.4992\n",
      "Epoch 3/10 | Train Loss: 0.2323, Train Acc: 0.8980 | Val Loss: 1.5966, Val Acc: 0.5036\n",
      "Epoch 4/10 | Train Loss: 0.2017, Train Acc: 0.9134 | Val Loss: 1.8520, Val Acc: 0.4992\n",
      "Epoch 5/10 | Train Loss: 0.1819, Train Acc: 0.9238 | Val Loss: 1.8391, Val Acc: 0.4984\n",
      "Epoch 6/10 | Train Loss: 0.1090, Train Acc: 0.9576 | Val Loss: 2.5006, Val Acc: 0.4935\n",
      "Epoch 7/10 | Train Loss: 0.0817, Train Acc: 0.9714 | Val Loss: 2.7831, Val Acc: 0.4947\n",
      "Epoch 8/10 | Train Loss: 0.0609, Train Acc: 0.9798 | Val Loss: 3.1731, Val Acc: 0.4935\n",
      "Epoch 9/10 | Train Loss: 0.0287, Train Acc: 0.9918 | Val Loss: 3.4780, Val Acc: 0.4996\n",
      "Epoch 10/10 | Train Loss: 0.0132, Train Acc: 0.9963 | Val Loss: 4.0038, Val Acc: 0.4980\n",
      "Saved model for experiment 6.\n",
      "Experiment 6: Final Validation Accuracy: 0.4980\n",
      "Experiment 6: Test Accuracy: 0.5077\n",
      "Experiment 7: 데이터 구간 [17500:42500]\n",
      "Loaded model from experiment 6 for fine-tuning.\n",
      "Experiment 7: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.2940, Train Acc: 0.8910 | Val Loss: 0.7848, Val Acc: 0.5117\n",
      "Epoch 2/10 | Train Loss: 0.2223, Train Acc: 0.9016 | Val Loss: 1.2862, Val Acc: 0.5089\n",
      "Epoch 3/10 | Train Loss: 0.1990, Train Acc: 0.9133 | Val Loss: 0.9313, Val Acc: 0.5250\n",
      "Epoch 4/10 | Train Loss: 0.1807, Train Acc: 0.9199 | Val Loss: 1.1224, Val Acc: 0.5008\n",
      "Epoch 5/10 | Train Loss: 0.1268, Train Acc: 0.9466 | Val Loss: 1.4010, Val Acc: 0.5117\n",
      "Epoch 6/10 | Train Loss: 0.0993, Train Acc: 0.9555 | Val Loss: 1.2803, Val Acc: 0.5065\n",
      "Epoch 7/10 | Train Loss: 0.0815, Train Acc: 0.9648 | Val Loss: 0.8002, Val Acc: 0.5048\n",
      "Epoch 8/10 | Train Loss: 0.0559, Train Acc: 0.9750 | Val Loss: 1.3263, Val Acc: 0.5044\n",
      "Epoch 9/10 | Train Loss: 0.0415, Train Acc: 0.9819 | Val Loss: 1.5602, Val Acc: 0.5044\n",
      "Epoch 10/10 | Train Loss: 0.0367, Train Acc: 0.9848 | Val Loss: 1.7335, Val Acc: 0.4782\n",
      "Saved model for experiment 7.\n",
      "Experiment 7: Final Validation Accuracy: 0.4782\n",
      "Experiment 7: Test Accuracy: 0.5020\n",
      "Experiment 8: 데이터 구간 [20000:45000]\n",
      "Loaded model from experiment 7 for fine-tuning.\n",
      "Experiment 8: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.2223, Train Acc: 0.8872 | Val Loss: 0.7031, Val Acc: 0.4818\n",
      "Epoch 2/10 | Train Loss: 0.2216, Train Acc: 0.8877 | Val Loss: 1.3505, Val Acc: 0.5016\n",
      "Epoch 3/10 | Train Loss: 0.2185, Train Acc: 0.8931 | Val Loss: 0.7188, Val Acc: 0.4996\n",
      "Epoch 4/10 | Train Loss: 0.2065, Train Acc: 0.9015 | Val Loss: 0.7848, Val Acc: 0.4915\n",
      "Epoch 5/10 | Train Loss: 0.1521, Train Acc: 0.9237 | Val Loss: 0.8817, Val Acc: 0.5012\n",
      "Epoch 6/10 | Train Loss: 0.1310, Train Acc: 0.9373 | Val Loss: 0.9907, Val Acc: 0.4915\n",
      "Epoch 7/10 | Train Loss: 0.1221, Train Acc: 0.9467 | Val Loss: 1.2421, Val Acc: 0.5032\n",
      "Epoch 8/10 | Train Loss: 0.0814, Train Acc: 0.9660 | Val Loss: 1.5619, Val Acc: 0.5166\n",
      "Epoch 9/10 | Train Loss: 0.0579, Train Acc: 0.9792 | Val Loss: 1.5395, Val Acc: 0.5170\n",
      "Epoch 10/10 | Train Loss: 0.0401, Train Acc: 0.9874 | Val Loss: 2.2960, Val Acc: 0.5081\n",
      "Saved model for experiment 8.\n",
      "Experiment 8: Final Validation Accuracy: 0.5081\n",
      "Experiment 8: Test Accuracy: 0.5032\n",
      "Experiment 9: 데이터 구간 [22500:47500]\n",
      "Loaded model from experiment 8 for fine-tuning.\n",
      "Experiment 9: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.2442, Train Acc: 0.8864 | Val Loss: 1.4573, Val Acc: 0.4818\n",
      "Epoch 2/10 | Train Loss: 0.2221, Train Acc: 0.8970 | Val Loss: 1.4591, Val Acc: 0.5101\n",
      "Epoch 3/10 | Train Loss: 0.2006, Train Acc: 0.9141 | Val Loss: 1.3368, Val Acc: 0.5077\n",
      "Epoch 4/10 | Train Loss: 0.1868, Train Acc: 0.9211 | Val Loss: 1.1688, Val Acc: 0.5117\n",
      "Epoch 5/10 | Train Loss: 0.1702, Train Acc: 0.9335 | Val Loss: 1.3654, Val Acc: 0.5125\n",
      "Epoch 6/10 | Train Loss: 0.1517, Train Acc: 0.9419 | Val Loss: 1.5977, Val Acc: 0.5097\n",
      "Epoch 7/10 | Train Loss: 0.1255, Train Acc: 0.9547 | Val Loss: 1.9987, Val Acc: 0.5036\n",
      "Epoch 8/10 | Train Loss: 0.0654, Train Acc: 0.9791 | Val Loss: 2.3335, Val Acc: 0.5057\n",
      "Epoch 9/10 | Train Loss: 0.0409, Train Acc: 0.9873 | Val Loss: 2.8565, Val Acc: 0.5057\n",
      "Epoch 10/10 | Train Loss: 0.0281, Train Acc: 0.9925 | Val Loss: 3.3959, Val Acc: 0.5008\n",
      "Saved model for experiment 9.\n",
      "Experiment 9: Final Validation Accuracy: 0.5008\n",
      "Experiment 9: Test Accuracy: 0.4834\n",
      "Experiment 10: 데이터 구간 [25000:50000]\n",
      "Loaded model from experiment 9 for fine-tuning.\n",
      "Experiment 10: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.2194, Train Acc: 0.9096 | Val Loss: 0.8168, Val Acc: 0.5065\n",
      "Epoch 2/10 | Train Loss: 0.1781, Train Acc: 0.9250 | Val Loss: 0.7975, Val Acc: 0.5020\n",
      "Epoch 3/10 | Train Loss: 0.1631, Train Acc: 0.9324 | Val Loss: 0.8768, Val Acc: 0.5044\n",
      "Epoch 4/10 | Train Loss: 0.1397, Train Acc: 0.9453 | Val Loss: 0.8379, Val Acc: 0.5000\n",
      "Epoch 5/10 | Train Loss: 0.1264, Train Acc: 0.9539 | Val Loss: 1.1414, Val Acc: 0.4919\n",
      "Epoch 6/10 | Train Loss: 0.0670, Train Acc: 0.9783 | Val Loss: 1.2850, Val Acc: 0.5000\n",
      "Epoch 7/10 | Train Loss: 0.0428, Train Acc: 0.9871 | Val Loss: 1.8213, Val Acc: 0.4947\n",
      "Epoch 8/10 | Train Loss: 0.0316, Train Acc: 0.9913 | Val Loss: 2.8602, Val Acc: 0.4947\n",
      "Epoch 9/10 | Train Loss: 0.0138, Train Acc: 0.9967 | Val Loss: 3.2885, Val Acc: 0.5053\n",
      "Epoch 10/10 | Train Loss: 0.0070, Train Acc: 0.9982 | Val Loss: 3.2287, Val Acc: 0.4960\n",
      "Saved model for experiment 10.\n",
      "Experiment 10: Final Validation Accuracy: 0.4960\n",
      "Experiment 10: Test Accuracy: 0.4972\n",
      "Experiment 11: 데이터 구간 [27500:52500]\n",
      "Loaded model from experiment 10 for fine-tuning.\n",
      "Experiment 11: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.1860, Train Acc: 0.9135 | Val Loss: 0.9742, Val Acc: 0.4903\n",
      "Epoch 2/10 | Train Loss: 0.1865, Train Acc: 0.9126 | Val Loss: 0.8106, Val Acc: 0.4911\n",
      "Epoch 3/10 | Train Loss: 0.1689, Train Acc: 0.9256 | Val Loss: 0.9060, Val Acc: 0.5028\n",
      "Epoch 4/10 | Train Loss: 0.1572, Train Acc: 0.9318 | Val Loss: 0.7623, Val Acc: 0.5065\n",
      "Epoch 5/10 | Train Loss: 0.1473, Train Acc: 0.9395 | Val Loss: 1.3725, Val Acc: 0.5028\n",
      "Epoch 6/10 | Train Loss: 0.1353, Train Acc: 0.9468 | Val Loss: 1.5160, Val Acc: 0.4915\n",
      "Epoch 7/10 | Train Loss: 0.1220, Train Acc: 0.9553 | Val Loss: 1.3760, Val Acc: 0.5036\n",
      "Epoch 8/10 | Train Loss: 0.0644, Train Acc: 0.9778 | Val Loss: 2.4495, Val Acc: 0.5000\n",
      "Epoch 9/10 | Train Loss: 0.0412, Train Acc: 0.9880 | Val Loss: 2.1229, Val Acc: 0.4992\n",
      "Epoch 10/10 | Train Loss: 0.0273, Train Acc: 0.9918 | Val Loss: 2.8161, Val Acc: 0.5008\n",
      "Saved model for experiment 11.\n",
      "Experiment 11: Final Validation Accuracy: 0.5008\n",
      "Experiment 11: Test Accuracy: 0.5101\n",
      "Experiment 12: 데이터 구간 [30000:55000]\n",
      "Loaded model from experiment 11 for fine-tuning.\n",
      "Experiment 12: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.1948, Train Acc: 0.9124 | Val Loss: 2.1627, Val Acc: 0.5028\n",
      "Epoch 2/10 | Train Loss: 0.1751, Train Acc: 0.9222 | Val Loss: 1.7299, Val Acc: 0.4939\n",
      "Epoch 3/10 | Train Loss: 0.1497, Train Acc: 0.9333 | Val Loss: 1.4948, Val Acc: 0.5044\n",
      "Epoch 4/10 | Train Loss: 0.1409, Train Acc: 0.9425 | Val Loss: 2.1864, Val Acc: 0.5133\n",
      "Epoch 5/10 | Train Loss: 0.1294, Train Acc: 0.9514 | Val Loss: 1.3739, Val Acc: 0.5129\n",
      "Epoch 6/10 | Train Loss: 0.1125, Train Acc: 0.9592 | Val Loss: 1.9900, Val Acc: 0.4988\n",
      "Epoch 7/10 | Train Loss: 0.1001, Train Acc: 0.9664 | Val Loss: 2.1709, Val Acc: 0.5178\n",
      "Epoch 8/10 | Train Loss: 0.0863, Train Acc: 0.9725 | Val Loss: 2.2479, Val Acc: 0.5077\n",
      "Epoch 9/10 | Train Loss: 0.0334, Train Acc: 0.9905 | Val Loss: 3.1997, Val Acc: 0.5190\n",
      "Epoch 10/10 | Train Loss: 0.0156, Train Acc: 0.9960 | Val Loss: 3.5156, Val Acc: 0.5048\n",
      "Saved model for experiment 12.\n",
      "Experiment 12: Final Validation Accuracy: 0.5048\n",
      "Experiment 12: Test Accuracy: 0.4968\n",
      "Experiment 13: 데이터 구간 [32500:57500]\n",
      "Loaded model from experiment 12 for fine-tuning.\n",
      "Experiment 13: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.1944, Train Acc: 0.9186 | Val Loss: 1.7843, Val Acc: 0.5024\n",
      "Epoch 2/10 | Train Loss: 0.1480, Train Acc: 0.9340 | Val Loss: 1.9000, Val Acc: 0.5040\n",
      "Epoch 3/10 | Train Loss: 0.1334, Train Acc: 0.9457 | Val Loss: 2.4045, Val Acc: 0.5024\n",
      "Epoch 4/10 | Train Loss: 0.1199, Train Acc: 0.9539 | Val Loss: 2.3626, Val Acc: 0.5020\n",
      "Epoch 5/10 | Train Loss: 0.0649, Train Acc: 0.9779 | Val Loss: 3.1250, Val Acc: 0.5032\n",
      "Epoch 6/10 | Train Loss: 0.0403, Train Acc: 0.9877 | Val Loss: 3.0803, Val Acc: 0.4907\n",
      "Epoch 7/10 | Train Loss: 0.0263, Train Acc: 0.9927 | Val Loss: 3.1000, Val Acc: 0.5028\n",
      "Epoch 8/10 | Train Loss: 0.0132, Train Acc: 0.9961 | Val Loss: 3.7450, Val Acc: 0.4919\n",
      "Epoch 9/10 | Train Loss: 0.0048, Train Acc: 0.9983 | Val Loss: 4.1393, Val Acc: 0.4923\n",
      "Epoch 10/10 | Train Loss: 0.0056, Train Acc: 0.9984 | Val Loss: 4.2512, Val Acc: 0.4847\n",
      "Saved model for experiment 13.\n",
      "Experiment 13: Final Validation Accuracy: 0.4847\n",
      "Experiment 13: Test Accuracy: 0.5028\n",
      "Experiment 14: 데이터 구간 [35000:60000]\n",
      "Loaded model from experiment 13 for fine-tuning.\n",
      "Experiment 14: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.2026, Train Acc: 0.9169 | Val Loss: 0.7436, Val Acc: 0.5149\n",
      "Epoch 2/10 | Train Loss: 0.1586, Train Acc: 0.9262 | Val Loss: 0.7169, Val Acc: 0.5222\n",
      "Epoch 3/10 | Train Loss: 0.1473, Train Acc: 0.9348 | Val Loss: 0.7210, Val Acc: 0.5194\n",
      "Epoch 4/10 | Train Loss: 0.1396, Train Acc: 0.9415 | Val Loss: 1.1472, Val Acc: 0.5234\n",
      "Epoch 5/10 | Train Loss: 0.1323, Train Acc: 0.9484 | Val Loss: 1.0620, Val Acc: 0.5137\n",
      "Epoch 6/10 | Train Loss: 0.0796, Train Acc: 0.9704 | Val Loss: 1.1248, Val Acc: 0.5246\n",
      "Epoch 7/10 | Train Loss: 0.0517, Train Acc: 0.9824 | Val Loss: 1.2722, Val Acc: 0.5174\n",
      "Epoch 8/10 | Train Loss: 0.0362, Train Acc: 0.9889 | Val Loss: 1.2864, Val Acc: 0.5194\n",
      "Epoch 9/10 | Train Loss: 0.0133, Train Acc: 0.9963 | Val Loss: 2.9964, Val Acc: 0.5166\n",
      "Epoch 10/10 | Train Loss: 0.0051, Train Acc: 0.9987 | Val Loss: 3.2275, Val Acc: 0.5210\n",
      "Saved model for experiment 14.\n",
      "Experiment 14: Final Validation Accuracy: 0.5210\n",
      "Experiment 14: Test Accuracy: 0.4939\n",
      "Experiment 15: 데이터 구간 [37500:62500]\n",
      "Loaded model from experiment 14 for fine-tuning.\n",
      "Experiment 15: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.1755, Train Acc: 0.9194 | Val Loss: 0.9341, Val Acc: 0.5065\n",
      "Epoch 2/10 | Train Loss: 0.1530, Train Acc: 0.9298 | Val Loss: 1.0257, Val Acc: 0.5186\n",
      "Epoch 3/10 | Train Loss: 0.1393, Train Acc: 0.9410 | Val Loss: 0.8881, Val Acc: 0.4980\n",
      "Epoch 4/10 | Train Loss: 0.1262, Train Acc: 0.9508 | Val Loss: 1.5445, Val Acc: 0.5028\n",
      "Epoch 5/10 | Train Loss: 0.1129, Train Acc: 0.9589 | Val Loss: 1.5943, Val Acc: 0.4984\n",
      "Epoch 6/10 | Train Loss: 0.0874, Train Acc: 0.9714 | Val Loss: 1.9499, Val Acc: 0.4992\n",
      "Epoch 7/10 | Train Loss: 0.0380, Train Acc: 0.9892 | Val Loss: 2.3837, Val Acc: 0.4956\n",
      "Epoch 8/10 | Train Loss: 0.0182, Train Acc: 0.9954 | Val Loss: 2.8781, Val Acc: 0.4871\n",
      "Epoch 9/10 | Train Loss: 0.0074, Train Acc: 0.9985 | Val Loss: 3.2022, Val Acc: 0.4952\n",
      "Epoch 10/10 | Train Loss: 0.0039, Train Acc: 0.9992 | Val Loss: 3.6239, Val Acc: 0.4887\n",
      "Saved model for experiment 15.\n",
      "Experiment 15: Final Validation Accuracy: 0.4887\n",
      "Experiment 15: Test Accuracy: 0.4960\n",
      "\n",
      "Final Average Validation Accuracy: 0.5002\n",
      "Final Average Test Accuracy: 0.5020\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta as ta\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from math import sqrt\n",
    "\n",
    "####################################\n",
    "# 1. 기술적 지표 계산 함수 (변경 없음)\n",
    "####################################\n",
    "def calculate_indicators(data):\n",
    "    data['William_R'] = ta.willr(data['high'], data['low'], data['close'])\n",
    "    data['ATR'] = ta.atr(data['high'], data['low'], data['close'])\n",
    "    data['OBV'] = ta.obv(data['close'], data['volume'])\n",
    "    data['Z_Score'] = (data['close'] - data['close'].rolling(window=20).mean()) / data['close'].rolling(window=20).std()\n",
    "    data['Entropy'] = ta.entropy(data['close'], length=14)\n",
    "    data['SMA_5'] = data['close'].rolling(window=5).mean()\n",
    "    data['SMA_10'] = data['close'].rolling(window=10).mean()\n",
    "    data['SMA_20'] = data['close'].rolling(window=20).mean()\n",
    "    data['SMA_60'] = data['close'].rolling(window=60).mean()\n",
    "    data['SMA_120'] = data['close'].rolling(window=120).mean()\n",
    "    data['SMA_250'] = data['close'].rolling(window=250).mean()\n",
    "    data['RSI'] = ta.rsi(data['close'])\n",
    "    bb = ta.bbands(data['close'])\n",
    "    data['BB_Upper'], data['BB_Middle'], data['BB_Lower'] = bb.iloc[:, 0], bb.iloc[:, 1], bb.iloc[:, 2]\n",
    "    macd = ta.macd(data['close'])\n",
    "    data['MACD'] = macd.iloc[:, 0]\n",
    "    data['Stochastic'] = ta.stoch(data['high'], data['low'], data['close']).iloc[:, 0]\n",
    "    return data.dropna()\n",
    "\n",
    "####################################\n",
    "# 1-2. 추가 feature 계산 (가격 차이)\n",
    "####################################\n",
    "def calculate_price_differences(data): # 포함 시 50.07%\n",
    "    data['close_open'] = data['close'] - data['open']\n",
    "    data['high_low'] = data['high'] - data['low']\n",
    "    data['high_open'] = data['high'] - data['open']\n",
    "    data['high_close'] = data['high'] - data['close']\n",
    "    data['open_low'] = data['open'] - data['low']\n",
    "    data['close_low'] = data['close'] - data['low']\n",
    "    return data\n",
    "\n",
    "####################################\n",
    "# 2. Datetime Feature One-Hot Encoding (각 feature 128차원)\n",
    "####################################\n",
    "def encode_datetime_features_onehot(data, dim=128):\n",
    "    if 'datetime' not in data.columns:\n",
    "        data['datetime'] = pd.to_datetime(data.index)\n",
    "    \n",
    "    data['hour_of_day'] = data['datetime'].dt.hour\n",
    "    data['day_of_week'] = data['datetime'].dt.dayofweek\n",
    "    data['week_of_month'] = (data['datetime'].dt.day - 1) // 7 + 1\n",
    "    data['month'] = data['datetime'].dt.month\n",
    "\n",
    "    def onehot_with_fixed_dim(series, prefix, dim):\n",
    "        dummies = pd.get_dummies(series, prefix=prefix)\n",
    "        expected_cols = [f\"{prefix}_{i}\" for i in range(dim)]\n",
    "        dummies = dummies.reindex(columns=expected_cols, fill_value=0)\n",
    "        return dummies\n",
    "\n",
    "    hour_one_hot = onehot_with_fixed_dim(data['hour_of_day'], 'Hour', dim)\n",
    "    day_one_hot = onehot_with_fixed_dim(data['day_of_week'], 'Day', dim)\n",
    "    week_one_hot = onehot_with_fixed_dim(data['week_of_month'], 'Week', dim)\n",
    "    month_one_hot = onehot_with_fixed_dim(data['month'], 'Month', dim)\n",
    "    \n",
    "    data = pd.concat([data, hour_one_hot, day_one_hot, week_one_hot, month_one_hot], axis=1)\n",
    "    return data\n",
    "\n",
    "####################################\n",
    "# 3. Rolling MinMax Scaling (분모 0 방지)\n",
    "####################################\n",
    "def rolling_minmax_scale(series, window=24):\n",
    "    roll_min = series.rolling(window=window, min_periods=window).min()\n",
    "    roll_max = series.rolling(window=window, min_periods=window).max()\n",
    "    # 기본 minmax scaling 계산 (분모에 아주 작은 epsilon 추가)\n",
    "    scaled = (series - roll_min) / ((roll_max - roll_min) + 1e-8)\n",
    "    # 무한대나 -무한대 값을 NaN으로 대체하고, NaN은 최대값 1.0으로 대체\n",
    "    scaled = scaled.replace([np.inf, -np.inf], np.nan)\n",
    "    scaled = scaled.fillna(1.0)\n",
    "    # 혹시 1보다 큰 값이 있다면 최대값 1.0으로 클리핑\n",
    "    scaled = scaled.clip(upper=1.0)\n",
    "    return scaled\n",
    "\n",
    "\n",
    "####################################\n",
    "# 4. Binning 후 One-Hot 인코딩 (각 feature를 128차원으로)\n",
    "####################################\n",
    "def bin_and_encode(data, features, bins=128, drop_original=True):\n",
    "    for feature in features:\n",
    "        data[f'{feature}_Bin'] = pd.cut(data[feature], bins=bins, labels=False)\n",
    "        one_hot = pd.get_dummies(data[f'{feature}_Bin'], prefix=f'{feature}_Bin')\n",
    "        expected_columns = [f'{feature}_Bin_{i}' for i in range(bins)]\n",
    "        one_hot = one_hot.reindex(columns=expected_columns, fill_value=0)\n",
    "        data = pd.concat([data, one_hot], axis=1)\n",
    "        if drop_original:\n",
    "            data.drop(columns=[f'{feature}_Bin'], inplace=True)\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    data[numeric_cols] = data[numeric_cols].astype(np.float32)\n",
    "    return data\n",
    "\n",
    "####################################\n",
    "# 5. 데이터 로드 및 전처리\n",
    "####################################\n",
    "data = pd.read_csv(\"BTC_upbit_KRW_min5.csv\", index_col=0)\n",
    "data.columns = ['open', 'high', 'low', 'close', 'volume', 'value']\n",
    "data.index = pd.to_datetime(data.index)\n",
    "\n",
    "# 기본 지표 계산 및 추가 feature 생성\n",
    "data = calculate_indicators(data)\n",
    "# data = calculate_price_differences(data)\n",
    "data = encode_datetime_features_onehot(data, dim=128)\n",
    "\n",
    "# 타깃: close 값을 그대로 사용 (continuous)\n",
    "data['close_target'] = data['close']\n",
    "\n",
    "####################################\n",
    "# [Bollinger Bands 관련 새로운 feature 계산]\n",
    "####################################\n",
    "data['BB_diff'] = data['BB_Upper'] - data['BB_Lower']\n",
    "data['BB_close_upper'] = data['close'] - data['BB_Upper']\n",
    "data['BB_close_lower'] = data['close'] - data['BB_Lower']\n",
    "\n",
    "####################################\n",
    "# [인코딩 대상 feature 목록 업데이트]\n",
    "####################################\n",
    "# 원래 feature들을 대상으로 전일 대비 상승률(= pct_change()*100)을 계산\n",
    "features_to_bin = ['open', 'high', 'low', 'volume', 'value', 'William_R',\n",
    "                   'ATR', 'OBV', 'Z_Score', 'Entropy', 'SMA_5', 'SMA_10', \n",
    "                   'SMA_20', 'SMA_60', 'SMA_120', 'SMA_250', 'RSI', 'MACD', 'Stochastic',\n",
    "                   'BB_diff', 'BB_close_upper', 'BB_close_lower']\n",
    "\n",
    "# 각 feature에 대해 전일 대비 상승률을 계산하고 rolling minmax scaling 적용\n",
    "for feature in features_to_bin:\n",
    "    col_pct = feature + '_pct'\n",
    "    data[col_pct] = data[feature].pct_change() * 100\n",
    "    data[col_pct] = rolling_minmax_scale(data[col_pct], window=24)\n",
    "    \n",
    "# NaN 제거 (pct_change 및 rolling scaling으로 인한)\n",
    "data = data.dropna()\n",
    "\n",
    "# pct_change 결과에 대해 binning과 one-hot 인코딩 수행\n",
    "features_pct = [f + '_pct' for f in features_to_bin]\n",
    "data = bin_and_encode(data, features_pct, bins=128, drop_original=True)\n",
    "\n",
    "# datetime one-hot encoding된 컬럼 추출\n",
    "datetime_onehot_features = [col for col in data.columns if col.startswith('Hour_') or \n",
    "                              col.startswith('Day_') or col.startswith('Week_') or \n",
    "                              col.startswith('Month_')]\n",
    "\n",
    "# 최종 입력 데이터 구성: 각 지표의 상승률에 대한 one-hot 벡터와 datetime one-hot 벡터 결합\n",
    "final_input_columns = []\n",
    "for feature in features_pct:\n",
    "    final_input_columns.extend([f'{feature}_Bin_{i}' for i in range(128)])\n",
    "final_input_columns.extend(datetime_onehot_features)\n",
    "\n",
    "final_target_column = ['close_target']\n",
    "\n",
    "data_input = data[final_input_columns]\n",
    "data_target = data[final_target_column]\n",
    "\n",
    "####################################\n",
    "# 6-2. Dataset 정의 (입력과 타깃을 별도로 사용)\n",
    "####################################\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, input_data, target_data, lookback=24):\n",
    "        self.input_data = input_data.values\n",
    "        self.target_data = target_data.values  # shape: (N, 1)\n",
    "        self.lookback = lookback\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_data) - self.lookback\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.input_data[idx: idx + self.lookback, :]\n",
    "        y = self.target_data[idx + self.lookback, 0]\n",
    "        y_prev = self.target_data[idx + self.lookback - 1, 0]\n",
    "        y_target = 1 if y > y_prev else 0\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y_target, dtype=torch.long)\n",
    "\n",
    "####################################\n",
    "# 7. Transformer Encoder 직접 구현\n",
    "####################################\n",
    "# 7-1. Multi-Head Self-Attention\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == embed_dim, \"embed_dim은 num_heads로 나누어떨어져야 합니다.\"\n",
    "        \n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key   = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out   = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / sqrt(self.head_dim)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, V)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "\n",
    "# 7-2. Feed-Forward Network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ffn_dim):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, ffn_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(ffn_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "# 7-3. Transformer Encoder Layer (Self-Attention + FFN + Residual + LayerNorm)\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = FeedForward(embed_dim, ffn_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attn_out = self.self_attn(x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_out)\n",
    "        return x\n",
    "\n",
    "# 7-4. Encoder-Only Transformer 직접 구현 (lookback=24이므로, max_seq_len=24)\n",
    "class EncoderOnlyTransformerCustom(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=512, num_heads=8, num_layers=6, ffn_dim=2048, num_classes=2, max_seq_len=24):\n",
    "        super(EncoderOnlyTransformerCustom, self).__init__()\n",
    "        self.token_embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embedding_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(embedding_dim, num_heads, ffn_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x = self.token_embedding(x)\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        x = x + self.position_embedding(positions)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x[:, -1, :]\n",
    "        return self.fc(x)\n",
    "\n",
    "####################################\n",
    "# 8. 학습 및 평가 루프 (Fine-tuning 및 Validation Accuracy 출력)\n",
    "####################################\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return total_loss / len(data_loader), correct / total\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, lr, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_state = None\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, device)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = model.state_dict()\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(data, num_experiments=16, lookback=24, num_epochs=10):\n",
    "    # 최종 입력은 각 지표의 상승률(one-hot 인코딩된)과 datetime one-hot 인코딩 결합\n",
    "    input_cols = []\n",
    "    for feature in features_pct:\n",
    "        input_cols.extend([f'{feature}_Bin_{i}' for i in range(128)])\n",
    "    input_cols.extend(datetime_onehot_features)\n",
    "    target_cols = ['close_target']\n",
    "    \n",
    "    data_input = data[input_cols]\n",
    "    data_target = data[target_cols]\n",
    "    \n",
    "    # 여기서 모든 컬럼을 numeric 타입으로 변환\n",
    "    data_input = data_input.apply(pd.to_numeric)\n",
    "    data_input = data_input.astype(np.float32)\n",
    "    data_target = data_target.apply(pd.to_numeric)\n",
    "    data_target = data_target.astype(np.float32)\n",
    "    \n",
    "    step_size = 2500  # 데이터 구간 이동 단위\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    val_acc_list = []\n",
    "    test_acc_list = []\n",
    "    data_input.to_csv(\"onehotenc.csv\")\n",
    "    \n",
    "    for exp in range(num_experiments):\n",
    "        train_start = exp * step_size\n",
    "        train_end = train_start + step_size * 8\n",
    "        val_end = train_end + step_size\n",
    "        test_end = val_end + step_size\n",
    "        if test_end > len(data_input):\n",
    "            break\n",
    "        print(f\"Experiment {exp}: 데이터 구간 [{train_start}:{test_end}]\")\n",
    "        # print(data)\n",
    "        \n",
    "        train_input = data_input.iloc[train_start:train_end]\n",
    "        train_target = data_target.iloc[train_start:train_end]\n",
    "        val_input = data_input.iloc[train_end:val_end]\n",
    "        val_target = data_target.iloc[train_end:val_end]\n",
    "        test_input = data_input.iloc[val_end:test_end]\n",
    "        test_target = data_target.iloc[val_end:test_end]\n",
    "        \n",
    "        train_dataset = TimeSeriesDataset(train_input, train_target, lookback=lookback)\n",
    "        val_dataset = TimeSeriesDataset(val_input, val_target, lookback=lookback)\n",
    "        test_dataset = TimeSeriesDataset(test_input, test_target, lookback=lookback)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        lr = 1e-4\n",
    "        input_dim = data_input.shape[1]\n",
    "        model = EncoderOnlyTransformerCustom(input_dim=input_dim, embedding_dim=512, num_heads=8, \n",
    "                                               num_layers=6, ffn_dim=2048, num_classes=2, max_seq_len=lookback).to(device)\n",
    "        model_path = f\"model_experiment_{exp}.pth\"\n",
    "        if exp > 0:\n",
    "            try:\n",
    "                model.load_state_dict(torch.load(f\"model_experiment_{exp - 1}.pth\"))\n",
    "                print(f\"Loaded model from experiment {exp - 1} for fine-tuning.\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Model file for experiment {exp - 1} not found. Starting fresh training.\")\n",
    "        \n",
    "        print(f\"Experiment {exp}: Training with lr={lr} (Fine-Tuning)\")\n",
    "        model = train_model(model, train_loader, val_loader, num_epochs, lr, device)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Saved model for experiment {exp}.\")\n",
    "        \n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, device)\n",
    "        val_acc_list.append(val_acc)\n",
    "        print(f\"Experiment {exp}: Final Validation Accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "        test_loss, test_acc = evaluate_model(model, test_loader, device)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(f\"Experiment {exp}: Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    if len(val_acc_list) > 0:\n",
    "        avg_val_acc = sum(val_acc_list) / len(val_acc_list)\n",
    "        avg_test_acc = sum(test_acc_list) / len(test_acc_list)\n",
    "        print(f\"\\nFinal Average Validation Accuracy: {avg_val_acc:.4f}\")\n",
    "        print(f\"Final Average Test Accuracy: {avg_test_acc:.4f}\")\n",
    "    else:\n",
    "        print(\"실험이 한 번도 실행되지 않았습니다.\")\n",
    "\n",
    "\n",
    "# # features_pct 리스트: 각 원본 feature에 대해 '_pct'가 붙은 컬럼명\n",
    "# features_pct = [f + '_pct' for f in features_to_bin]\n",
    "\n",
    "train_and_evaluate(data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39coin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
