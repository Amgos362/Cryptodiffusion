{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_32480\\596311216.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['close_target'] = data['close']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experiment 0: 데이터 구간 [0:25000]\n",
      "Experiment 0: Training Diffusion Model\n",
      "Epoch 1/10, Loss: 0.9116\n",
      "Epoch 2/10, Loss: 0.6441\n",
      "Epoch 3/10, Loss: 0.5454\n",
      "Epoch 4/10, Loss: 0.5273\n",
      "Epoch 5/10, Loss: 0.4875\n",
      "Epoch 6/10, Loss: 0.4613\n",
      "Epoch 7/10, Loss: 0.4215\n",
      "Epoch 8/10, Loss: 0.4034\n",
      "Epoch 9/10, Loss: 0.3565\n",
      "Epoch 10/10, Loss: 0.3303\n",
      "Saved model for experiment 0.\n",
      "Validation Evaluation:\n",
      "Evaluation Accuracy: 0.5513\n",
      "Test Evaluation:\n",
      "Evaluation Accuracy: 0.5828\n",
      "\n",
      "Experiment 1: 데이터 구간 [2500:27500]\n",
      "Loaded model from experiment 0 for fine-tuning.\n",
      "Experiment 1: Training Diffusion Model\n",
      "Epoch 1/10, Loss: 0.3608\n",
      "Epoch 2/10, Loss: 0.3295\n",
      "Epoch 3/10, Loss: 0.3193\n",
      "Epoch 4/10, Loss: 0.2910\n",
      "Epoch 5/10, Loss: 0.2821\n",
      "Epoch 6/10, Loss: 0.2726\n",
      "Epoch 7/10, Loss: 0.2537\n",
      "Epoch 8/10, Loss: 0.2479\n",
      "Epoch 9/10, Loss: 0.2477\n",
      "Epoch 10/10, Loss: 0.2383\n",
      "Saved model for experiment 1.\n",
      "Validation Evaluation:\n",
      "Evaluation Accuracy: 0.5557\n",
      "Test Evaluation:\n",
      "Evaluation Accuracy: 0.5388\n",
      "\n",
      "Experiment 2: 데이터 구간 [5000:30000]\n",
      "Loaded model from experiment 1 for fine-tuning.\n",
      "Experiment 2: Training Diffusion Model\n",
      "Epoch 1/10, Loss: 0.2993\n",
      "Epoch 2/10, Loss: 0.2639\n",
      "Epoch 3/10, Loss: 0.2448\n",
      "Epoch 4/10, Loss: 0.2345\n",
      "Epoch 5/10, Loss: 0.2123\n",
      "Epoch 6/10, Loss: 0.2102\n",
      "Epoch 7/10, Loss: 0.2002\n",
      "Epoch 8/10, Loss: 0.1949\n",
      "Epoch 9/10, Loss: 0.1798\n",
      "Epoch 10/10, Loss: 0.1798\n",
      "Saved model for experiment 2.\n",
      "Validation Evaluation:\n",
      "Evaluation Accuracy: 0.5529\n",
      "Test Evaluation:\n",
      "Evaluation Accuracy: 0.5444\n",
      "\n",
      "Experiment 3: 데이터 구간 [7500:32500]\n",
      "Loaded model from experiment 2 for fine-tuning.\n",
      "Experiment 3: Training Diffusion Model\n",
      "Epoch 1/10, Loss: 0.2899\n",
      "Epoch 2/10, Loss: 0.2338\n",
      "Epoch 3/10, Loss: 0.2046\n",
      "Epoch 4/10, Loss: 0.1864\n",
      "Epoch 5/10, Loss: 0.1871\n",
      "Epoch 6/10, Loss: 0.1794\n",
      "Epoch 7/10, Loss: 0.1721\n",
      "Epoch 8/10, Loss: 0.1685\n",
      "Epoch 9/10, Loss: 0.1626\n",
      "Epoch 10/10, Loss: 0.1534\n",
      "Saved model for experiment 3.\n",
      "Validation Evaluation:\n",
      "Evaluation Accuracy: 0.5703\n",
      "Test Evaluation:\n",
      "Evaluation Accuracy: 0.5473\n",
      "\n",
      "Final Average Validation Accuracy: 0.5576\n",
      "Final Average Test Accuracy: 0.5533\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# rolling minmax scaling 함수 (window=24)\n",
    "def rolling_minmax_scale(series, window=24):\n",
    "    roll_min = series.rolling(window=window, min_periods=window).min()\n",
    "    roll_max = series.rolling(window=window, min_periods=window).max()\n",
    "    scaled = (series - roll_min) / ((roll_max - roll_min) + 1e-8)\n",
    "    scaled = scaled.replace([np.inf, -np.inf], np.nan)\n",
    "    scaled = scaled.fillna(1.0)\n",
    "    return scaled.clip(upper=1.0)\n",
    "\n",
    "# binning 및 one-hot 인코딩 함수 (결과를 정수 0,1로)\n",
    "def bin_and_encode(data, features, bins=100, drop_original=True):\n",
    "    for feature in features:\n",
    "        data[f'{feature}_Bin'] = pd.cut(data[feature], bins=bins, labels=False)\n",
    "        one_hot = pd.get_dummies(data[f'{feature}_Bin'], prefix=f'{feature}_Bin').astype(np.int32)\n",
    "        expected_columns = [f'{feature}_Bin_{i}' for i in range(bins)]\n",
    "        one_hot = one_hot.reindex(columns=expected_columns, fill_value=0)\n",
    "        data = pd.concat([data, one_hot], axis=1)\n",
    "        if drop_original:\n",
    "            data.drop(columns=[f'{feature}_Bin'], inplace=True)\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    for col in numeric_cols:\n",
    "        data[col] = data[col].astype(np.float32)\n",
    "    return data\n",
    "\n",
    "##############################################\n",
    "# Diffusion 모델을 위한 Dataset 정의\n",
    "##############################################\n",
    "class DiffusionTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, input_data, target_data, lookback=24):\n",
    "        self.input_data = input_data.values\n",
    "        self.target_data = target_data.values\n",
    "        self.lookback = lookback\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_data) - self.lookback\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 조건 데이터: lookback window\n",
    "        x = self.input_data[idx: idx + self.lookback, :]\n",
    "        # 타깃: close_target 값을 이용해 상승이면 1, 하락이면 0으로 설정 (float형)\n",
    "        y = self.target_data[idx + self.lookback, 0]\n",
    "        y_prev = self.target_data[idx + self.lookback - 1, 0]\n",
    "        label = 1.0 if y > y_prev else 0.0\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor([label], dtype=torch.float32)\n",
    "\n",
    "##############################################\n",
    "# Condition Encoder: 시계열 window를 벡터로 인코딩\n",
    "##############################################\n",
    "class ConditionEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, lookback, condition_dim):\n",
    "        super(ConditionEncoder, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim * lookback, condition_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(condition_dim, condition_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch, lookback, input_dim]\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "##############################################\n",
    "# DiffusionClassifier: diffusion process를 통한 노이즈 예측 모델\n",
    "##############################################\n",
    "class DiffusionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, lookback, condition_dim=128, num_timesteps=100, hidden_dim=128):\n",
    "        super(DiffusionClassifier, self).__init__()\n",
    "        self.num_timesteps = num_timesteps\n",
    "        # diffusion 스케줄 (선형 beta schedule)\n",
    "        betas = torch.linspace(1e-4, 0.02, num_timesteps)\n",
    "        alphas = 1 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas', alphas)\n",
    "        self.register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        \n",
    "        # 조건 인코더: 시계열 데이터를 조건으로 임베딩\n",
    "        self.condition_encoder = ConditionEncoder(input_dim, lookback, condition_dim)\n",
    "        # timestep 임베딩\n",
    "        self.time_embedding = nn.Embedding(num_timesteps, hidden_dim)\n",
    "        # 노이즈 예측 네트워크: 입력은 [y_noisy, condition, timestep embedding]\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(1 + condition_dim + hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_condition, y_noisy, t):\n",
    "        # x_condition: [batch, lookback, input_dim]\n",
    "        # y_noisy: [batch, 1] - 노이즈가 추가된 타깃\n",
    "        # t: [batch] - timestep 인덱스\n",
    "        cond = self.condition_encoder(x_condition)           # [batch, condition_dim]\n",
    "        t_emb = self.time_embedding(t)                         # [batch, hidden_dim]\n",
    "        inp = torch.cat([y_noisy, cond, t_emb], dim=1)          # [batch, 1+condition_dim+hidden_dim]\n",
    "        predicted_noise = self.model(inp)                      # [batch, 1]\n",
    "        return predicted_noise\n",
    "    \n",
    "    def sample(self, x_condition, device):\n",
    "        \"\"\"\n",
    "        reverse diffusion 과정을 통해 조건 x_condition에 대해 예측값을 샘플링\n",
    "        최종 출력은 continuous 값으로, 임계값 0.5로 분류 가능함.\n",
    "        \"\"\"\n",
    "        batch_size = x_condition.size(0)\n",
    "        # 초기 y: 정규분포 노이즈\n",
    "        y = torch.randn(batch_size, 1, device=device)\n",
    "        for t in reversed(range(self.num_timesteps)):\n",
    "            t_tensor = torch.full((batch_size,), t, device=device, dtype=torch.long)\n",
    "            predicted_noise = self.forward(x_condition, y, t_tensor)\n",
    "            alpha = self.alphas[t]\n",
    "            alpha_cumprod = self.alphas_cumprod[t]\n",
    "            beta = self.betas[t]\n",
    "            # DDPM 업데이트: 간단화된 형태\n",
    "            y = (1 / torch.sqrt(alpha)) * (y - (beta / torch.sqrt(1 - alpha_cumprod)) * predicted_noise)\n",
    "            # t > 0이면 약간의 노이즈 추가\n",
    "            if t > 0:\n",
    "                noise = torch.randn_like(y)\n",
    "                y = y + torch.sqrt(beta) * noise\n",
    "        return y\n",
    "\n",
    "##############################################\n",
    "# Diffusion 모델 학습 및 평가 함수\n",
    "##############################################\n",
    "def train_diffusion_model(model, dataloader, num_epochs, device, lr=1e-4):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    mse_loss = nn.MSELoss()\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device)  # [batch, lookback, input_dim]\n",
    "            y = y.to(device)  # [batch, 1] (0.0 or 1.0)\n",
    "            batch_size = x.size(0)\n",
    "            # 각 배치마다 timestep t를 균등 샘플링\n",
    "            t = torch.randint(0, model.num_timesteps, (batch_size,), device=device).long()\n",
    "            # 해당 timestep에 따른 누적 알파값\n",
    "            alphas_cumprod_t = model.alphas_cumprod[t].view(batch_size, 1)\n",
    "            # 노이즈 샘플링\n",
    "            noise = torch.randn_like(y)\n",
    "            # y_noisy = sqrt(alpha_cumprod)*y + sqrt(1-alpha_cumprod)*noise\n",
    "            y_noisy = torch.sqrt(alphas_cumprod_t) * y + torch.sqrt(1 - alphas_cumprod_t) * noise\n",
    "            optimizer.zero_grad()\n",
    "            predicted_noise = model(x, y_noisy, t)\n",
    "            loss = mse_loss(predicted_noise, noise)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "def evaluate_diffusion_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            # reverse diffusion 과정을 통해 예측값 샘플링\n",
    "            y_sampled = model.sample(x, device)\n",
    "            # 0.5 기준으로 분류\n",
    "            y_pred = (y_sampled >= 0.5).float()\n",
    "            correct += (y_pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    acc = correct / total\n",
    "    print(f\"Evaluation Accuracy: {acc:.4f}\")\n",
    "    return acc\n",
    "\n",
    "##############################################\n",
    "# 데이터 로드 및 전처리 (OHLC 4개 데이터 사용)\n",
    "##############################################\n",
    "data = pd.read_csv(\"BTC_upbit_KRW_min60.csv\", index_col=0)\n",
    "data = data[['open', 'high', 'low', 'close']]\n",
    "data.index = pd.to_datetime(data.index)\n",
    "\n",
    "ohlc_features = ['open', 'high', 'low', 'close']\n",
    "for feature in ohlc_features:\n",
    "    data[feature] = rolling_minmax_scale(data[feature], window=24)\n",
    "\n",
    "data = bin_and_encode(data, ohlc_features, bins=100, drop_original=True)\n",
    "# 타깃은 원본 close 값 사용 (실험 목적)\n",
    "data['close_target'] = data['close']\n",
    "data = data.dropna()\n",
    "\n",
    "# 최종 입력: _Bin_ 접미사가 있는 열들만 사용\n",
    "final_input_columns = [col for col in data.columns if '_Bin_' in col]\n",
    "final_target_column = ['close_target']\n",
    "\n",
    "data_input = data[final_input_columns]\n",
    "data_target = data[final_target_column]\n",
    "\n",
    "##############################################\n",
    "# 실험 실행: Diffusion Model 기반 주가 상승/하락 예측\n",
    "##############################################\n",
    "def train_and_evaluate_diffusion(data, num_experiments=4, lookback=24, num_epochs=10):\n",
    "    final_input_columns = [col for col in data.columns if '_Bin_' in col]\n",
    "    target_cols = ['close_target']\n",
    "    \n",
    "    data_input = data[final_input_columns]\n",
    "    data_target = data[target_cols]\n",
    "    \n",
    "    data_input = data_input.apply(pd.to_numeric).astype(np.float32)\n",
    "    data_target = data_target.apply(pd.to_numeric).astype(np.float32)\n",
    "    \n",
    "    step_size = 2500\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    val_acc_list = []\n",
    "    test_acc_list = []\n",
    "    \n",
    "    for exp in range(num_experiments):\n",
    "        train_start = exp * step_size\n",
    "        train_end = train_start + step_size * 8\n",
    "        val_end = train_end + step_size\n",
    "        test_end = val_end + step_size\n",
    "        if test_end > len(data_input):\n",
    "            break\n",
    "        print(f\"\\nExperiment {exp}: 데이터 구간 [{train_start}:{test_end}]\")\n",
    "        \n",
    "        train_input = data_input.iloc[train_start:train_end]\n",
    "        train_target = data_target.iloc[train_start:train_end]\n",
    "        val_input = data_input.iloc[train_end:val_end]\n",
    "        val_target = data_target.iloc[train_end:val_end]\n",
    "        test_input = data_input.iloc[val_end:test_end]\n",
    "        test_target = data_target.iloc[val_end:test_end]\n",
    "        \n",
    "        train_dataset = DiffusionTimeSeriesDataset(train_input, train_target, lookback=lookback)\n",
    "        val_dataset = DiffusionTimeSeriesDataset(val_input, val_target, lookback=lookback)\n",
    "        test_dataset = DiffusionTimeSeriesDataset(test_input, test_target, lookback=lookback)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        input_dim = train_input.shape[1]\n",
    "        model = DiffusionClassifier(input_dim=input_dim, lookback=lookback, \n",
    "                                    condition_dim=128, num_timesteps=100, hidden_dim=128).to(device)\n",
    "        model_path = f\"diffusion_model_experiment_{exp}.pth\"\n",
    "        if exp > 0:\n",
    "            try:\n",
    "                model.load_state_dict(torch.load(f\"diffusion_model_experiment_{exp - 1}.pth\"))\n",
    "                print(f\"Loaded model from experiment {exp - 1} for fine-tuning.\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Model file for experiment {exp - 1} not found. Starting fresh training.\")\n",
    "        \n",
    "        print(f\"Experiment {exp}: Training Diffusion Model\")\n",
    "        train_diffusion_model(model, train_loader, num_epochs, device, lr=1e-4)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Saved model for experiment {exp}.\")\n",
    "        \n",
    "        print(\"Validation Evaluation:\")\n",
    "        val_acc = evaluate_diffusion_model(model, val_loader, device)\n",
    "        val_acc_list.append(val_acc)\n",
    "        \n",
    "        print(\"Test Evaluation:\")\n",
    "        test_acc = evaluate_diffusion_model(model, test_loader, device)\n",
    "        test_acc_list.append(test_acc)\n",
    "    \n",
    "    if val_acc_list:\n",
    "        avg_val_acc = sum(val_acc_list) / len(val_acc_list)\n",
    "        avg_test_acc = sum(test_acc_list) / len(test_acc_list)\n",
    "        print(f\"\\nFinal Average Validation Accuracy: {avg_val_acc:.4f}\")\n",
    "        print(f\"Final Average Test Accuracy: {avg_test_acc:.4f}\")\n",
    "    else:\n",
    "        print(\"실험이 한 번도 실행되지 않았습니다.\")\n",
    "\n",
    "# 최종적으로 Diffusion Model 실험 실행\n",
    "train_and_evaluate_diffusion(data, num_experiments=4, lookback=24, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_16176\\8014991.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['close_target'] = data['close']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experiment 0: 데이터 구간 [0:320000]\n",
      "Experiment 0: Training Diffusion Model\n",
      "Epoch 1/10, Loss: 0.5566\n",
      "Epoch 2/10, Loss: 0.4920\n",
      "Epoch 3/10, Loss: 0.4769\n",
      "Epoch 4/10, Loss: 0.4562\n",
      "Epoch 5/10, Loss: 0.4325\n",
      "Epoch 6/10, Loss: 0.4058\n",
      "Epoch 7/10, Loss: 0.3693\n",
      "Epoch 8/10, Loss: 0.3273\n",
      "Epoch 9/10, Loss: 0.2844\n",
      "Epoch 10/10, Loss: 0.2462\n",
      "Saved model for experiment 0.\n",
      "Validation Evaluation:\n",
      "Evaluation Accuracy: 0.6002\n",
      "Test Evaluation:\n",
      "Evaluation Accuracy: 0.5946\n",
      "\n",
      "Experiment 1: 데이터 구간 [32000:352000]\n",
      "Loaded model from experiment 0 for fine-tuning.\n",
      "Experiment 1: Training Diffusion Model\n",
      "Epoch 1/10, Loss: 0.2551\n",
      "Epoch 2/10, Loss: 0.2178\n",
      "Epoch 3/10, Loss: 0.1908\n",
      "Epoch 4/10, Loss: 0.1644\n",
      "Epoch 5/10, Loss: 0.1419\n",
      "Epoch 6/10, Loss: 0.1251\n",
      "Epoch 7/10, Loss: 0.1110\n",
      "Epoch 8/10, Loss: 0.0991\n",
      "Epoch 9/10, Loss: 0.0914\n",
      "Epoch 10/10, Loss: 0.0831\n",
      "Saved model for experiment 1.\n",
      "Validation Evaluation:\n",
      "Evaluation Accuracy: 0.5915\n",
      "Test Evaluation:\n",
      "Evaluation Accuracy: 0.5971\n",
      "\n",
      "Experiment 2: 데이터 구간 [64000:384000]\n",
      "Loaded model from experiment 1 for fine-tuning.\n",
      "Experiment 2: Training Diffusion Model\n",
      "Epoch 1/10, Loss: 0.1673\n",
      "Epoch 2/10, Loss: 0.1343\n",
      "Epoch 3/10, Loss: 0.1109\n",
      "Epoch 4/10, Loss: 0.0977\n",
      "Epoch 5/10, Loss: 0.0854\n",
      "Epoch 6/10, Loss: 0.0779\n",
      "Epoch 7/10, Loss: 0.0721\n",
      "Epoch 8/10, Loss: 0.0653\n",
      "Epoch 9/10, Loss: 0.0614\n",
      "Epoch 10/10, Loss: 0.0589\n",
      "Saved model for experiment 2.\n",
      "Validation Evaluation:\n",
      "Evaluation Accuracy: 0.5892\n",
      "Test Evaluation:\n",
      "Evaluation Accuracy: 0.5929\n",
      "\n",
      "Experiment 3: 데이터 구간 [96000:416000]\n",
      "Loaded model from experiment 2 for fine-tuning.\n",
      "Experiment 3: Training Diffusion Model\n",
      "Epoch 1/10, Loss: 0.1386\n",
      "Epoch 2/10, Loss: 0.1137\n",
      "Epoch 3/10, Loss: 0.0966\n",
      "Epoch 4/10, Loss: 0.0830\n",
      "Epoch 5/10, Loss: 0.0738\n",
      "Epoch 6/10, Loss: 0.0665\n",
      "Epoch 7/10, Loss: 0.0619\n",
      "Epoch 8/10, Loss: 0.0575\n",
      "Epoch 9/10, Loss: 0.0552\n",
      "Epoch 10/10, Loss: 0.0500\n",
      "Saved model for experiment 3.\n",
      "Validation Evaluation:\n",
      "Evaluation Accuracy: 0.5909\n",
      "Test Evaluation:\n",
      "Evaluation Accuracy: 0.5887\n",
      "\n",
      "Experiment 4: 데이터 구간 [128000:448000]\n",
      "Loaded model from experiment 3 for fine-tuning.\n",
      "Experiment 4: Training Diffusion Model\n",
      "Epoch 1/10, Loss: 0.1253\n",
      "Epoch 2/10, Loss: 0.1030\n",
      "Epoch 3/10, Loss: 0.0873\n",
      "Epoch 4/10, Loss: 0.0773\n",
      "Epoch 5/10, Loss: 0.0692\n",
      "Epoch 6/10, Loss: 0.0630\n",
      "Epoch 7/10, Loss: 0.0580\n",
      "Epoch 8/10, Loss: 0.0533\n",
      "Epoch 9/10, Loss: 0.0510\n",
      "Epoch 10/10, Loss: 0.0490\n",
      "Saved model for experiment 4.\n",
      "Validation Evaluation:\n",
      "Evaluation Accuracy: 0.5852\n",
      "Test Evaluation:\n",
      "Evaluation Accuracy: 0.5935\n",
      "\n",
      "Experiment 5: 데이터 구간 [160000:480000]\n",
      "Loaded model from experiment 4 for fine-tuning.\n",
      "Experiment 5: Training Diffusion Model\n",
      "Epoch 1/10, Loss: 0.1162\n",
      "Epoch 2/10, Loss: 0.0990\n",
      "Epoch 3/10, Loss: 0.0855\n",
      "Epoch 4/10, Loss: 0.0763\n",
      "Epoch 5/10, Loss: 0.0659\n",
      "Epoch 6/10, Loss: 0.0586\n",
      "Epoch 7/10, Loss: 0.0547\n",
      "Epoch 8/10, Loss: 0.0502\n",
      "Epoch 9/10, Loss: 0.0464\n",
      "Epoch 10/10, Loss: 0.0445\n",
      "Saved model for experiment 5.\n",
      "Validation Evaluation:\n",
      "Evaluation Accuracy: 0.5939\n",
      "Test Evaluation:\n",
      "Evaluation Accuracy: 0.5924\n",
      "\n",
      "Experiment 6: 데이터 구간 [192000:512000]\n",
      "Loaded model from experiment 5 for fine-tuning.\n",
      "Experiment 6: Training Diffusion Model\n",
      "Epoch 1/10, Loss: 0.1118\n",
      "Epoch 2/10, Loss: 0.0924\n",
      "Epoch 3/10, Loss: 0.0819\n",
      "Epoch 4/10, Loss: 0.0701\n",
      "Epoch 5/10, Loss: 0.0639\n",
      "Epoch 6/10, Loss: 0.0552\n",
      "Epoch 7/10, Loss: 0.0496\n",
      "Epoch 8/10, Loss: 0.0456\n",
      "Epoch 9/10, Loss: 0.0437\n",
      "Epoch 10/10, Loss: 0.0409\n",
      "Saved model for experiment 6.\n",
      "Validation Evaluation:\n",
      "Evaluation Accuracy: 0.5915\n",
      "Test Evaluation:\n",
      "Evaluation Accuracy: 0.5908\n",
      "\n",
      "Experiment 7: 데이터 구간 [224000:544000]\n",
      "Loaded model from experiment 6 for fine-tuning.\n",
      "Experiment 7: Training Diffusion Model\n",
      "Epoch 1/10, Loss: 0.1089\n",
      "Epoch 2/10, Loss: 0.0892\n",
      "Epoch 3/10, Loss: 0.0782\n",
      "Epoch 4/10, Loss: 0.0695\n",
      "Epoch 5/10, Loss: 0.0597\n",
      "Epoch 6/10, Loss: 0.0527\n",
      "Epoch 7/10, Loss: 0.0484\n",
      "Epoch 8/10, Loss: 0.0454\n",
      "Epoch 9/10, Loss: 0.0406\n",
      "Epoch 10/10, Loss: 0.0384\n",
      "Saved model for experiment 7.\n",
      "Validation Evaluation:\n",
      "Evaluation Accuracy: 0.5943\n",
      "Test Evaluation:\n",
      "Evaluation Accuracy: 0.5965\n",
      "\n",
      "Experiment 8: 데이터 구간 [256000:576000]\n",
      "Loaded model from experiment 7 for fine-tuning.\n",
      "Experiment 8: Training Diffusion Model\n",
      "Epoch 1/10, Loss: 0.1070\n",
      "Epoch 2/10, Loss: 0.0910\n",
      "Epoch 3/10, Loss: 0.0777\n",
      "Epoch 4/10, Loss: 0.0682\n",
      "Epoch 5/10, Loss: 0.0614\n",
      "Epoch 6/10, Loss: 0.0525\n",
      "Epoch 7/10, Loss: 0.0496\n",
      "Epoch 8/10, Loss: 0.0458\n",
      "Epoch 9/10, Loss: 0.0405\n",
      "Epoch 10/10, Loss: 0.0395\n",
      "Saved model for experiment 8.\n",
      "Validation Evaluation:\n",
      "Evaluation Accuracy: 0.5903\n",
      "Test Evaluation:\n",
      "Evaluation Accuracy: 0.6005\n",
      "\n",
      "Experiment 9: 데이터 구간 [288000:608000]\n",
      "Loaded model from experiment 8 for fine-tuning.\n",
      "Experiment 9: Training Diffusion Model\n",
      "Epoch 1/10, Loss: 0.1086\n",
      "Epoch 2/10, Loss: 0.0897\n",
      "Epoch 3/10, Loss: 0.0792\n",
      "Epoch 4/10, Loss: 0.0681\n",
      "Epoch 5/10, Loss: 0.0622\n",
      "Epoch 6/10, Loss: 0.0547\n",
      "Epoch 7/10, Loss: 0.0489\n",
      "Epoch 8/10, Loss: 0.0473\n",
      "Epoch 9/10, Loss: 0.0427\n",
      "Epoch 10/10, Loss: 0.0400\n",
      "Saved model for experiment 9.\n",
      "Validation Evaluation:\n",
      "Evaluation Accuracy: 0.6064\n",
      "Test Evaluation:\n",
      "Evaluation Accuracy: 0.6135\n",
      "\n",
      "Experiment 10: 데이터 구간 [320000:640000]\n",
      "Loaded model from experiment 9 for fine-tuning.\n",
      "Experiment 10: Training Diffusion Model\n",
      "Epoch 1/10, Loss: 0.1056\n",
      "Epoch 2/10, Loss: 0.0913\n",
      "Epoch 3/10, Loss: 0.0798\n",
      "Epoch 4/10, Loss: 0.0722\n",
      "Epoch 5/10, Loss: 0.0634\n",
      "Epoch 6/10, Loss: 0.0584\n",
      "Epoch 7/10, Loss: 0.0534\n",
      "Epoch 8/10, Loss: 0.0495\n",
      "Epoch 9/10, Loss: 0.0456\n",
      "Epoch 10/10, Loss: 0.0423\n",
      "Saved model for experiment 10.\n",
      "Validation Evaluation:\n",
      "Evaluation Accuracy: 0.6088\n",
      "Test Evaluation:\n",
      "Evaluation Accuracy: 0.6141\n",
      "\n",
      "Experiment 11: 데이터 구간 [352000:672000]\n",
      "Loaded model from experiment 10 for fine-tuning.\n",
      "Experiment 11: Training Diffusion Model\n",
      "Epoch 1/10, Loss: 0.1047\n",
      "Epoch 2/10, Loss: 0.0912\n",
      "Epoch 3/10, Loss: 0.0820\n",
      "Epoch 4/10, Loss: 0.0753\n",
      "Epoch 5/10, Loss: 0.0676\n",
      "Epoch 6/10, Loss: 0.0629\n",
      "Epoch 7/10, Loss: 0.0591\n",
      "Epoch 8/10, Loss: 0.0537\n",
      "Epoch 9/10, Loss: 0.0503\n",
      "Epoch 10/10, Loss: 0.0469\n",
      "Saved model for experiment 11.\n",
      "Validation Evaluation:\n",
      "Evaluation Accuracy: 0.6167\n",
      "Test Evaluation:\n",
      "Evaluation Accuracy: 0.5989\n",
      "\n",
      "Experiment 12: 데이터 구간 [384000:704000]\n",
      "Loaded model from experiment 11 for fine-tuning.\n",
      "Experiment 12: Training Diffusion Model\n",
      "Epoch 1/10, Loss: 0.1027\n",
      "Epoch 2/10, Loss: 0.0928\n",
      "Epoch 3/10, Loss: 0.0865\n",
      "Epoch 4/10, Loss: 0.0797\n",
      "Epoch 5/10, Loss: 0.0729\n",
      "Epoch 6/10, Loss: 0.0701\n",
      "Epoch 7/10, Loss: 0.0651\n",
      "Epoch 8/10, Loss: 0.0623\n",
      "Epoch 9/10, Loss: 0.0587\n",
      "Epoch 10/10, Loss: 0.0547\n",
      "Saved model for experiment 12.\n",
      "Validation Evaluation:\n",
      "Evaluation Accuracy: 0.5989\n",
      "Test Evaluation:\n",
      "Evaluation Accuracy: 0.5871\n",
      "\n",
      "Experiment 13: 데이터 구간 [416000:736000]\n",
      "Loaded model from experiment 12 for fine-tuning.\n",
      "Experiment 13: Training Diffusion Model\n",
      "Epoch 1/10, Loss: 0.1130\n",
      "Epoch 2/10, Loss: 0.0994\n",
      "Epoch 3/10, Loss: 0.0878\n",
      "Epoch 4/10, Loss: 0.0794\n",
      "Epoch 5/10, Loss: 0.0714\n",
      "Epoch 6/10, Loss: 0.0657\n",
      "Epoch 7/10, Loss: 0.0598\n",
      "Epoch 8/10, Loss: 0.0560\n",
      "Epoch 9/10, Loss: 0.0531\n",
      "Epoch 10/10, Loss: 0.0490\n",
      "Saved model for experiment 13.\n",
      "Validation Evaluation:\n",
      "Evaluation Accuracy: 0.5841\n",
      "Test Evaluation:\n",
      "Evaluation Accuracy: 0.5869\n",
      "\n",
      "Experiment 14: 데이터 구간 [448000:768000]\n",
      "Loaded model from experiment 13 for fine-tuning.\n",
      "Experiment 14: Training Diffusion Model\n",
      "Epoch 1/10, Loss: 0.1093\n",
      "Epoch 2/10, Loss: 0.0944\n",
      "Epoch 3/10, Loss: 0.0830\n",
      "Epoch 4/10, Loss: 0.0747\n",
      "Epoch 5/10, Loss: 0.0664\n",
      "Epoch 6/10, Loss: 0.0597\n",
      "Epoch 7/10, Loss: 0.0552\n",
      "Epoch 8/10, Loss: 0.0506\n",
      "Epoch 9/10, Loss: 0.0484\n",
      "Epoch 10/10, Loss: 0.0456\n",
      "Saved model for experiment 14.\n",
      "Validation Evaluation:\n",
      "Evaluation Accuracy: 0.5863\n",
      "Test Evaluation:\n",
      "Evaluation Accuracy: 0.5864\n",
      "\n",
      "Final Average Validation Accuracy: 0.5952\n",
      "Final Average Test Accuracy: 0.5956\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# rolling minmax scaling 함수 (window=24)\n",
    "def rolling_minmax_scale(series, window=12):\n",
    "    roll_min = series.rolling(window=window, min_periods=window).min()\n",
    "    roll_max = series.rolling(window=window, min_periods=window).max()\n",
    "    scaled = (series - roll_min) / ((roll_max - roll_min) + 1e-8)\n",
    "    scaled = scaled.replace([np.inf, -np.inf], np.nan)\n",
    "    scaled = scaled.fillna(1.0)\n",
    "    return scaled.clip(upper=1.0)\n",
    "\n",
    "# binning 및 one-hot 인코딩 함수 (결과를 정수 0,1로)\n",
    "def bin_and_encode(data, features, bins=100, drop_original=True):\n",
    "    for feature in features:\n",
    "        data[f'{feature}_Bin'] = pd.cut(data[feature], bins=bins, labels=False)\n",
    "        one_hot = pd.get_dummies(data[f'{feature}_Bin'], prefix=f'{feature}_Bin').astype(np.int32)\n",
    "        expected_columns = [f'{feature}_Bin_{i}' for i in range(bins)]\n",
    "        one_hot = one_hot.reindex(columns=expected_columns, fill_value=0)\n",
    "        data = pd.concat([data, one_hot], axis=1)\n",
    "        if drop_original:\n",
    "            data.drop(columns=[f'{feature}_Bin'], inplace=True)\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    for col in numeric_cols:\n",
    "        data[col] = data[col].astype(np.float32)\n",
    "    return data\n",
    "\n",
    "##############################################\n",
    "# Diffusion 모델을 위한 Dataset 정의\n",
    "##############################################\n",
    "class DiffusionTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, input_data, target_data, lookback=12):\n",
    "        self.input_data = input_data.values\n",
    "        self.target_data = target_data.values\n",
    "        self.lookback = lookback\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_data) - self.lookback\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 조건 데이터: lookback window\n",
    "        x = self.input_data[idx: idx + self.lookback, :]\n",
    "        # 타깃: close_target 값을 이용해 상승이면 1, 하락이면 0으로 설정 (float형)\n",
    "        y = self.target_data[idx + self.lookback, 0]\n",
    "        y_prev = self.target_data[idx + self.lookback - 1, 0]\n",
    "        label = 1.0 if y > y_prev else 0.0\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor([label], dtype=torch.float32)\n",
    "\n",
    "##############################################\n",
    "# Condition Encoder: 시계열 window를 벡터로 인코딩\n",
    "##############################################\n",
    "class ConditionEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, lookback, condition_dim):\n",
    "        super(ConditionEncoder, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim * lookback, condition_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(condition_dim, condition_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch, lookback, input_dim]\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "##############################################\n",
    "# DiffusionClassifier: diffusion process를 통한 노이즈 예측 모델\n",
    "##############################################\n",
    "class DiffusionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, lookback, condition_dim=128, num_timesteps=100, hidden_dim=128):\n",
    "        super(DiffusionClassifier, self).__init__()\n",
    "        self.num_timesteps = num_timesteps\n",
    "        # diffusion 스케줄 (선형 beta schedule)\n",
    "        betas = torch.linspace(1e-4, 0.02, num_timesteps)\n",
    "        alphas = 1 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas', alphas)\n",
    "        self.register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        \n",
    "        # 조건 인코더: 시계열 데이터를 조건으로 임베딩\n",
    "        self.condition_encoder = ConditionEncoder(input_dim, lookback, condition_dim)\n",
    "        # timestep 임베딩\n",
    "        self.time_embedding = nn.Embedding(num_timesteps, hidden_dim)\n",
    "        # 노이즈 예측 네트워크: 입력은 [y_noisy, condition, timestep embedding]\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(1 + condition_dim + hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_condition, y_noisy, t):\n",
    "        # x_condition: [batch, lookback, input_dim]\n",
    "        # y_noisy: [batch, 1] - 노이즈가 추가된 타깃\n",
    "        # t: [batch] - timestep 인덱스\n",
    "        cond = self.condition_encoder(x_condition)           # [batch, condition_dim]\n",
    "        t_emb = self.time_embedding(t)                         # [batch, hidden_dim]\n",
    "        inp = torch.cat([y_noisy, cond, t_emb], dim=1)          # [batch, 1+condition_dim+hidden_dim]\n",
    "        predicted_noise = self.model(inp)                      # [batch, 1]\n",
    "        return predicted_noise\n",
    "    \n",
    "    def sample(self, x_condition, device):\n",
    "        \"\"\"\n",
    "        reverse diffusion 과정을 통해 조건 x_condition에 대해 예측값을 샘플링\n",
    "        최종 출력은 continuous 값으로, 임계값 0.5로 분류 가능함.\n",
    "        \"\"\"\n",
    "        batch_size = x_condition.size(0)\n",
    "        # 초기 y: 정규분포 노이즈\n",
    "        y = torch.randn(batch_size, 1, device=device)\n",
    "        for t in reversed(range(self.num_timesteps)):\n",
    "            t_tensor = torch.full((batch_size,), t, device=device, dtype=torch.long)\n",
    "            predicted_noise = self.forward(x_condition, y, t_tensor)\n",
    "            alpha = self.alphas[t]\n",
    "            alpha_cumprod = self.alphas_cumprod[t]\n",
    "            beta = self.betas[t]\n",
    "            # DDPM 업데이트: 간단화된 형태\n",
    "            y = (1 / torch.sqrt(alpha)) * (y - (beta / torch.sqrt(1 - alpha_cumprod)) * predicted_noise)\n",
    "            # t > 0이면 약간의 노이즈 추가\n",
    "            if t > 0:\n",
    "                noise = torch.randn_like(y)\n",
    "                y = y + torch.sqrt(beta) * noise\n",
    "        return y\n",
    "\n",
    "##############################################\n",
    "# Diffusion 모델 학습 및 평가 함수\n",
    "##############################################\n",
    "def train_diffusion_model(model, dataloader, num_epochs, device, lr=1e-4):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    mse_loss = nn.MSELoss()\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device)  # [batch, lookback, input_dim]\n",
    "            y = y.to(device)  # [batch, 1] (0.0 or 1.0)\n",
    "            batch_size = x.size(0)\n",
    "            # 각 배치마다 timestep t를 균등 샘플링\n",
    "            t = torch.randint(0, model.num_timesteps, (batch_size,), device=device).long()\n",
    "            # 해당 timestep에 따른 누적 알파값\n",
    "            alphas_cumprod_t = model.alphas_cumprod[t].view(batch_size, 1)\n",
    "            # 노이즈 샘플링\n",
    "            noise = torch.randn_like(y)\n",
    "            # y_noisy = sqrt(alpha_cumprod)*y + sqrt(1-alpha_cumprod)*noise\n",
    "            y_noisy = torch.sqrt(alphas_cumprod_t) * y + torch.sqrt(1 - alphas_cumprod_t) * noise\n",
    "            optimizer.zero_grad()\n",
    "            predicted_noise = model(x, y_noisy, t)\n",
    "            loss = mse_loss(predicted_noise, noise)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "def evaluate_diffusion_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            # reverse diffusion 과정을 통해 예측값 샘플링\n",
    "            y_sampled = model.sample(x, device)\n",
    "            # 0.5 기준으로 분류\n",
    "            y_pred = (y_sampled >= 0.5).float()\n",
    "            correct += (y_pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    acc = correct / total\n",
    "    print(f\"Evaluation Accuracy: {acc:.4f}\")\n",
    "    return acc\n",
    "\n",
    "##############################################\n",
    "# 데이터 로드 및 전처리 (OHLC 4개 데이터 사용)\n",
    "##############################################\n",
    "data = pd.read_csv(\"ETH_upbit_KRW_min5_0309.csv\", index_col=0)\n",
    "data = data[['open', 'high', 'low', 'close']]\n",
    "data.index = pd.to_datetime(data.index)\n",
    "\n",
    "ohlc_features = ['open', 'high', 'low', 'close']\n",
    "for feature in ohlc_features:\n",
    "    data[feature] = rolling_minmax_scale(data[feature], window=12)\n",
    "\n",
    "data = bin_and_encode(data, ohlc_features, bins=100, drop_original=True)\n",
    "# 타깃은 원본 close 값 사용 (실험 목적)\n",
    "data['close_target'] = data['close']\n",
    "data = data.dropna()\n",
    "\n",
    "# 최종 입력: _Bin_ 접미사가 있는 열들만 사용\n",
    "final_input_columns = [col for col in data.columns if '_Bin_' in col]\n",
    "final_target_column = ['close_target']\n",
    "\n",
    "data_input = data[final_input_columns]\n",
    "data_target = data[final_target_column]\n",
    "\n",
    "##############################################\n",
    "# 실험 실행: Diffusion Model 기반 주가 상승/하락 예측\n",
    "##############################################\n",
    "def train_and_evaluate_diffusion(data, num_experiments=16, lookback=12, num_epochs=10):\n",
    "    final_input_columns = [col for col in data.columns if '_Bin_' in col]\n",
    "    target_cols = ['close_target']\n",
    "    \n",
    "    data_input = data[final_input_columns]\n",
    "    data_target = data[target_cols]\n",
    "    \n",
    "    data_input = data_input.apply(pd.to_numeric).astype(np.float32)\n",
    "    data_target = data_target.apply(pd.to_numeric).astype(np.float32)\n",
    "    \n",
    "    step_size = 32000\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    val_acc_list = []\n",
    "    test_acc_list = []\n",
    "    \n",
    "    for exp in range(num_experiments):\n",
    "        train_start = exp * step_size\n",
    "        train_end = train_start + step_size * 8\n",
    "        val_end = train_end + step_size\n",
    "        test_end = val_end + step_size\n",
    "        if test_end > len(data_input):\n",
    "            break\n",
    "        print(f\"\\nExperiment {exp}: 데이터 구간 [{train_start}:{test_end}]\")\n",
    "        \n",
    "        train_input = data_input.iloc[train_start:train_end]\n",
    "        train_target = data_target.iloc[train_start:train_end]\n",
    "        val_input = data_input.iloc[train_end:val_end]\n",
    "        val_target = data_target.iloc[train_end:val_end]\n",
    "        test_input = data_input.iloc[val_end:test_end]\n",
    "        test_target = data_target.iloc[val_end:test_end]\n",
    "        \n",
    "        train_dataset = DiffusionTimeSeriesDataset(train_input, train_target, lookback=lookback)\n",
    "        val_dataset = DiffusionTimeSeriesDataset(val_input, val_target, lookback=lookback)\n",
    "        test_dataset = DiffusionTimeSeriesDataset(test_input, test_target, lookback=lookback)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        input_dim = train_input.shape[1]\n",
    "        model = DiffusionClassifier(input_dim=input_dim, lookback=lookback, \n",
    "                                    condition_dim=128, num_timesteps=100, hidden_dim=128).to(device)\n",
    "        model_path = f\"diffusion_model_experiment_{exp}.pth\"\n",
    "        if exp > 0:\n",
    "            try:\n",
    "                model.load_state_dict(torch.load(f\"diffusion_model_experiment_{exp - 1}.pth\"))\n",
    "                print(f\"Loaded model from experiment {exp - 1} for fine-tuning.\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Model file for experiment {exp - 1} not found. Starting fresh training.\")\n",
    "        \n",
    "        print(f\"Experiment {exp}: Training Diffusion Model\")\n",
    "        train_diffusion_model(model, train_loader, num_epochs, device, lr=1e-4)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Saved model for experiment {exp}.\")\n",
    "        \n",
    "        print(\"Validation Evaluation:\")\n",
    "        val_acc = evaluate_diffusion_model(model, val_loader, device)\n",
    "        val_acc_list.append(val_acc)\n",
    "        \n",
    "        print(\"Test Evaluation:\")\n",
    "        test_acc = evaluate_diffusion_model(model, test_loader, device)\n",
    "        test_acc_list.append(test_acc)\n",
    "    \n",
    "    if val_acc_list:\n",
    "        avg_val_acc = sum(val_acc_list) / len(val_acc_list)\n",
    "        avg_test_acc = sum(test_acc_list) / len(test_acc_list)\n",
    "        print(f\"\\nFinal Average Validation Accuracy: {avg_val_acc:.4f}\")\n",
    "        print(f\"Final Average Test Accuracy: {avg_test_acc:.4f}\")\n",
    "    else:\n",
    "        print(\"실험이 한 번도 실행되지 않았습니다.\")\n",
    "\n",
    "# 최종적으로 Diffusion Model 실험 실행\n",
    "train_and_evaluate_diffusion(data, num_experiments=15, lookback=12, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14968\\2195934350.py:185: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['close_target'] = data['close_scaled']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experiment 0: 데이터 구간 [0:312000]\n",
      "Experiment 0: Training Diffusion Model\n",
      "Epoch 1/10, Loss: 0.368460\n",
      "Epoch 2/10, Loss: 0.251631\n",
      "Epoch 3/10, Loss: 0.232721\n",
      "Epoch 4/10, Loss: 0.221808\n",
      "Epoch 5/10, Loss: 0.209884\n",
      "Epoch 6/10, Loss: 0.197628\n",
      "Epoch 7/10, Loss: 0.189020\n",
      "Epoch 8/10, Loss: 0.180835\n",
      "Epoch 9/10, Loss: 0.172085\n",
      "Epoch 10/10, Loss: 0.165783\n",
      "Saved model for experiment 0.\n",
      "Validation Evaluation:\n",
      "Evaluation MSE: 0.050323, Accuracy: 0.5410\n",
      "Test Evaluation:\n",
      "Evaluation MSE: 0.051397, Accuracy: 0.5469\n",
      "Experiment 0: Validation MSE: 0.050323, val_Accuracy: 0.5410, test_Accuracy: 0.5469\n",
      "\n",
      "Experiment 1: 데이터 구간 [31200:343200]\n",
      "Loaded model from experiment 0 for fine-tuning.\n",
      "Experiment 1: Training Diffusion Model\n",
      "Epoch 1/10, Loss: 0.168132\n",
      "Epoch 2/10, Loss: 0.158581\n",
      "Epoch 3/10, Loss: 0.148966\n",
      "Epoch 4/10, Loss: 0.142342\n",
      "Epoch 5/10, Loss: 0.136289\n",
      "Epoch 6/10, Loss: 0.132642\n",
      "Epoch 7/10, Loss: 0.128058\n",
      "Epoch 8/10, Loss: 0.121237\n",
      "Epoch 9/10, Loss: 0.118595\n",
      "Epoch 10/10, Loss: 0.113431\n",
      "Saved model for experiment 1.\n",
      "Validation Evaluation:\n",
      "Evaluation MSE: 0.045490, Accuracy: 0.5428\n",
      "Test Evaluation:\n",
      "Evaluation MSE: 0.045410, Accuracy: 0.5428\n",
      "Experiment 1: Validation MSE: 0.045490, val_Accuracy: 0.5428, test_Accuracy: 0.5428\n",
      "\n",
      "Experiment 2: 데이터 구간 [62400:374400]\n",
      "Loaded model from experiment 1 for fine-tuning.\n",
      "Experiment 2: Training Diffusion Model\n",
      "Epoch 1/10, Loss: 0.138456\n",
      "Epoch 2/10, Loss: 0.128559\n",
      "Epoch 3/10, Loss: 0.124374\n",
      "Epoch 4/10, Loss: 0.116805\n",
      "Epoch 5/10, Loss: 0.112082\n",
      "Epoch 6/10, Loss: 0.110472\n",
      "Epoch 7/10, Loss: 0.106042\n",
      "Epoch 8/10, Loss: 0.102573\n",
      "Epoch 9/10, Loss: 0.101824\n",
      "Epoch 10/10, Loss: 0.098687\n",
      "Saved model for experiment 2.\n",
      "Validation Evaluation:\n",
      "Evaluation MSE: 0.042195, Accuracy: 0.5409\n",
      "Test Evaluation:\n",
      "Evaluation MSE: 0.040809, Accuracy: 0.5469\n",
      "Experiment 2: Validation MSE: 0.042195, val_Accuracy: 0.5409, test_Accuracy: 0.5469\n",
      "\n",
      "Experiment 3: 데이터 구간 [93600:405600]\n",
      "Loaded model from experiment 2 for fine-tuning.\n",
      "Experiment 3: Training Diffusion Model\n",
      "Epoch 1/10, Loss: 0.127453\n",
      "Epoch 2/10, Loss: 0.116312\n",
      "Epoch 3/10, Loss: 0.112384\n",
      "Epoch 4/10, Loss: 0.106576\n",
      "Epoch 5/10, Loss: 0.103341\n",
      "Epoch 6/10, Loss: 0.098902\n",
      "Epoch 7/10, Loss: 0.097087\n",
      "Epoch 8/10, Loss: 0.094571\n",
      "Epoch 9/10, Loss: 0.091644\n",
      "Epoch 10/10, Loss: 0.089576\n",
      "Saved model for experiment 3.\n",
      "Validation Evaluation:\n",
      "Evaluation MSE: 0.039766, Accuracy: 0.5280\n",
      "Test Evaluation:\n",
      "Evaluation MSE: 0.038142, Accuracy: 0.5305\n",
      "Experiment 3: Validation MSE: 0.039766, val_Accuracy: 0.5280, test_Accuracy: 0.5305\n",
      "\n",
      "Experiment 4: 데이터 구간 [124800:436800]\n",
      "Loaded model from experiment 3 for fine-tuning.\n",
      "Experiment 4: Training Diffusion Model\n",
      "Epoch 1/10, Loss: 0.119985\n",
      "Epoch 2/10, Loss: 0.112576\n",
      "Epoch 3/10, Loss: 0.105616\n",
      "Epoch 4/10, Loss: 0.100611\n",
      "Epoch 5/10, Loss: 0.097003\n",
      "Epoch 6/10, Loss: 0.093556\n",
      "Epoch 7/10, Loss: 0.090262\n",
      "Epoch 8/10, Loss: 0.088206\n",
      "Epoch 9/10, Loss: 0.086198\n",
      "Epoch 10/10, Loss: 0.085626\n",
      "Saved model for experiment 4.\n",
      "Validation Evaluation:\n",
      "Evaluation MSE: 0.037574, Accuracy: 0.5402\n",
      "Test Evaluation:\n",
      "Evaluation MSE: 0.039903, Accuracy: 0.5363\n",
      "Experiment 4: Validation MSE: 0.037574, val_Accuracy: 0.5402, test_Accuracy: 0.5363\n",
      "\n",
      "Experiment 5: 데이터 구간 [156000:468000]\n",
      "Loaded model from experiment 4 for fine-tuning.\n",
      "Experiment 5: Training Diffusion Model\n",
      "Epoch 1/10, Loss: 0.117324\n",
      "Epoch 2/10, Loss: 0.107533\n",
      "Epoch 3/10, Loss: 0.101312\n",
      "Epoch 4/10, Loss: 0.096742\n",
      "Epoch 5/10, Loss: 0.091993\n",
      "Epoch 6/10, Loss: 0.089638\n",
      "Epoch 7/10, Loss: 0.086938\n",
      "Epoch 8/10, Loss: 0.084810\n",
      "Epoch 9/10, Loss: 0.081590\n",
      "Epoch 10/10, Loss: 0.080718\n",
      "Saved model for experiment 5.\n",
      "Validation Evaluation:\n",
      "Evaluation MSE: 0.039990, Accuracy: 0.5391\n",
      "Test Evaluation:\n",
      "Evaluation MSE: 0.038122, Accuracy: 0.5388\n",
      "Experiment 5: Validation MSE: 0.039990, val_Accuracy: 0.5391, test_Accuracy: 0.5388\n",
      "\n",
      "Experiment 6: 데이터 구간 [187200:499200]\n",
      "Loaded model from experiment 5 for fine-tuning.\n",
      "Experiment 6: Training Diffusion Model\n",
      "Epoch 1/10, Loss: 0.117095\n",
      "Epoch 2/10, Loss: 0.105530\n",
      "Epoch 3/10, Loss: 0.099612\n",
      "Epoch 4/10, Loss: 0.094598\n",
      "Epoch 5/10, Loss: 0.089995\n",
      "Epoch 6/10, Loss: 0.088900\n",
      "Epoch 7/10, Loss: 0.084644\n",
      "Epoch 8/10, Loss: 0.081978\n",
      "Epoch 9/10, Loss: 0.079382\n",
      "Epoch 10/10, Loss: 0.078280\n",
      "Saved model for experiment 6.\n",
      "Validation Evaluation:\n",
      "Evaluation MSE: 0.037513, Accuracy: 0.5414\n",
      "Test Evaluation:\n",
      "Evaluation MSE: 0.038710, Accuracy: 0.5411\n",
      "Experiment 6: Validation MSE: 0.037513, val_Accuracy: 0.5414, test_Accuracy: 0.5411\n",
      "\n",
      "Experiment 7: 데이터 구간 [218400:530400]\n",
      "Loaded model from experiment 6 for fine-tuning.\n",
      "Experiment 7: Training Diffusion Model\n",
      "Epoch 1/10, Loss: 0.111501\n",
      "Epoch 2/10, Loss: 0.103079\n",
      "Epoch 3/10, Loss: 0.097473\n",
      "Epoch 4/10, Loss: 0.092592\n",
      "Epoch 5/10, Loss: 0.088685\n",
      "Epoch 6/10, Loss: 0.086290\n",
      "Epoch 7/10, Loss: 0.081597\n",
      "Epoch 8/10, Loss: 0.079489\n",
      "Epoch 9/10, Loss: 0.077636\n",
      "Epoch 10/10, Loss: 0.076924\n",
      "Saved model for experiment 7.\n",
      "Validation Evaluation:\n",
      "Evaluation MSE: 0.038874, Accuracy: 0.5432\n",
      "Test Evaluation:\n",
      "Evaluation MSE: 0.040953, Accuracy: 0.5489\n",
      "Experiment 7: Validation MSE: 0.038874, val_Accuracy: 0.5432, test_Accuracy: 0.5489\n",
      "\n",
      "Experiment 8: 데이터 구간 [249600:561600]\n",
      "Loaded model from experiment 7 for fine-tuning.\n",
      "Experiment 8: Training Diffusion Model\n",
      "Epoch 1/10, Loss: 0.111761\n",
      "Epoch 2/10, Loss: 0.102330\n",
      "Epoch 3/10, Loss: 0.096278\n",
      "Epoch 4/10, Loss: 0.092280\n",
      "Epoch 5/10, Loss: 0.087983\n",
      "Epoch 6/10, Loss: 0.086153\n",
      "Epoch 7/10, Loss: 0.081198\n",
      "Epoch 8/10, Loss: 0.079881\n",
      "Epoch 9/10, Loss: 0.078410\n",
      "Epoch 10/10, Loss: 0.075139\n",
      "Saved model for experiment 8.\n",
      "Validation Evaluation:\n",
      "Evaluation MSE: 0.041078, Accuracy: 0.5367\n",
      "Test Evaluation:\n",
      "Evaluation MSE: 0.043403, Accuracy: 0.5506\n",
      "Experiment 8: Validation MSE: 0.041078, val_Accuracy: 0.5367, test_Accuracy: 0.5506\n",
      "\n",
      "Experiment 9: 데이터 구간 [280800:592800]\n",
      "Loaded model from experiment 8 for fine-tuning.\n",
      "Experiment 9: Training Diffusion Model\n",
      "Epoch 1/10, Loss: 0.111666\n",
      "Epoch 2/10, Loss: 0.104653\n",
      "Epoch 3/10, Loss: 0.097073\n",
      "Epoch 4/10, Loss: 0.093292\n",
      "Epoch 5/10, Loss: 0.088614\n",
      "Epoch 6/10, Loss: 0.084920\n",
      "Epoch 7/10, Loss: 0.082107\n",
      "Epoch 8/10, Loss: 0.080194\n",
      "Epoch 9/10, Loss: 0.076871\n",
      "Epoch 10/10, Loss: 0.074364\n",
      "Saved model for experiment 9.\n",
      "Validation Evaluation:\n",
      "Evaluation MSE: 0.044548, Accuracy: 0.5477\n",
      "Test Evaluation:\n",
      "Evaluation MSE: 0.046314, Accuracy: 0.5525\n",
      "Experiment 9: Validation MSE: 0.044548, val_Accuracy: 0.5477, test_Accuracy: 0.5525\n",
      "\n",
      "Experiment 10: 데이터 구간 [312000:624000]\n",
      "Loaded model from experiment 9 for fine-tuning.\n",
      "Experiment 10: Training Diffusion Model\n",
      "Epoch 1/10, Loss: 0.113920\n",
      "Epoch 2/10, Loss: 0.103300\n",
      "Epoch 3/10, Loss: 0.098090\n",
      "Epoch 4/10, Loss: 0.092743\n",
      "Epoch 5/10, Loss: 0.090886\n",
      "Epoch 6/10, Loss: 0.085456\n",
      "Epoch 7/10, Loss: 0.082864\n",
      "Epoch 8/10, Loss: 0.078750\n",
      "Epoch 9/10, Loss: 0.077537\n",
      "Epoch 10/10, Loss: 0.075062\n",
      "Saved model for experiment 10.\n",
      "Validation Evaluation:\n",
      "Evaluation MSE: 0.047962, Accuracy: 0.5569\n",
      "Test Evaluation:\n",
      "Evaluation MSE: 0.054790, Accuracy: 0.5634\n",
      "Experiment 10: Validation MSE: 0.047962, val_Accuracy: 0.5569, test_Accuracy: 0.5634\n",
      "\n",
      "Experiment 11: 데이터 구간 [343200:655200]\n",
      "Loaded model from experiment 10 for fine-tuning.\n",
      "Experiment 11: Training Diffusion Model\n",
      "Epoch 1/10, Loss: 0.113207\n",
      "Epoch 2/10, Loss: 0.104309\n",
      "Epoch 3/10, Loss: 0.099808\n",
      "Epoch 4/10, Loss: 0.093509\n",
      "Epoch 5/10, Loss: 0.090753\n",
      "Epoch 6/10, Loss: 0.087326\n",
      "Epoch 7/10, Loss: 0.083780\n",
      "Epoch 8/10, Loss: 0.082311\n",
      "Epoch 9/10, Loss: 0.079275\n",
      "Epoch 10/10, Loss: 0.076649\n",
      "Saved model for experiment 11.\n",
      "Validation Evaluation:\n",
      "Evaluation MSE: 0.056429, Accuracy: 0.5536\n",
      "Test Evaluation:\n",
      "Evaluation MSE: 0.046692, Accuracy: 0.5498\n",
      "Experiment 11: Validation MSE: 0.056429, val_Accuracy: 0.5536, test_Accuracy: 0.5498\n",
      "\n",
      "Experiment 12: 데이터 구간 [374400:686400]\n",
      "Loaded model from experiment 11 for fine-tuning.\n",
      "Experiment 12: Training Diffusion Model\n",
      "Epoch 1/10, Loss: 0.113028\n",
      "Epoch 2/10, Loss: 0.103678\n",
      "Epoch 3/10, Loss: 0.099523\n",
      "Epoch 4/10, Loss: 0.095439\n",
      "Epoch 5/10, Loss: 0.093188\n",
      "Epoch 6/10, Loss: 0.090977\n",
      "Epoch 7/10, Loss: 0.085956\n",
      "Epoch 8/10, Loss: 0.084617\n",
      "Epoch 9/10, Loss: 0.082438\n",
      "Epoch 10/10, Loss: 0.081648\n",
      "Saved model for experiment 12.\n",
      "Validation Evaluation:\n",
      "Evaluation MSE: 0.048201, Accuracy: 0.5513\n",
      "Test Evaluation:\n",
      "Evaluation MSE: 0.040365, Accuracy: 0.5377\n",
      "Experiment 12: Validation MSE: 0.048201, val_Accuracy: 0.5513, test_Accuracy: 0.5377\n",
      "\n",
      "Experiment 13: 데이터 구간 [405600:717600]\n",
      "Loaded model from experiment 12 for fine-tuning.\n",
      "Experiment 13: Training Diffusion Model\n",
      "Epoch 1/10, Loss: 0.117760\n",
      "Epoch 2/10, Loss: 0.109167\n",
      "Epoch 3/10, Loss: 0.104310\n",
      "Epoch 4/10, Loss: 0.099587\n",
      "Epoch 5/10, Loss: 0.095267\n",
      "Epoch 6/10, Loss: 0.091753\n",
      "Epoch 7/10, Loss: 0.088987\n",
      "Epoch 8/10, Loss: 0.087492\n",
      "Epoch 9/10, Loss: 0.083492\n",
      "Epoch 10/10, Loss: 0.081140\n",
      "Saved model for experiment 13.\n",
      "Validation Evaluation:\n",
      "Evaluation MSE: 0.041076, Accuracy: 0.5452\n",
      "Test Evaluation:\n",
      "Evaluation MSE: 0.040798, Accuracy: 0.5416\n",
      "Experiment 13: Validation MSE: 0.041076, val_Accuracy: 0.5452, test_Accuracy: 0.5416\n",
      "\n",
      "Experiment 14: 데이터 구간 [436800:748800]\n",
      "Loaded model from experiment 13 for fine-tuning.\n",
      "Experiment 14: Training Diffusion Model\n",
      "Epoch 1/10, Loss: 0.120303\n",
      "Epoch 2/10, Loss: 0.110655\n",
      "Epoch 3/10, Loss: 0.104176\n",
      "Epoch 4/10, Loss: 0.100440\n",
      "Epoch 5/10, Loss: 0.094707\n",
      "Epoch 6/10, Loss: 0.090459\n",
      "Epoch 7/10, Loss: 0.085984\n",
      "Epoch 8/10, Loss: 0.083752\n",
      "Epoch 9/10, Loss: 0.081443\n",
      "Epoch 10/10, Loss: 0.079923\n",
      "Saved model for experiment 14.\n",
      "Validation Evaluation:\n",
      "Evaluation MSE: 0.041792, Accuracy: 0.5374\n",
      "Test Evaluation:\n",
      "Evaluation MSE: 0.042755, Accuracy: 0.5434\n",
      "Experiment 14: Validation MSE: 0.041792, val_Accuracy: 0.5374, test_Accuracy: 0.5434\n",
      "\n",
      "Final Average Validation MSE: 0.043521\n",
      "Final Average Test MSE: 0.043238\n",
      "Final Average Val Accuracy: 0.5430\n",
      "Final Average Test Accuracy: 0.5447\n",
      "\n",
      "총 수행 시간: 6159.92초\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from math import sqrt\n",
    "\n",
    "##############################################\n",
    "# 데이터 전처리: rolling minmax scaling 및 binning\n",
    "##############################################\n",
    "def rolling_minmax_scale(series, window=24):\n",
    "    roll_min = series.rolling(window=window, min_periods=window).min()\n",
    "    roll_max = series.rolling(window=window, min_periods=window).max()\n",
    "    scaled = (series - roll_min) / ((roll_max - roll_min) + 1e-8)\n",
    "    scaled = scaled.replace([np.inf, -np.inf], np.nan)\n",
    "    scaled = scaled.fillna(1.0)\n",
    "    return scaled.clip(upper=1.0)\n",
    "\n",
    "def bin_and_encode(data, features, bins=100, drop_original=True):\n",
    "    for feature in features:\n",
    "        data[f'{feature}_Bin'] = pd.cut(data[feature], bins=bins, labels=False)\n",
    "        one_hot = pd.get_dummies(data[f'{feature}_Bin'], prefix=f'{feature}_Bin').astype(np.int32)\n",
    "        expected_columns = [f'{feature}_Bin_{i}' for i in range(bins)]\n",
    "        one_hot = one_hot.reindex(columns=expected_columns, fill_value=0)\n",
    "        data = pd.concat([data, one_hot], axis=1)\n",
    "        if drop_original:\n",
    "            data.drop(columns=[f'{feature}_Bin'], inplace=True)\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    for col in numeric_cols:\n",
    "        data[col] = data[col].astype(np.float32)\n",
    "    return data\n",
    "\n",
    "##############################################\n",
    "# Diffusion 모델을 위한 Dataset 정의 (회귀 + 방향 평가용)\n",
    "##############################################\n",
    "class DiffusionTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, input_data, target_data, lookback=24):\n",
    "        self.input_data = input_data.values\n",
    "        self.target_data = target_data.values  # continuous target 값 (scaled close)\n",
    "        self.lookback = lookback\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_data) - self.lookback\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 조건 데이터: lookback window\n",
    "        x = self.input_data[idx: idx + self.lookback, :]\n",
    "        # 타깃: 다음 시점의 target (continuous)\n",
    "        target = self.target_data[idx + self.lookback, 0]\n",
    "        # 이전 시점의 target (분류 평가에 사용)\n",
    "        prev_target = self.target_data[idx + self.lookback - 1, 0]\n",
    "        return (torch.tensor(x, dtype=torch.float32),\n",
    "                torch.tensor([target], dtype=torch.float32),\n",
    "                torch.tensor([prev_target], dtype=torch.float32))\n",
    "\n",
    "##############################################\n",
    "# Condition Encoder: 시계열 window를 벡터로 인코딩\n",
    "##############################################\n",
    "class ConditionEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, lookback, condition_dim):\n",
    "        super(ConditionEncoder, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim * lookback, condition_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(condition_dim, condition_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "##############################################\n",
    "# DiffusionClassifier: 조건부 diffusion 모델 (회귀, 평가 시 방향 분류)\n",
    "##############################################\n",
    "class DiffusionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, lookback, condition_dim=128, num_timesteps=100, hidden_dim=128):\n",
    "        super(DiffusionClassifier, self).__init__()\n",
    "        self.num_timesteps = num_timesteps\n",
    "        betas = torch.linspace(1e-4, 0.02, num_timesteps)\n",
    "        alphas = 1 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas', alphas)\n",
    "        self.register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        \n",
    "        self.condition_encoder = ConditionEncoder(input_dim, lookback, condition_dim)\n",
    "        self.time_embedding = nn.Embedding(num_timesteps, hidden_dim)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(1 + condition_dim + hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_condition, y_noisy, t):\n",
    "        cond = self.condition_encoder(x_condition)\n",
    "        t_emb = self.time_embedding(t)\n",
    "        inp = torch.cat([y_noisy, cond, t_emb], dim=1)\n",
    "        predicted_noise = self.model(inp)\n",
    "        return predicted_noise\n",
    "    \n",
    "    def sample(self, x_condition, device):\n",
    "        batch_size = x_condition.size(0)\n",
    "        y = torch.randn(batch_size, 1, device=device)\n",
    "        for t in reversed(range(self.num_timesteps)):\n",
    "            t_tensor = torch.full((batch_size,), t, device=device, dtype=torch.long)\n",
    "            predicted_noise = self.forward(x_condition, y, t_tensor)\n",
    "            alpha = self.alphas[t]\n",
    "            alpha_cumprod = self.alphas_cumprod[t]\n",
    "            beta = self.betas[t]\n",
    "            y = (1 / torch.sqrt(alpha)) * (y - (beta / torch.sqrt(1 - alpha_cumprod)) * predicted_noise)\n",
    "            if t > 0:\n",
    "                noise = torch.randn_like(y)\n",
    "                y = y + torch.sqrt(beta) * noise\n",
    "        return y\n",
    "\n",
    "##############################################\n",
    "# Diffusion 모델 학습 및 평가 함수 (회귀 + 방향 평가)\n",
    "##############################################\n",
    "def train_diffusion_model(model, dataloader, num_epochs, device, lr=1e-4):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    mse_loss = nn.MSELoss()\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for x, y, _ in dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            batch_size = x.size(0)\n",
    "            t = torch.randint(0, model.num_timesteps, (batch_size,), device=device).long()\n",
    "            alphas_cumprod_t = model.alphas_cumprod[t].view(batch_size, 1)\n",
    "            noise = torch.randn_like(y)\n",
    "            y_noisy = torch.sqrt(alphas_cumprod_t) * y + torch.sqrt(1 - alphas_cumprod_t) * noise\n",
    "            optimizer.zero_grad()\n",
    "            predicted_noise = model(x, y_noisy, t)\n",
    "            loss = mse_loss(predicted_noise, noise)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.6f}\")\n",
    "\n",
    "def evaluate_diffusion_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    mse_loss = nn.MSELoss()\n",
    "    total_mse = 0\n",
    "    total_samples = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y, y_prev in dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_prev = y_prev.to(device)\n",
    "            y_sampled = model.sample(x, device)\n",
    "            loss = mse_loss(y_sampled, y)\n",
    "            total_mse += loss.item() * y.size(0)\n",
    "            total_samples += y.size(0)\n",
    "            # 예측된 방향: 1 if 예측값 > y_prev, 0 otherwise\n",
    "            y_pred_class = (y_sampled > y_prev).float()\n",
    "            # 실제 방향: 1 if y > y_prev, else 0\n",
    "            y_true_class = (y > y_prev).float()\n",
    "            correct += (y_pred_class == y_true_class).sum().item()\n",
    "    avg_mse = total_mse / total_samples\n",
    "    accuracy = correct / total_samples\n",
    "    print(f\"Evaluation MSE: {avg_mse:.6f}, Accuracy: {accuracy:.4f}\")\n",
    "    return avg_mse, accuracy\n",
    "\n",
    "##############################################\n",
    "# 데이터 로드 및 전처리 (OHLC 값 사용: 원본 값에 scaling 후 인코딩)\n",
    "##############################################\n",
    "data = pd.read_csv(\"ETH_upbit_KRW_min5_0309.csv\", index_col=0)\n",
    "data.index = pd.to_datetime(data.index)\n",
    "data = data[['open', 'high', 'low', 'close']]\n",
    "\n",
    "# 각 OHLC에 대해 rolling minmax scaling 적용 후 새 컬럼 생성 (scaled 값)\n",
    "for feature in ['open', 'high', 'low', 'close']:\n",
    "    data[feature + '_scaled'] = rolling_minmax_scale(data[feature], window=24)\n",
    "data = data.dropna()\n",
    "\n",
    "# one-hot 인코딩: _scaled 컬럼 대상 (각 100구간 → 총 400차원)\n",
    "features_to_bin = ['open_scaled', 'high_scaled', 'low_scaled', 'close_scaled']\n",
    "data = bin_and_encode(data, features_to_bin, bins=100, drop_original=True)\n",
    "\n",
    "# 타깃: close_scaled 컬럼을 그대로 사용 (continuous regression target)\n",
    "data['close_target'] = data['close_scaled']\n",
    "data = data.dropna()\n",
    "\n",
    "# 최종 입력: '_scaled_Bin_'가 포함된 열들만 선택\n",
    "final_input_columns = [col for col in data.columns if '_scaled_Bin_' in col]\n",
    "final_target_column = ['close_target']\n",
    "\n",
    "data_input = data[final_input_columns]\n",
    "data_target = data[final_target_column]\n",
    "\n",
    "##############################################\n",
    "# 실험 실행: Diffusion Model 기반 주가 예측 (회귀 + 방향 평가)\n",
    "##############################################\n",
    "def train_and_evaluate_diffusion(data, num_experiments=16, lookback=24, num_epochs=10):\n",
    "    final_input_columns = [col for col in data.columns if '_scaled_Bin_' in col]\n",
    "    target_cols = ['close_target']\n",
    "    \n",
    "    data_input = data[final_input_columns]\n",
    "    data_target = data[target_cols]\n",
    "    \n",
    "    data_input = data_input.apply(pd.to_numeric).astype(np.float32)\n",
    "    data_target = data_target.apply(pd.to_numeric).astype(np.float32)\n",
    "    \n",
    "    step_size = 31200\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    val_mse_list = []\n",
    "    test_mse_list = []\n",
    "    val_acc_list = []\n",
    "    test_acc_list = []\n",
    "    \n",
    "    for exp in range(num_experiments):\n",
    "        train_start = exp * step_size\n",
    "        train_end = train_start + step_size * 8\n",
    "        val_end = train_end + step_size\n",
    "        test_end = val_end + step_size\n",
    "        if test_end > len(data_input):\n",
    "            break\n",
    "        print(f\"\\nExperiment {exp}: 데이터 구간 [{train_start}:{test_end}]\")\n",
    "        \n",
    "        train_input = data_input.iloc[train_start:train_end]\n",
    "        train_target = data_target.iloc[train_start:train_end]\n",
    "        val_input = data_input.iloc[train_end:val_end]\n",
    "        val_target = data_target.iloc[train_end:val_end]\n",
    "        test_input = data_input.iloc[val_end:test_end]\n",
    "        test_target = data_target.iloc[val_end:test_end]\n",
    "        \n",
    "        train_dataset = DiffusionTimeSeriesDataset(train_input, train_target, lookback=lookback)\n",
    "        val_dataset = DiffusionTimeSeriesDataset(val_input, val_target, lookback=lookback)\n",
    "        test_dataset = DiffusionTimeSeriesDataset(test_input, test_target, lookback=lookback)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        input_dim = train_input.shape[1]\n",
    "        model = DiffusionClassifier(input_dim=input_dim, lookback=lookback, \n",
    "                                    condition_dim=128, num_timesteps=100, hidden_dim=128).to(device)\n",
    "        model_path = f\"diffusion_model_experiment_{exp}.pth\"\n",
    "        if exp > 0:\n",
    "            try:\n",
    "                model.load_state_dict(torch.load(f\"diffusion_model_experiment_{exp - 1}.pth\"))\n",
    "                print(f\"Loaded model from experiment {exp - 1} for fine-tuning.\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Model file for experiment {exp - 1} not found. Starting fresh training.\")\n",
    "        \n",
    "        print(f\"Experiment {exp}: Training Diffusion Model\")\n",
    "        train_diffusion_model(model, train_loader, num_epochs, device, lr=1e-4)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Saved model for experiment {exp}.\")\n",
    "        \n",
    "        print(\"Validation Evaluation:\")\n",
    "        val_mse, val_acc = evaluate_diffusion_model(model, val_loader, device)\n",
    "        val_mse_list.append(val_mse)\n",
    "        val_acc_list.append(val_acc)\n",
    "        \n",
    "        print(\"Test Evaluation:\")\n",
    "        test_mse, test_acc = evaluate_diffusion_model(model, test_loader, device)\n",
    "        test_mse_list.append(test_mse)\n",
    "        test_acc_list.append(test_acc)\n",
    "    \n",
    "        print(f\"Experiment {exp}: Validation MSE: {val_mse:.6f}, val_Accuracy: {val_acc:.4f}, test_Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    if val_mse_list:\n",
    "        avg_val_mse = sum(val_mse_list) / len(val_mse_list)\n",
    "        avg_test_mse = sum(test_mse_list) / len(test_mse_list)\n",
    "        avg_val_acc = sum(val_acc_list) / len(val_acc_list)\n",
    "        avg_test_acc = sum(test_acc_list) / len(test_acc_list)\n",
    "        print(f\"\\nFinal Average Validation MSE: {avg_val_mse:.6f}\")\n",
    "        print(f\"Final Average Test MSE: {avg_test_mse:.6f}\")\n",
    "        print(f\"Final Average Val Accuracy: {avg_val_acc:.4f}\")\n",
    "        print(f\"Final Average Test Accuracy: {avg_test_acc:.4f}\")\n",
    "    else:\n",
    "        print(\"실험이 한 번도 실행되지 않았습니다.\")\n",
    "\n",
    "##############################################\n",
    "# 전체 실행 시간 측정\n",
    "##############################################\n",
    "start_time = time.time()\n",
    "train_and_evaluate_diffusion(data, num_experiments=16, lookback=24, num_epochs=10)\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "print(f\"\\n총 수행 시간: {elapsed:.2f}초\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39coin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
