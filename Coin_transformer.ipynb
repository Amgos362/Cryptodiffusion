{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Unnamed: 0       open       high        low      close    volume  \\\n",
      "0  2017-09-25 12:00:00  4201000.0  4224000.0  4195000.0  4224000.0  8.518420   \n",
      "1  2017-09-25 12:05:00  4225000.0  4241000.0  4208000.0  4227000.0  7.421608   \n",
      "2  2017-09-25 12:10:00  4215000.0  4236000.0  4212000.0  4227000.0  6.693031   \n",
      "3  2017-09-25 12:15:00  4227000.0  4244000.0  4210000.0  4227000.0  9.531738   \n",
      "4  2017-09-25 12:20:00  4215000.0  4242000.0  4203000.0  4215000.0  9.113169   \n",
      "\n",
      "          value  \n",
      "0  3.585738e+07  \n",
      "1  3.137140e+07  \n",
      "2  2.826349e+07  \n",
      "3  4.027968e+07  \n",
      "4  3.846652e+07  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.0001\n",
      "Epoch [2/50], Loss: 0.0001\n",
      "Epoch [3/50], Loss: 0.0001\n",
      "Epoch [4/50], Loss: 0.0000\n",
      "Epoch [5/50], Loss: 0.0000\n",
      "Epoch [6/50], Loss: 0.0000\n",
      "Epoch [7/50], Loss: 0.0000\n",
      "Epoch [8/50], Loss: 0.0000\n",
      "Epoch [9/50], Loss: 0.0000\n",
      "Epoch [10/50], Loss: 0.0004\n",
      "Epoch [11/50], Loss: 0.0000\n",
      "Epoch [12/50], Loss: 0.0000\n",
      "Epoch [13/50], Loss: 0.0000\n",
      "Epoch [14/50], Loss: 0.0000\n",
      "Epoch [15/50], Loss: 0.0000\n",
      "Epoch [16/50], Loss: 0.0000\n",
      "Epoch [17/50], Loss: 0.0000\n",
      "Epoch [18/50], Loss: 0.0000\n",
      "Epoch [19/50], Loss: 0.0000\n",
      "Epoch [20/50], Loss: 0.0000\n",
      "Epoch [21/50], Loss: 0.0001\n",
      "Epoch [22/50], Loss: 0.0000\n",
      "Epoch [23/50], Loss: 0.0000\n",
      "Epoch [24/50], Loss: 0.0000\n",
      "Epoch [25/50], Loss: 0.0000\n",
      "Epoch [26/50], Loss: 0.0000\n",
      "Epoch [27/50], Loss: 0.0000\n",
      "Epoch [28/50], Loss: 0.0000\n",
      "Epoch [29/50], Loss: 0.0000\n",
      "Epoch [30/50], Loss: 0.0001\n",
      "Epoch [31/50], Loss: 0.0000\n",
      "Epoch [32/50], Loss: 0.0000\n",
      "Epoch [33/50], Loss: 0.0000\n",
      "Epoch [34/50], Loss: 0.0000\n",
      "Epoch [35/50], Loss: 0.0001\n",
      "Epoch [36/50], Loss: 0.0000\n",
      "Epoch [37/50], Loss: 0.0000\n",
      "Epoch [38/50], Loss: 0.0000\n",
      "Epoch [39/50], Loss: 0.0000\n",
      "Epoch [40/50], Loss: 0.0000\n",
      "Epoch [41/50], Loss: 0.0000\n",
      "Epoch [42/50], Loss: 0.0000\n",
      "Epoch [43/50], Loss: 0.0000\n",
      "Epoch [44/50], Loss: 0.0000\n",
      "Epoch [45/50], Loss: 0.0000\n",
      "Epoch [46/50], Loss: 0.0001\n",
      "Epoch [47/50], Loss: 0.0000\n",
      "Epoch [48/50], Loss: 0.0000\n",
      "Epoch [49/50], Loss: 0.0000\n",
      "Epoch [50/50], Loss: 0.0000\n",
      "Predicted Close Price: 154100854.43\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1.  \n",
    "file_path = 'BTC_upbit_KRW_min5.csv'  #    .\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "#  \n",
    "print(data.head())\n",
    "\n",
    "#    (Open, High, Low, Close, Volume)\n",
    "ohlcv = data[['open', 'high', 'low', 'close', 'volume']].values\n",
    "\n",
    "# 2.  \n",
    "scaler = MinMaxScaler()\n",
    "ohlcv = scaler.fit_transform(ohlcv)\n",
    "\n",
    "# 3.   \n",
    "def create_sequences(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i + seq_length])\n",
    "        y.append(data[i + seq_length, 3])  # (Close) \n",
    "    return torch.tensor(np.array(X), dtype=torch.float32), torch.tensor(np.array(y), dtype=torch.float32)\n",
    "\n",
    "SEQ_LENGTH = 48  #   48 5    (4 )\n",
    "X, y = create_sequences(ohlcv, SEQ_LENGTH)\n",
    "\n",
    "# 4.    \n",
    "class BitcoinDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx].to(device), self.y[idx].to(device)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset = BitcoinDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 5.   \n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, num_layers, hidden_dim):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformer(x)\n",
    "        x = self.fc(x[-1])  #    \n",
    "        return x\n",
    "\n",
    "model = TransformerModel(input_dim=5, num_heads=1, num_layers=2, hidden_dim=64).to(device)\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 6.  \n",
    "EPOCHS = 50\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch_X, batch_y in dataloader:\n",
    "        batch_X = batch_X.permute(1, 0, 2)  # (batch, seq, feature) -> (seq, batch, feature)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_X)\n",
    "        loss = criterion(output.squeeze(), batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 7.    \n",
    "model.eval()\n",
    "test_seq = X[-1].unsqueeze(1).to(device)  #  \n",
    "prediction = model(test_seq).item()\n",
    "predicted_close = scaler.inverse_transform([[0, 0, 0, prediction, 0]])[0, 3]\n",
    "print(f'Predicted Close Price: {predicted_close:.2f}')\n",
    "\n",
    "# 8.   \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(data.index[-100:], data['close'].values[-100:], label='Actual')\n",
    "plt.axhline(y=predicted_close, color='r', linestyle='--', label='Prediction')\n",
    "plt.legend()\n",
    "plt.title('Bitcoin 5-Minute Close Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price (KRW)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 153\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFine-Tuning\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mfine_tune\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPre-Training\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Epoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m# Pre-training \u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfine_tune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# Fine-tuning \u001b[39;00m\n\u001b[0;32m    156\u001b[0m train_model(model, dataloader, criterion, optimizer, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, fine_tune\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[10], line 143\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloader, criterion, optimizer, num_epochs, fine_tune)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    142\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(masked_outputs, masked_targets)\n\u001b[1;32m--> 143\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    146\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 1.  : SMA,  , RSI, MACD \n",
    "def calculate_indicators(data):\n",
    "    #   (SMA) \n",
    "    sma_windows = [5, 10, 20, 60, 120, 250]\n",
    "    for window in sma_windows:\n",
    "        data[f'{window}SMA'] = data['close'].rolling(window=window).mean()\n",
    "\n",
    "    #   \n",
    "    rolling_mean = data['close'].rolling(window=20).mean()\n",
    "    rolling_std = data['close'].rolling(window=20).std()\n",
    "    data['BB_Middle'] = rolling_mean\n",
    "    data['BB_Upper'] = rolling_mean + (rolling_std * 2)\n",
    "    data['BB_Lower'] = rolling_mean - (rolling_std * 2)\n",
    "\n",
    "    # RSI \n",
    "    delta = data['close'].diff(1)\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / (loss + 1e-10)  #  \n",
    "    data['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    # MACD \n",
    "    exp1 = data['close'].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = data['close'].ewm(span=26, adjust=False).mean()\n",
    "    data['MACD'] = exp1 - exp2\n",
    "    data['MACD_Signal'] = data['MACD'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "    return data\n",
    "\n",
    "#     \n",
    "data = pd.read_csv(\"BTC_upbit_KRW_min5.csv\")\n",
    "data = calculate_indicators(data)\n",
    "\n",
    "#   NA  \n",
    "data = data.dropna()\n",
    "\n",
    "#     \n",
    "features = ['open', 'high', 'low', 'close', 'volume', '5SMA', '10SMA', '20SMA', '60SMA', '120SMA', '250SMA', 'BB_Upper', 'BB_Lower', 'RSI', 'MACD', 'MACD_Signal']\n",
    "data = data[features].values\n",
    "scaler = MinMaxScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "# Tensor \n",
    "data_tensor = torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "#       \n",
    "class BTCMinuteDataset(Dataset):\n",
    "    def __init__(self, data, seq_length=120, mask_length=10):\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "        self.mask_length = mask_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.data[idx:idx + self.seq_length]\n",
    "        input_seq = seq[:-self.mask_length]\n",
    "        target_seq = seq[-self.mask_length:]\n",
    "        mask = torch.zeros_like(target_seq)\n",
    "        mask_indices = torch.randperm(self.mask_length)[:int(self.mask_length * 0.5)]\n",
    "        mask[mask_indices] = 1\n",
    "        masked_target = target_seq.clone()\n",
    "        masked_target[mask == 0] = -1  #    \n",
    "        return input_seq, target_seq, masked_target, mask\n",
    "\n",
    "dataset = BTCMinuteDataset(data_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "# 3.  \n",
    "class BERTForTimeSeries(nn.Module):\n",
    "    def __init__(self, input_dim, seq_length, hidden_dim=128, num_layers=4, num_heads=8):\n",
    "        super(BERTForTimeSeries, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        # BERT \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        #  \n",
    "        self.fc = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.encoder(x, src_key_padding_mask=mask)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "#  \n",
    "input_dim = 16  # 8  \n",
    "seq_length = 120\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERTForTimeSeries(input_dim=input_dim, seq_length=seq_length).to(device)\n",
    "\n",
    "\n",
    "#    \n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 4. Pre-training  Fine-tuning\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=10, fine_tune=False):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for input_seq, target_seq, masked_target, mask in dataloader:\n",
    "            #  GPU \n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "            masked_target = masked_target.to(device)\n",
    "            mask = mask.to(device)\n",
    "\n",
    "            #    \n",
    "            input_seq = input_seq.permute(1, 0, 2)  # (batch, seq, feature) -> (seq, batch, feature)\n",
    "            target_seq = target_seq.permute(1, 0, 2)\n",
    "            masked_target = masked_target.permute(1, 0, 2)\n",
    "            mask = mask.permute(1, 0, 2)  #   \n",
    "\n",
    "            #  \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_seq)\n",
    "\n",
    "            #    \n",
    "            mask = mask.bool()  #  Boolean \n",
    "            masked_outputs = outputs[-mask.size(0):][mask]  #  \n",
    "            masked_targets = masked_target[mask]  #  \n",
    "            \n",
    "            if masked_outputs.numel() == 0 or masked_targets.numel() == 0:\n",
    "                #        \n",
    "                print(\"Warning: No masked values to calculate loss.\")\n",
    "                continue\n",
    "\n",
    "            loss = criterion(masked_outputs, masked_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"{'Fine-Tuning' if fine_tune else 'Pre-Training'} Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Pre-training \n",
    "train_model(model, dataloader, criterion, optimizer, num_epochs=10, fine_tune=False)\n",
    "\n",
    "# Fine-tuning \n",
    "train_model(model, dataloader, criterion, optimizer, num_epochs=5, fine_tune=True)\n",
    "\n",
    "#  \n",
    "torch.save(model.state_dict(), \"bert_time_series_model.pth\")\n",
    "print(\"Model saved successfully.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (7577540,) (7577539,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 161\u001b[0m\n\u001b[0;32m    156\u001b[0m evaluation_data \u001b[38;5;241m=\u001b[39m data_tensor\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m#     \u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m#  ETH  :\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# evaluation_data = eth_data_tensor.numpy()\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m#   \u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m mse, mae, da, pnl \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluation_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# Hit Ratio \u001b[39;00m\n\u001b[0;32m    164\u001b[0m hit_ratio \u001b[38;5;241m=\u001b[39m calculate_hit_ratio(model, data_tensor\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "Cell \u001b[1;32mIn[11], line 143\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, data, seq_length, mask_length, features)\u001b[0m\n\u001b[0;32m    140\u001b[0m mse \u001b[38;5;241m=\u001b[39m mean_squared_error(actuals, predictions)\n\u001b[0;32m    141\u001b[0m mae \u001b[38;5;241m=\u001b[39m mean_absolute_error(actuals, predictions)\n\u001b[0;32m    142\u001b[0m directional_hits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\n\u001b[1;32m--> 143\u001b[0m     (\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactuals\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m (np\u001b[38;5;241m.\u001b[39marray(actuals[\u001b[38;5;241m1\u001b[39m:]) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39marray(actuals[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    144\u001b[0m )\n\u001b[0;32m    145\u001b[0m da \u001b[38;5;241m=\u001b[39m directional_hits \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(actuals[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    146\u001b[0m total_pnl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(profits)\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (7577540,) (7577539,) "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# 1.  : SMA,  , RSI, MACD \n",
    "def calculate_indicators(data):\n",
    "    #   (SMA) \n",
    "    sma_windows = [5, 10, 20, 60, 120, 250]\n",
    "    for window in sma_windows:\n",
    "        data[f'{window}SMA'] = data['close'].rolling(window=window).mean()\n",
    "\n",
    "    #   \n",
    "    rolling_mean = data['close'].rolling(window=20).mean()\n",
    "    rolling_std = data['close'].rolling(window=20).std()\n",
    "    data['BB_Middle'] = rolling_mean\n",
    "    data['BB_Upper'] = rolling_mean + (rolling_std * 2)\n",
    "    data['BB_Lower'] = rolling_mean - (rolling_std * 2)\n",
    "\n",
    "    # RSI \n",
    "    delta = data['close'].diff(1)\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / (loss + 1e-10)  #  \n",
    "    data['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    # MACD \n",
    "    exp1 = data['close'].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = data['close'].ewm(span=26, adjust=False).mean()\n",
    "    data['MACD'] = exp1 - exp2\n",
    "    data['MACD_Signal'] = data['MACD'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "    return data\n",
    "\n",
    "# Feature  \n",
    "features = ['open', 'high', 'low', 'close', 'volume', '5SMA', '10SMA', '20SMA', '60SMA', '120SMA', '250SMA',\n",
    "             'BB_Upper', 'BB_Lower', 'RSI', 'MACD', 'MACD_Signal']\n",
    "\n",
    "#     \n",
    "input_dim = len(features)  #  \n",
    "seq_length = 120  #   \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  #  \n",
    "\n",
    "#      \n",
    "model = BERTForTimeSeries(input_dim=input_dim, seq_length=seq_length).to(device)\n",
    "model.load_state_dict(torch.load(\"bert_time_series_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# 2.   \n",
    "data = pd.read_csv(\"ETH_upbit_KRW_min5.csv\")\n",
    "new_data = calculate_indicators(data)\n",
    "new_data = new_data.dropna()\n",
    "new_data = new_data[features].values[-120:]\n",
    "new_data = scaler.transform(new_data)\n",
    "input_tensor = torch.tensor(new_data, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "# 3.  \n",
    "with torch.no_grad():\n",
    "    input_tensor = input_tensor.permute(1, 0, 2)  # (seq_length, batch_size, features)\n",
    "    predictions = model(input_tensor)\n",
    "\n",
    "# 4.   \n",
    "predicted_values = predictions[-10:].squeeze(1).cpu().numpy()  # (10, features)\n",
    "predicted_values = scaler.inverse_transform(predicted_values)\n",
    "\n",
    "\n",
    "\n",
    "def calculate_hit_ratio(model, data, seq_length=120, mask_length=10):\n",
    "    model.eval()  #    \n",
    "    hits = 0\n",
    "    total = 0\n",
    "\n",
    "    for idx in range(len(data) - seq_length):\n",
    "        #  \n",
    "        input_seq = data[idx:idx + seq_length - mask_length]\n",
    "        target_seq = data[idx + seq_length - mask_length:idx + seq_length]\n",
    "\n",
    "        #      \n",
    "        input_tensor = torch.tensor(input_seq, dtype=torch.float32).unsqueeze(0).permute(1, 0, 2).to(device)\n",
    "\n",
    "        #  \n",
    "        with torch.no_grad():\n",
    "            predictions = model(input_tensor)\n",
    "            predicted_values = predictions[-mask_length:].squeeze(1).cpu().numpy()\n",
    "\n",
    "        #     \n",
    "        predicted_close = predicted_values[:, features.index('close')]\n",
    "        actual_close = target_seq[:, features.index('close')]\n",
    "\n",
    "        #   ( /  / )\n",
    "        for pred, actual in zip(predicted_close, actual_close):\n",
    "            if (pred - input_seq[-1, features.index('close')]) * (actual - input_seq[-1, features.index('close')]) > 0:\n",
    "                hits += 1\n",
    "            total += 1\n",
    "\n",
    "    # Hit Ratio \n",
    "    hit_ratio = hits / total if total > 0 else 0\n",
    "    print(f\"Hit Ratio: {hit_ratio:.4f}\")\n",
    "    return hit_ratio\n",
    "\n",
    "\n",
    "def evaluate_model(model, data, seq_length=120, mask_length=10, features=None):\n",
    "    model.eval()  #   \n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    profits = []\n",
    "\n",
    "    for idx in range(len(data) - seq_length):\n",
    "        #  \n",
    "        input_seq = data[idx:idx + seq_length - mask_length]\n",
    "        target_seq = data[idx + seq_length - mask_length:idx + seq_length]\n",
    "\n",
    "        #   \n",
    "        input_tensor = torch.tensor(input_seq, dtype=torch.float32).unsqueeze(0).permute(1, 0, 2).to(device)\n",
    "\n",
    "        #  \n",
    "        with torch.no_grad():\n",
    "            pred = model(input_tensor)\n",
    "            predicted_values = pred[-mask_length:].squeeze(1).cpu().numpy()\n",
    "\n",
    "        # (Close) \n",
    "        predicted_close = predicted_values[:, features.index('close')]\n",
    "        actual_close = target_seq[:, features.index('close')]\n",
    "\n",
    "        #  \n",
    "        predictions.extend(predicted_close)\n",
    "        actuals.extend(actual_close)\n",
    "\n",
    "        # PnL : /  / \n",
    "        for prev_close, pred, actual in zip(input_seq[-1:, features.index('close')], predicted_close, actual_close):\n",
    "            if pred > prev_close:  #   -> \n",
    "                profits.append(actual - prev_close)\n",
    "            elif pred < prev_close:  #   -> \n",
    "                profits.append(prev_close - actual)\n",
    "\n",
    "    #  \n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    directional_hits = sum(\n",
    "        (np.array(predictions) - np.array(actuals[:-1])) * (np.array(actuals[1:]) - np.array(actuals[:-1])) > 0\n",
    "    )\n",
    "    da = directional_hits / len(actuals[:-1])\n",
    "    total_pnl = sum(profits)\n",
    "\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"Directional Accuracy (DA): {da:.4f}\")\n",
    "    print(f\"Profit and Loss (PnL): {total_pnl:.4f}\")\n",
    "\n",
    "    return mse, mae, da, total_pnl\n",
    "\n",
    "#    (BTC  ETH)\n",
    "evaluation_data = data_tensor.numpy()  #     \n",
    "#  ETH  :\n",
    "# evaluation_data = eth_data_tensor.numpy()\n",
    "\n",
    "#   \n",
    "mse, mae, da, pnl = evaluate_model(model, evaluation_data, features=features)\n",
    "\n",
    "# Hit Ratio \n",
    "hit_ratio = calculate_hit_ratio(model, data_tensor.numpy())\n",
    "\n",
    "\n",
    "# 5.  \n",
    "predicted_close_prices = predicted_values[:, features.index('close')]\n",
    "print(data['close'].tail(10), \"2024-12-18 16:30:00\")\n",
    "print(\"Predicted Close Prices:\", predicted_close_prices)\n",
    "print(predicted_close_prices.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Ratio: 0.4935\n",
      "758113    151994000.0\n",
      "758114    152000000.0\n",
      "758115    152053000.0\n",
      "758116    152123000.0\n",
      "758117    152200000.0\n",
      "758118    152320000.0\n",
      "758119    152499000.0\n",
      "758120    152326000.0\n",
      "758121    152208000.0\n",
      "758122    152206000.0\n",
      "Name: close, dtype: float64 2024-12-18 16:30:00\n",
      "Predicted Close Prices: [1.5313317e+08 1.5310178e+08 1.5309360e+08 1.5311077e+08 1.5303133e+08\n",
      " 1.5310317e+08 1.5307259e+08 1.5300530e+08 1.5288952e+08 1.5287104e+08]\n",
      "153041220.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1.  \n",
    "model = BERTForTimeSeries(input_dim=input_dim, seq_length=seq_length).to(device)\n",
    "model.load_state_dict(torch.load(\"bert_time_series_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# 2.   \n",
    "data = pd.read_csv(\"BTC_upbit_KRW_min5.csv\")\n",
    "new_data = calculate_indicators(data)\n",
    "new_data = new_data.dropna()\n",
    "new_data = new_data[features].values[-120:]\n",
    "new_data = scaler.transform(new_data)\n",
    "input_tensor = torch.tensor(new_data, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "# 3.  \n",
    "with torch.no_grad():\n",
    "    input_tensor = input_tensor.permute(1, 0, 2)  # (seq_length, batch_size, features)\n",
    "    predictions = model(input_tensor)\n",
    "\n",
    "# 4.   \n",
    "predicted_values = predictions[-10:].squeeze(1).cpu().numpy()  # (10, features)\n",
    "predicted_values = scaler.inverse_transform(predicted_values)\n",
    "\n",
    "def calculate_hit_ratio(model, data, seq_length=120, mask_length=10):\n",
    "    model.eval()  #    \n",
    "    hits = 0\n",
    "    total = 0\n",
    "\n",
    "    for idx in range(len(data) - seq_length):\n",
    "        #  \n",
    "        input_seq = data[idx:idx + seq_length - mask_length]\n",
    "        target_seq = data[idx + seq_length - mask_length:idx + seq_length]\n",
    "\n",
    "        #      \n",
    "        input_tensor = torch.tensor(input_seq, dtype=torch.float32).unsqueeze(0).permute(1, 0, 2).to(device)\n",
    "\n",
    "        #  \n",
    "        with torch.no_grad():\n",
    "            predictions = model(input_tensor)\n",
    "            predicted_values = predictions[-mask_length:].squeeze(1).cpu().numpy()\n",
    "\n",
    "        #     \n",
    "        predicted_close = predicted_values[:, features.index('close')]\n",
    "        actual_close = target_seq[:, features.index('close')]\n",
    "\n",
    "        #   ( /  / )\n",
    "        for pred, actual in zip(predicted_close, actual_close):\n",
    "            if (pred - input_seq[-1, features.index('close')]) * (actual - input_seq[-1, features.index('close')]) > 0:\n",
    "                hits += 1\n",
    "            total += 1\n",
    "\n",
    "    # Hit Ratio \n",
    "    hit_ratio = hits / total if total > 0 else 0\n",
    "    print(f\"Hit Ratio: {hit_ratio:.4f}\")\n",
    "    return hit_ratio\n",
    "\n",
    "# Hit Ratio \n",
    "hit_ratio = calculate_hit_ratio(model, data_tensor.numpy())\n",
    "\n",
    "\n",
    "# 5.  \n",
    "predicted_close_prices = predicted_values[:, features.index('close')]\n",
    "print(data['close'].tail(10), \"2024-12-18 16:30:00\")\n",
    "print(\"Predicted Close Prices:\", predicted_close_prices)\n",
    "print(predicted_close_prices.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 148\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mtotal\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# 5.     \u001b[39;00m\n\u001b[1;32m--> 148\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_only_transformer.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 136\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloader, num_epochs, lr)\u001b[0m\n\u001b[0;32m    134\u001b[0m x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    135\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 136\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y)\n\u001b[0;32m    138\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 114\u001b[0m, in \u001b[0;36mEncoderOnlyTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    112\u001b[0m x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding(positions)\n\u001b[0;32m    113\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Convert to (seq_len, batch_size, embedding_dim)\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m x \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Take the last token's representation\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:391\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    388\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 391\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[0;32m    394\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:714\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    712\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[0;32m    713\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 714\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    715\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[0;32m    717\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:722\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[1;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    720\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor,\n\u001b[0;32m    721\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 722\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    726\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1241\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   1227\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1228\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[0;32m   1229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1238\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[0;32m   1239\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[0;32m   1240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1241\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\functional.py:5336\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   5334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_separate_proj_weight:\n\u001b[0;32m   5335\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m in_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is False but in_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 5336\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m \u001b[43m_in_projection_packed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5338\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m q_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is True but q_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\functional.py:4857\u001b[0m, in \u001b[0;36m_in_projection_packed\u001b[1;34m(q, k, v, w, b)\u001b[0m\n\u001b[0;32m   4854\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mis\u001b[39;00m v:\n\u001b[0;32m   4855\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m q \u001b[38;5;129;01mis\u001b[39;00m k:\n\u001b[0;32m   4856\u001b[0m         \u001b[38;5;66;03m# self-attention\u001b[39;00m\n\u001b[1;32m-> 4857\u001b[0m         proj \u001b[38;5;241m=\u001b[39m \u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4858\u001b[0m         \u001b[38;5;66;03m# reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\u001b[39;00m\n\u001b[0;32m   4859\u001b[0m         proj \u001b[38;5;241m=\u001b[39m proj\u001b[38;5;241m.\u001b[39munflatten(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m3\u001b[39m, E))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta as ta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1.   \n",
    "def calculate_indicators(data):\n",
    "    #   \n",
    "    # data['Ichimoku'] = ta.ichimoku(data['high'], data['low'], data['close'], timed=True)  # Ensure 'timed' parameter\n",
    "    # data['SMI'] = ta.smi(data['close'], data['high'], data['low'], fast=5, slow=20, signal=5)\n",
    "    data['William_R'] = ta.willr(data['high'], data['low'], data['close'])\n",
    "    # data['Fisher'] = ta.fisher(data['high'], data['low'])\n",
    "    data['ATR'] = ta.atr(data['high'], data['low'], data['close'])\n",
    "    data['OBV'] = ta.obv(data['close'], data['volume'])\n",
    "    data['Z_Score'] = (data['close'] - data['close'].rolling(window=20).mean()) / data['close'].rolling(window=20).std()\n",
    "    data['Entropy'] = ta.entropy(data['close'], length=14)\n",
    "    data['SMA_5'] = data['close'].rolling(window=5).mean()\n",
    "    data['RSI'] = ta.rsi(data['close'])\n",
    "    bb = ta.bbands(data['close'])\n",
    "    data['BB_Upper'], data['BB_Middle'], data['BB_Lower'] = bb.iloc[:, 0], bb.iloc[:, 1], bb.iloc[:, 2]\n",
    "    macd = ta.macd(data['close'])\n",
    "    data['MACD'] = macd.iloc[:, 0]\n",
    "    data['Stochastic'] = ta.stoch(data['high'], data['low'], data['close']).iloc[:, 0]\n",
    "\n",
    "    #  \n",
    "    data = data.dropna()\n",
    "    return data\n",
    "\n",
    "# 2. Datetime    One-Hot Encoding\n",
    "def encode_datetime_features(data):\n",
    "    # Datetime  \n",
    "    if 'datetime' not in data.columns:\n",
    "        data['datetime'] = pd.to_datetime(data.index)\n",
    "\n",
    "    # Datetime   \n",
    "    data['day_of_week'] = data['datetime'].dt.dayofweek  # 0=, 6=\n",
    "    data['week_of_month'] = (data['datetime'].dt.day - 1) // 7 + 1  #  \n",
    "    data['month'] = data['datetime'].dt.month  # \n",
    "\n",
    "    # One-Hot Encoding\n",
    "    day_one_hot = pd.get_dummies(data['day_of_week'], prefix='Day')\n",
    "    week_one_hot = pd.get_dummies(data['week_of_month'], prefix='Week')\n",
    "    month_one_hot = pd.get_dummies(data['month'], prefix='Month')\n",
    "\n",
    "    # \n",
    "    data = pd.concat([data, day_one_hot, week_one_hot, month_one_hot], axis=1)\n",
    "    return data\n",
    "\n",
    "# 3.    \n",
    "data = pd.read_csv(\"BTC_upbit_KRW_min5.csv\", index_col=0)  # Replace with your file path\n",
    "data.columns = ['open', 'high', 'low', 'close', 'volume', 'value']\n",
    "\n",
    "# Ensure datetime index\n",
    "data.index = pd.to_datetime(data.index)  # Convert index to datetime format\n",
    "\n",
    "#   \n",
    "data = calculate_indicators(data)\n",
    "\n",
    "# Datetime \n",
    "data = encode_datetime_features(data)\n",
    "\n",
    "# Feature   \n",
    "features = ['open', 'high', 'low', 'close', 'volume', 'value', 'William_R',\n",
    "            'ATR', 'OBV', 'Z_Score', 'Entropy', 'SMA_5', 'RSI', 'BB_Upper', 'BB_Middle', 'BB_Lower',\n",
    "            'MACD', 'Stochastic'] + list(data.filter(regex='Day_').columns) + list(data.filter(regex='Week_').columns) + list(data.filter(regex='Month_').columns)\n",
    "\n",
    "data = data[features].dropna()\n",
    "scaler = MinMaxScaler()\n",
    "data[features] = scaler.fit_transform(data[features])\n",
    "\n",
    "# 4. Dataset \n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, lookback=50, target_idx=-1):\n",
    "        self.data = data\n",
    "        self.lookback = lookback\n",
    "        self.target_idx = target_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.lookback\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.lookback, :]\n",
    "        y = self.data[idx + self.lookback, self.target_idx]\n",
    "        y_target = 1 if y > self.data[idx + self.lookback - 1, self.target_idx] else 0\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y_target, dtype=torch.long)\n",
    "\n",
    "#  \n",
    "data_array = data.values  # Convert DataFrame to numpy array\n",
    "target_idx = features.index('close')  # Target column index\n",
    "dataset = TimeSeriesDataset(data_array, target_idx=target_idx)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 3. Transformer  \n",
    "class EncoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=512, num_heads=8, num_layers=4, ffn_dim=1024, num_classes=2):\n",
    "        super(EncoderOnlyTransformer, self).__init__()\n",
    "        self.token_embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(512, embedding_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim, nhead=num_heads, dim_feedforward=ffn_dim\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, feature_dim = x.shape\n",
    "        x = self.token_embedding(x)\n",
    "        positions = torch.arange(seq_len).unsqueeze(0).repeat(batch_size, 1).to(x.device)\n",
    "        x += self.position_embedding(positions)\n",
    "        x = x.permute(1, 0, 2)  # Convert to (seq_len, batch_size, embedding_dim)\n",
    "        x = self.encoder(x)\n",
    "        x = x[-1]  # Take the last token's representation\n",
    "        return self.fc(x)\n",
    "\n",
    "#  \n",
    "input_dim = len(features)\n",
    "model = EncoderOnlyTransformer(input_dim=input_dim).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 4.  \n",
    "def train_model(model, dataloader, num_epochs=10, lr=1e-4):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss:.4f}, Accuracy: {correct / total:.4f}\")\n",
    "\n",
    "# 5.     \n",
    "train_model(model, dataloader)\n",
    "torch.save(model.state_dict(), \"encoder_only_transformer.pth\")\n",
    "print(\"Model saved successfully!\")\n",
    "\n",
    "#  \n",
    "model = EncoderOnlyTransformer(input_dim=input_dim)\n",
    "model.load_state_dict(torch.load(\"encoder_only_transformer.pth\"))\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Ratio: 0.5194\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 1.  \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = EncoderOnlyTransformer(input_dim=input_dim).to(device)\n",
    "model.load_state_dict(torch.load(\"encoder_only_transformer.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# 2.    \n",
    "data = pd.read_csv(\"BTC_upbit_KRW_min5.csv\", index_col=0)\n",
    "data.columns = ['open', 'high', 'low', 'close', 'volume', 'value']\n",
    "\n",
    "# Ensure datetime index\n",
    "data.index = pd.to_datetime(data.index)\n",
    "\n",
    "#     \n",
    "data = calculate_indicators(data)\n",
    "data = encode_datetime_features(data)\n",
    "\n",
    "# Feature   \n",
    "data = data[features].dropna()\n",
    "scaler = MinMaxScaler()\n",
    "data[features] = scaler.fit_transform(data[features])\n",
    "\n",
    "# 3.   \n",
    "test_data_array = data[features].values\n",
    "X_test, y_test = prepare_test_data(test_data_array, lookback=15, target_idx=features.index('close'))\n",
    "\n",
    "# 4.   \n",
    "def test_hit_ratio(model, X_test, y_test, device='cuda'):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(X_test)):\n",
    "            pred = model(X_test[i:i+1])\n",
    "            predictions.append(pred.argmax(1).item())\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    y_test = y_test.cpu().numpy()\n",
    "\n",
    "    hit_ratio = np.mean(predictions == y_test)\n",
    "    return hit_ratio\n",
    "\n",
    "# 5.   \n",
    "hit_ratio = test_hit_ratio(model, X_test, y_test, device)\n",
    "\n",
    "# 6.  \n",
    "print(f\"Hit Ratio: {hit_ratio:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Ratio: 0.5517\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 1.  \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = EncoderOnlyTransformer(input_dim=input_dim).to(device)\n",
    "model.load_state_dict(torch.load(\"encoder_only_transformer.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# 2.    \n",
    "data = pd.read_csv(\"ETH_upbit_KRW_min5.csv\", index_col=0)\n",
    "data.columns = ['open', 'high', 'low', 'close', 'volume', 'value']\n",
    "\n",
    "# Ensure datetime index\n",
    "data.index = pd.to_datetime(data.index)\n",
    "\n",
    "#     \n",
    "data = calculate_indicators(data)\n",
    "data = encode_datetime_features(data)\n",
    "\n",
    "# Feature   \n",
    "data = data[features].dropna()\n",
    "scaler = MinMaxScaler()\n",
    "data[features] = scaler.fit_transform(data[features])\n",
    "\n",
    "# 3.   \n",
    "test_data_array = data[features].values\n",
    "X_test, y_test = prepare_test_data(test_data_array, lookback=15, target_idx=features.index('close'))\n",
    "\n",
    "# 4.   \n",
    "def test_hit_ratio(model, X_test, y_test, device='cuda'):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(X_test)):\n",
    "            pred = model(X_test[i:i+1])\n",
    "            predictions.append(pred.argmax(1).item())\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    y_test = y_test.cpu().numpy()\n",
    "\n",
    "    hit_ratio = np.mean(predictions == y_test)\n",
    "    return hit_ratio\n",
    "\n",
    "# 5.   \n",
    "hit_ratio = test_hit_ratio(model, X_test, y_test, device)\n",
    "\n",
    "# 6.  \n",
    "print(f\"Hit Ratio: {hit_ratio:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta as ta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1.   \n",
    "def calculate_indicators(data):\n",
    "    #   \n",
    "    # data['Ichimoku'] = ta.ichimoku(data['high'], data['low'], data['close'], timed=True)  # Ensure 'timed' parameter\n",
    "    # data['SMI'] = ta.smi(data['close'], data['high'], data['low'], fast=5, slow=20, signal=5)\n",
    "    data['William_R'] = ta.willr(data['high'], data['low'], data['close'])\n",
    "    # data['Fisher'] = ta.fisher(data['high'], data['low'])\n",
    "    data['ATR'] = ta.atr(data['high'], data['low'], data['close'])\n",
    "    data['OBV'] = ta.obv(data['close'], data['volume'])\n",
    "    data['Z_Score'] = (data['close'] - data['close'].rolling(window=20).mean()) / data['close'].rolling(window=20).std()\n",
    "    data['Entropy'] = ta.entropy(data['close'], length=14)\n",
    "    data['SMA_5'] = data['close'].rolling(window=5).mean()\n",
    "    data['RSI'] = ta.rsi(data['close'])\n",
    "    bb = ta.bbands(data['close'])\n",
    "    data['BB_Upper'], data['BB_Middle'], data['BB_Lower'] = bb.iloc[:, 0], bb.iloc[:, 1], bb.iloc[:, 2]\n",
    "    macd = ta.macd(data['close'])\n",
    "    data['MACD'] = macd.iloc[:, 0]\n",
    "    data['Stochastic'] = ta.stoch(data['high'], data['low'], data['close']).iloc[:, 0]\n",
    "\n",
    "    #  \n",
    "    data = data.dropna()\n",
    "    return data\n",
    "\n",
    "# 2. Datetime    One-Hot Encoding\n",
    "def encode_datetime_features(data):\n",
    "    # Datetime  \n",
    "    if 'datetime' not in data.columns:\n",
    "        data['datetime'] = pd.to_datetime(data.index)\n",
    "\n",
    "    # Datetime   \n",
    "    data['day_of_week'] = data['datetime'].dt.dayofweek  # 0=, 6=\n",
    "    data['week_of_month'] = (data['datetime'].dt.day - 1) // 7 + 1  #  \n",
    "    data['month'] = data['datetime'].dt.month  # \n",
    "\n",
    "    # One-Hot Encoding\n",
    "    day_one_hot = pd.get_dummies(data['day_of_week'], prefix='Day')\n",
    "    week_one_hot = pd.get_dummies(data['week_of_month'], prefix='Week')\n",
    "    month_one_hot = pd.get_dummies(data['month'], prefix='Month')\n",
    "\n",
    "    # \n",
    "    data = pd.concat([data, day_one_hot, week_one_hot, month_one_hot], axis=1)\n",
    "    return data\n",
    "\n",
    "# 3.    \n",
    "data = pd.read_csv(\"BTC_upbit_KRW_min5.csv\", index_col=0)  # Replace with your file path\n",
    "data.columns = ['open', 'high', 'low', 'close', 'volume', 'value']\n",
    "\n",
    "# Ensure datetime index\n",
    "data.index = pd.to_datetime(data.index)  # Convert index to datetime format\n",
    "\n",
    "#   \n",
    "data = calculate_indicators(data)\n",
    "\n",
    "# Datetime \n",
    "data = encode_datetime_features(data)\n",
    "\n",
    "# Feature   \n",
    "features = ['open', 'high', 'low', 'close', 'volume', 'value', 'William_R',\n",
    "            'ATR', 'OBV', 'Z_Score', 'Entropy', 'SMA_5', 'RSI', 'BB_Upper', 'BB_Middle', 'BB_Lower',\n",
    "            'MACD', 'Stochastic'] + list(data.filter(regex='Day_').columns) + list(data.filter(regex='Week_').columns) + list(data.filter(regex='Month_').columns)\n",
    "\n",
    "data = data[features].dropna()\n",
    "scaler = MinMaxScaler()\n",
    "data[features] = scaler.fit_transform(data[features])\n",
    "\n",
    "# 4. Dataset \n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, lookback=15, target_idx=-1):\n",
    "        self.data = data\n",
    "        self.lookback = lookback\n",
    "        self.target_idx = target_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.lookback\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.lookback, :]\n",
    "        y = self.data[idx + self.lookback, self.target_idx]\n",
    "        y_target = 1 if y > self.data[idx + self.lookback - 1, self.target_idx] else 0\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y_target, dtype=torch.long)\n",
    "\n",
    "#  \n",
    "data_array = data.values  # Convert DataFrame to numpy array\n",
    "target_idx = features.index('close')  # Target column index\n",
    "dataset = TimeSeriesDataset(data_array, target_idx=target_idx)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 3. Transformer  \n",
    "class EncoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=512, num_heads=8, num_layers=4, ffn_dim=1024, num_classes=2):\n",
    "        super(EncoderOnlyTransformer, self).__init__()\n",
    "        self.token_embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(512, embedding_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim, nhead=num_heads, dim_feedforward=ffn_dim\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, feature_dim = x.shape\n",
    "        x = self.token_embedding(x)\n",
    "        positions = torch.arange(seq_len).unsqueeze(0).repeat(batch_size, 1).to(x.device)\n",
    "        x += self.position_embedding(positions)\n",
    "        x = x.permute(1, 0, 2)  # Convert to (seq_len, batch_size, embedding_dim)\n",
    "        x = self.encoder(x)\n",
    "        x = x[-1]  # Take the last token's representation\n",
    "        return self.fc(x)\n",
    "\n",
    "#  \n",
    "input_dim = len(features)\n",
    "model = EncoderOnlyTransformer(input_dim=input_dim).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 4.  \n",
    "def train_model(model, dataloader, num_epochs=10, lr=1e-4):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss:.4f}, Accuracy: {correct / total:.4f}\")\n",
    "\n",
    "# 5.     \n",
    "train_model(model, dataloader)\n",
    "torch.save(model.state_dict(), \"encoder_only_transformer.pth\")\n",
    "print(\"Model saved successfully!\")\n",
    "\n",
    "#  \n",
    "model = EncoderOnlyTransformer(input_dim=input_dim)\n",
    "model.load_state_dict(torch.load(\"encoder_only_transformer.pth\"))\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 13136.8396, Train Accuracy: 0.5198, Validation Loss: 1641.6305, Validation Accuracy: 0.5137\n",
      "Epoch 2/10, Train Loss: 13126.4803, Train Accuracy: 0.5183, Validation Loss: 1641.3948, Validation Accuracy: 0.5137\n",
      "Epoch 3/10, Train Loss: 13125.4778, Train Accuracy: 0.5194, Validation Loss: 1641.2636, Validation Accuracy: 0.5137\n",
      "Epoch 4/10, Train Loss: 13124.2227, Train Accuracy: 0.5199, Validation Loss: 1641.4210, Validation Accuracy: 0.5137\n",
      "Epoch 5/10, Train Loss: 13123.7730, Train Accuracy: 0.5203, Validation Loss: 1642.0603, Validation Accuracy: 0.5137\n",
      "Epoch 6/10, Train Loss: 13123.4425, Train Accuracy: 0.5202, Validation Loss: 1644.9337, Validation Accuracy: 0.5137\n",
      "Epoch 7/10, Train Loss: 13122.9776, Train Accuracy: 0.5206, Validation Loss: 1641.1916, Validation Accuracy: 0.5137\n",
      "Epoch 8/10, Train Loss: 13123.0886, Train Accuracy: 0.5208, Validation Loss: 1642.0011, Validation Accuracy: 0.5137\n",
      "Epoch 9/10, Train Loss: 13122.6383, Train Accuracy: 0.5209, Validation Loss: 1641.2724, Validation Accuracy: 0.5137\n",
      "Epoch 10/10, Train Loss: 13122.2778, Train Accuracy: 0.5208, Validation Loss: 1641.2694, Validation Accuracy: 0.5137\n",
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta as ta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1.   \n",
    "def calculate_indicators(data):\n",
    "    #   \n",
    "    data['William_R'] = ta.willr(data['high'], data['low'], data['close'])\n",
    "    data['ATR'] = ta.atr(data['high'], data['low'], data['close'])\n",
    "    data['OBV'] = ta.obv(data['close'], data['volume'])\n",
    "    data['Z_Score'] = (data['close'] - data['close'].rolling(window=20).mean()) / data['close'].rolling(window=20).std()\n",
    "    data['Entropy'] = ta.entropy(data['close'], length=14)\n",
    "    data['SMA_5'] = data['close'].rolling(window=5).mean()\n",
    "    data['RSI'] = ta.rsi(data['close'])\n",
    "    bb = ta.bbands(data['close'])\n",
    "    data['BB_Upper'], data['BB_Middle'], data['BB_Lower'] = bb.iloc[:, 0], bb.iloc[:, 1], bb.iloc[:, 2]\n",
    "    macd = ta.macd(data['close'])\n",
    "    data['MACD'] = macd.iloc[:, 0]\n",
    "    data['Stochastic'] = ta.stoch(data['high'], data['low'], data['close']).iloc[:, 0]\n",
    "\n",
    "    #  \n",
    "    data = data.dropna()\n",
    "    return data\n",
    "\n",
    "# 2. Datetime    One-Hot Encoding\n",
    "def encode_datetime_features(data):\n",
    "    if 'datetime' not in data.columns:\n",
    "        data['datetime'] = pd.to_datetime(data.index)\n",
    "    data['day_of_week'] = data['datetime'].dt.dayofweek  # 0=, 6=\n",
    "    data['week_of_month'] = (data['datetime'].dt.day - 1) // 7 + 1  #  \n",
    "    data['month'] = data['datetime'].dt.month\n",
    "    day_one_hot = pd.get_dummies(data['day_of_week'], prefix='Day')\n",
    "    week_one_hot = pd.get_dummies(data['week_of_month'], prefix='Week')\n",
    "    month_one_hot = pd.get_dummies(data['month'], prefix='Month')\n",
    "    data = pd.concat([data, day_one_hot, week_one_hot, month_one_hot], axis=1)\n",
    "    return data\n",
    "\n",
    "# 3.    \n",
    "data = pd.read_csv(\"BTC_upbit_KRW_min5.csv\", index_col=0)\n",
    "data.columns = ['open', 'high', 'low', 'close', 'volume', 'value']\n",
    "data.index = pd.to_datetime(data.index)\n",
    "data = calculate_indicators(data)\n",
    "data = encode_datetime_features(data)\n",
    "\n",
    "# Feature   \n",
    "features = ['open', 'high', 'low', 'close', 'volume', 'value', 'William_R',\n",
    "            'ATR', 'OBV', 'Z_Score', 'Entropy', 'SMA_5', 'RSI', 'BB_Upper', 'BB_Middle', 'BB_Lower',\n",
    "            'MACD', 'Stochastic'] + list(data.filter(regex='Day_').columns) + list(data.filter(regex='Week_').columns) + list(data.filter(regex='Month_').columns)\n",
    "\n",
    "data = data[features].dropna()\n",
    "scaler = MinMaxScaler()\n",
    "data[features] = scaler.fit_transform(data[features])\n",
    "\n",
    "# 4. Feature   One-Hot Encoding\n",
    "def bin_and_encode(data, features, bins=5):\n",
    "    for feature in features:\n",
    "        # \n",
    "        data[f'{feature}_Bin'] = pd.cut(data[feature], bins=bins, labels=False)\n",
    "        # One-Hot Encoding\n",
    "        one_hot = pd.get_dummies(data[f'{feature}_Bin'], prefix=f'{feature}_Bin')\n",
    "        #   \n",
    "        data = pd.concat([data, one_hot], axis=1)\n",
    "        #   column \n",
    "        data.drop(columns=[f'{feature}_Bin'], inplace=True)\n",
    "    #    float32 \n",
    "    data = data.astype(np.float32)\n",
    "    return data\n",
    "\n",
    "\n",
    "#   One-Hot Encoding \n",
    "data = bin_and_encode(data, features)\n",
    "\n",
    "#  : Train 80%, Validation 10%, Test 10%\n",
    "train_size = int(len(data) * 0.8)\n",
    "val_size = int(len(data) * 0.1)\n",
    "train_data = data[:train_size]\n",
    "val_data = data[train_size:train_size + val_size]\n",
    "test_data = data[train_size + val_size:]\n",
    "\n",
    "# Dataset \n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, lookback=15, target_idx=-1):\n",
    "        self.data = data\n",
    "        self.lookback = lookback\n",
    "        self.target_idx = target_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.lookback\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.lookback, :]\n",
    "        y = self.data[idx + self.lookback, self.target_idx]\n",
    "        y_target = 1 if y > self.data[idx + self.lookback - 1, self.target_idx] else 0\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y_target, dtype=torch.long)\n",
    "\n",
    "train_dataset = TimeSeriesDataset(train_data.values, target_idx=features.index('close'))\n",
    "val_dataset = TimeSeriesDataset(val_data.values, target_idx=features.index('close'))\n",
    "test_dataset = TimeSeriesDataset(test_data.values, target_idx=features.index('close'))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Transformer        .\n",
    "class EncoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=512, num_heads=8, num_layers=4, ffn_dim=1024, num_classes=2):\n",
    "        super(EncoderOnlyTransformer, self).__init__()\n",
    "        self.token_embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(512, embedding_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim, nhead=num_heads, dim_feedforward=ffn_dim, batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.token_embedding(x)\n",
    "        x = self.encoder(x)\n",
    "        return self.fc(x[:, -1, :])  #    \n",
    "\n",
    "\n",
    "\n",
    "#  \n",
    "input_dim = len(data.columns)  #     \n",
    "model = EncoderOnlyTransformer(input_dim=input_dim).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# 5.   \n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-4):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                outputs = model(x)\n",
    "                val_loss += criterion(outputs, y).item()\n",
    "                val_correct += (outputs.argmax(1) == y).sum().item()\n",
    "                val_total += y.size(0)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {total_loss:.4f}, Train Accuracy: {correct / total:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_correct / val_total:.4f}\")\n",
    "\n",
    "# 6.  \n",
    "train_model(model, train_loader, val_loader)\n",
    "\n",
    "#  \n",
    "torch.save(model.state_dict(), \"encoder_only_transformer_split.pth\")\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Hit Ratio: 0.5090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1.  \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = EncoderOnlyTransformer(input_dim=input_dim).to(device)\n",
    "model.load_state_dict(torch.load(\"encoder_only_transformer_split.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# 2.   \n",
    "def test_hit_ratio(model, test_loader, device='cuda'):\n",
    "    \"\"\"\n",
    "     Hit Ratio  .\n",
    "    Args:\n",
    "        model:  Transformer \n",
    "        test_loader:  \n",
    "        device:   (GPU or CPU)\n",
    "    Returns:\n",
    "        Hit Ratio ()\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            predictions = outputs.argmax(1)  #  \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_targets.extend(y.cpu().numpy())\n",
    "\n",
    "    # Hit Ratio \n",
    "    hit_ratio = accuracy_score(all_targets, all_predictions)\n",
    "    return hit_ratio\n",
    "\n",
    "# 3.  \n",
    "hit_ratio = test_hit_ratio(model, test_loader, device)\n",
    "\n",
    "# 4.  \n",
    "print(f\"Test Hit Ratio: {hit_ratio:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 16435.3415, Accuracy: 0.5132\n",
      "Epoch 2/10, Loss: 16411.5511, Accuracy: 0.5169\n",
      "Epoch 3/10, Loss: 16412.3794, Accuracy: 0.5167\n",
      "Epoch 4/10, Loss: 16410.6262, Accuracy: 0.5166\n",
      "Epoch 5/10, Loss: 16409.9364, Accuracy: 0.5167\n",
      "Epoch 6/10, Loss: 16409.5730, Accuracy: 0.5173\n",
      "Epoch 7/10, Loss: 16408.9896, Accuracy: 0.5178\n",
      "Epoch 8/10, Loss: 16408.5250, Accuracy: 0.5178\n",
      "Epoch 9/10, Loss: 16407.8591, Accuracy: 0.5185\n",
      "Epoch 10/10, Loss: 16407.6277, Accuracy: 0.5185\n",
      "Model saved successfully!\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta as ta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1.   \n",
    "def calculate_indicators(data):\n",
    "    data['William_R'] = ta.willr(data['high'], data['low'], data['close'])\n",
    "    data['ATR'] = ta.atr(data['high'], data['low'], data['close'])\n",
    "    data['OBV'] = ta.obv(data['close'], data['volume'])\n",
    "    data['Z_Score'] = (data['close'] - data['close'].rolling(window=20).mean()) / data['close'].rolling(window=20).std()\n",
    "    data['Entropy'] = ta.entropy(data['close'], length=14)\n",
    "    data['SMA_5'] = data['close'].rolling(window=5).mean()\n",
    "    data['RSI'] = ta.rsi(data['close'])\n",
    "    bb = ta.bbands(data['close'])\n",
    "    data['BB_Upper'], data['BB_Middle'], data['BB_Lower'] = bb.iloc[:, 0], bb.iloc[:, 1], bb.iloc[:, 2]\n",
    "    macd = ta.macd(data['close'])\n",
    "    data['MACD'] = macd.iloc[:, 0]\n",
    "    data['Stochastic'] = ta.stoch(data['high'], data['low'], data['close']).iloc[:, 0]\n",
    "    data = data.dropna()\n",
    "    return data\n",
    "\n",
    "# 2. Feature  One-Hot Encoding\n",
    "def bin_and_one_hot_encode(data, feature, bins):\n",
    "    \"\"\"\n",
    "     feature  one-hot encoding.\n",
    "    Args:\n",
    "        data: DataFrame\n",
    "        feature:  feature \n",
    "        bins:    (e.g., [0, 0.25, 0.5, 0.75, 1.0])\n",
    "    Returns:\n",
    "        DataFrame ( feature one-hot encoding  )\n",
    "    \"\"\"\n",
    "    bin_labels = [f\"{feature}_bin_{i}\" for i in range(len(bins) - 1)]\n",
    "    data[f\"{feature}_bin\"] = pd.cut(data[feature], bins=bins, labels=bin_labels, include_lowest=True)\n",
    "    one_hot = pd.get_dummies(data[f\"{feature}_bin\"], prefix=feature)\n",
    "    data = pd.concat([data, one_hot], axis=1).drop(columns=[f\"{feature}_bin\"])\n",
    "    return data\n",
    "\n",
    "# 3.    \n",
    "data = pd.read_csv(\"BTC_upbit_KRW_min5.csv\", index_col=0)\n",
    "data.columns = ['open', 'high', 'low', 'close', 'volume', 'value']\n",
    "data.index = pd.to_datetime(data.index)\n",
    "\n",
    "#   \n",
    "data = calculate_indicators(data)\n",
    "\n",
    "# MinMax Scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features = ['open', 'high', 'low', 'close', 'volume', 'value', 'William_R', 'ATR', 'OBV',\n",
    "                   'Z_Score', 'Entropy', 'SMA_5', 'RSI', 'BB_Upper', 'BB_Middle', 'BB_Lower', 'MACD', 'Stochastic']\n",
    "data[scaled_features] = scaler.fit_transform(data[scaled_features])\n",
    "\n",
    "# Feature   One-Hot Encoding\n",
    "bins_dict = {\n",
    "    'open': np.linspace(0, 1, 6),  # 5 bins\n",
    "    'high': np.linspace(0, 1, 6),\n",
    "    'low': np.linspace(0, 1, 6),\n",
    "    'close': np.linspace(0, 1, 6),\n",
    "    'volume': np.linspace(0, 1, 4),  # 3 bins\n",
    "    'RSI': np.linspace(0, 1, 4),     # 3 bins\n",
    "}\n",
    "for feature, bins in bins_dict.items():\n",
    "    data = bin_and_one_hot_encode(data, feature, bins)\n",
    "\n",
    "# Datetime Feature One-Hot Encoding\n",
    "def encode_datetime_features(data):\n",
    "    data['day_of_week'] = data.index.dayofweek\n",
    "    data['week_of_month'] = (data.index.day - 1) // 7 + 1\n",
    "    data['month'] = data.index.month\n",
    "    for col in ['day_of_week', 'week_of_month', 'month']:\n",
    "        data = pd.concat([data, pd.get_dummies(data[col], prefix=col)], axis=1)\n",
    "        data = data.drop(columns=[col])\n",
    "    return data\n",
    "\n",
    "data = encode_datetime_features(data)\n",
    "\n",
    "# Feature Target \n",
    "data = data.dropna()  #  \n",
    "data = data.astype(np.float32)\n",
    "# Feature Target \n",
    "data = data.dropna()  #  \n",
    "features = [col for col in data.columns if 'bin' in col or '_bin_' in col or 'day_of_week_' in col or 'week_of_month_' in col or 'month_']\n",
    "\n",
    "# `features`    input_dim \n",
    "input_dim = len(features)\n",
    "\n",
    "target_idx = data.columns.get_loc(\"close\")  # Target \n",
    "\n",
    "# 4. Dataset \n",
    "# TimeSeriesDataset \n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, lookback=15, target_idx=-1):\n",
    "        self.data = data\n",
    "        self.lookback = lookback\n",
    "        self.target_idx = target_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.lookback\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # `features`   \n",
    "        x = self.data[idx:idx + self.lookback, :input_dim]  # input_dim  \n",
    "        y = self.data[idx + self.lookback, self.target_idx]\n",
    "        y_target = 1 if y > self.data[idx + self.lookback - 1, self.target_idx] else 0\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y_target, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Dataset \n",
    "data_array = data.values\n",
    "dataset = TimeSeriesDataset(data_array, lookback=15, target_idx=target_idx)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "# 5. Transformer    \n",
    "class EncoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=512, num_heads=8, num_layers=4, ffn_dim=1024, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(512, embedding_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim, nhead=num_heads, dim_feedforward=ffn_dim, batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.token_embedding(x)\n",
    "        positions = torch.arange(x.size(1), device=x.device).unsqueeze(0).expand(x.size(0), -1)\n",
    "        x += self.position_embedding(positions)\n",
    "        x = self.encoder(x)\n",
    "        return self.fc(x[:, -1])  #    \n",
    "\n",
    "\n",
    "# Train the model\n",
    "input_dim = len(features)\n",
    "model = EncoderOnlyTransformer(input_dim=input_dim)\n",
    "\n",
    "# 4.  \n",
    "def train_model(model, dataloader, num_epochs=10, lr=1e-4):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss:.4f}, Accuracy: {correct / total:.4f}\")\n",
    "\n",
    "# 5.     \n",
    "train_model(model, dataloader)\n",
    "torch.save(model.state_dict(), \"encoder_only_transformer_foreth.pth\")\n",
    "print(\"Model saved successfully!\")\n",
    "\n",
    "#  \n",
    "model = EncoderOnlyTransformer(input_dim=input_dim)\n",
    "model.load_state_dict(torch.load(\"encoder_only_transformer_foreth.pth\"))\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 1.  \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m EncoderOnlyTransformer(input_dim\u001b[38;5;241m=\u001b[39minput_dim)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_only_transformer_foreth.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# 1.  \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = EncoderOnlyTransformer(input_dim=input_dim).to(device)\n",
    "model.load_state_dict(torch.load(\"encoder_only_transformer_foreth.pth\"))\n",
    "model.eval()\n",
    "\n",
    "def prepare_test_data(data_array, lookback, target_idx):\n",
    "    \"\"\"\n",
    "         (X)   (y) .\n",
    "    Args:\n",
    "        data_array: numpy array,   \n",
    "        lookback: int,   \n",
    "        target_idx: int,   \n",
    "    Returns:\n",
    "        X: numpy array,   \n",
    "        y: numpy array,  \n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data_array) - lookback):\n",
    "        X.append(data_array[i:i + lookback, :])  # lookback  \n",
    "        y_value = data_array[i + lookback, target_idx]  #  \n",
    "        # (1)  (0) \n",
    "        y_label = 1 if y_value > data_array[i + lookback - 1, target_idx] else 0\n",
    "        y.append(y_label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# 2.    \n",
    "data = pd.read_csv(\"ETH_upbit_KRW_min5.csv\", index_col=0)\n",
    "data.columns = ['open', 'high', 'low', 'close', 'volume', 'value']\n",
    "data.index = pd.to_datetime(data.index)\n",
    "\n",
    "#     \n",
    "data = calculate_indicators(data)\n",
    "data = encode_datetime_features(data)\n",
    "\n",
    "# Feature    (BTC  )\n",
    "data = data.dropna()\n",
    "data[scaled_features] = scaler.transform(data[scaled_features])  # BTC     \n",
    "\n",
    "#   \n",
    "for feature in btc_features:\n",
    "    if feature not in data.columns:\n",
    "        data[feature] = 0  #  0 \n",
    "\n",
    "data = data[btc_features]  #   \n",
    "\n",
    "#    \n",
    "data = data.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "test_data_array = data.values.astype(np.float32)  #  float32 \n",
    "\n",
    "# 3.   \n",
    "X_test, y_test = prepare_test_data(test_data_array, lookback=15, target_idx=btc_features.index('close'))\n",
    "\n",
    "# 4.   \n",
    "def test_hit_ratio(model, X_test, y_test, device='cuda'):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(X_test)):\n",
    "            pred = model(X_test[i:i+1])  #   \n",
    "            predictions.append(pred.argmax(1).item())\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    y_test = y_test.cpu().numpy()\n",
    "\n",
    "    # Hit Ratio \n",
    "    hit_ratio = np.mean(predictions == y_test)\n",
    "    return hit_ratio\n",
    "\n",
    "# 5.   \n",
    "hit_ratio = test_hit_ratio(model, X_test, y_test, device)\n",
    "\n",
    "# 6.  \n",
    "print(f\"Hit Ratio: {hit_ratio:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         open      high       low     close    volume  \\\n",
      "2017-09-26 04:35:00  0.004864  0.004862  0.004916  0.004889  0.000006   \n",
      "2017-09-26 04:40:00  0.004877  0.004849  0.004929  0.004889  0.000004   \n",
      "2017-09-26 04:45:00  0.004877  0.004876  0.004929  0.004896  0.000005   \n",
      "2017-09-26 04:50:00  0.004890  0.004882  0.004935  0.004928  0.000009   \n",
      "2017-09-26 04:55:00  0.004883  0.004862  0.004935  0.004909  0.000004   \n",
      "\n",
      "                            value  William_R       ATR       OBV   Z_Score  \\\n",
      "2017-09-26 04:35:00  3.430454e-07   0.750000  0.000967  0.214222  0.610165   \n",
      "2017-09-26 04:40:00  2.538935e-07   0.687500  0.000879  0.214222  0.600447   \n",
      "2017-09-26 04:45:00  3.057546e-07   0.733333  0.000852  0.214222  0.602836   \n",
      "2017-09-26 04:50:00  5.748854e-07   1.000000  0.000826  0.214222  0.660790   \n",
      "2017-09-26 04:55:00  2.280902e-07   0.812500  0.000802  0.214222  0.619589   \n",
      "\n",
      "                     ...  Month_8_Bin_0  Month_8_Bin_99  Month_9_Bin_0  \\\n",
      "2017-09-26 04:35:00  ...            1.0             0.0            0.0   \n",
      "2017-09-26 04:40:00  ...            1.0             0.0            0.0   \n",
      "2017-09-26 04:45:00  ...            1.0             0.0            0.0   \n",
      "2017-09-26 04:50:00  ...            1.0             0.0            0.0   \n",
      "2017-09-26 04:55:00  ...            1.0             0.0            0.0   \n",
      "\n",
      "                     Month_9_Bin_99  Month_10_Bin_0  Month_10_Bin_99  \\\n",
      "2017-09-26 04:35:00             1.0             1.0              0.0   \n",
      "2017-09-26 04:40:00             1.0             1.0              0.0   \n",
      "2017-09-26 04:45:00             1.0             1.0              0.0   \n",
      "2017-09-26 04:50:00             1.0             1.0              0.0   \n",
      "2017-09-26 04:55:00             1.0             1.0              0.0   \n",
      "\n",
      "                     Month_11_Bin_0  Month_11_Bin_99  Month_12_Bin_0  \\\n",
      "2017-09-26 04:35:00             1.0              0.0             1.0   \n",
      "2017-09-26 04:40:00             1.0              0.0             1.0   \n",
      "2017-09-26 04:45:00             1.0              0.0             1.0   \n",
      "2017-09-26 04:50:00             1.0              0.0             1.0   \n",
      "2017-09-26 04:55:00             1.0              0.0             1.0   \n",
      "\n",
      "                     Month_12_Bin_99  \n",
      "2017-09-26 04:35:00              0.0  \n",
      "2017-09-26 04:40:00              0.0  \n",
      "2017-09-26 04:45:00              0.0  \n",
      "2017-09-26 04:50:00              0.0  \n",
      "2017-09-26 04:55:00              0.0  \n",
      "\n",
      "[5 rows x 2204 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (480x2204 and 70x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 200\u001b[0m\n\u001b[0;32m    196\u001b[0m                 total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExperiment \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mtotal\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 200\u001b[0m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 182\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(data, num_experiments, lookback, num_epochs)\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mtotal\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    181\u001b[0m \u001b[38;5;66;03m#  \u001b[39;00m\n\u001b[1;32m--> 182\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m#  \u001b[39;00m\n\u001b[0;32m    185\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), model_path)\n",
      "Cell \u001b[1;32mIn[1], line 170\u001b[0m, in \u001b[0;36mtrain_and_evaluate.<locals>.train_model\u001b[1;34m(model, train_loader, val_loader, num_epochs, lr)\u001b[0m\n\u001b[0;32m    168\u001b[0m x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    169\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 170\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y)\n\u001b[0;32m    172\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 110\u001b[0m, in \u001b[0;36mEncoderOnlyTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    109\u001b[0m     batch_size, seq_len, feature_dim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m--> 110\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m     positions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(seq_len)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(batch_size, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    112\u001b[0m     x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding(positions)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (480x2204 and 70x512)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta as ta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#    \n",
    "def calculate_indicators(data):\n",
    "    data['William_R'] = ta.willr(data['high'], data['low'], data['close'])\n",
    "    data['ATR'] = ta.atr(data['high'], data['low'], data['close'])\n",
    "    data['OBV'] = ta.obv(data['close'], data['volume'])\n",
    "    data['Z_Score'] = (data['close'] - data['close'].rolling(window=20).mean()) / data['close'].rolling(window=20).std()\n",
    "    data['Entropy'] = ta.entropy(data['close'], length=14)\n",
    "    data['SMA_5'] = data['close'].rolling(window=5).mean()\n",
    "    data['SMA_10'] = data['close'].rolling(window=10).mean()\n",
    "    data['SMA_20'] = data['close'].rolling(window=20).mean()\n",
    "    data['SMA_60'] = data['close'].rolling(window=60).mean()\n",
    "    data['SMA_120'] = data['close'].rolling(window=120).mean()\n",
    "    data['RSI'] = ta.rsi(data['close'])\n",
    "    bb = ta.bbands(data['close'])\n",
    "    data['BB_Upper'], data['BB_Middle'], data['BB_Lower'] = bb.iloc[:, 0], bb.iloc[:, 1], bb.iloc[:, 2]\n",
    "    macd = ta.macd(data['close'])\n",
    "    data['MACD'] = macd.iloc[:, 0]\n",
    "    data['Stochastic'] = ta.stoch(data['high'], data['low'], data['close']).iloc[:, 0]\n",
    "    return data.dropna()\n",
    "\n",
    "# Datetime Feature One-Hot Encoding\n",
    "def encode_datetime_features(data):\n",
    "    if 'datetime' not in data.columns:\n",
    "        data['datetime'] = pd.to_datetime(data.index)\n",
    "    data['hour_of_day'] = data['datetime'].dt.hour\n",
    "    data['day_of_week'] = data['datetime'].dt.dayofweek\n",
    "    data['week_of_month'] = (data['datetime'].dt.day - 1) // 7 + 1\n",
    "    data['month'] = data['datetime'].dt.month\n",
    "    hour_one_hot = pd.get_dummies(data['hour_of_day'], prefix='Hour')\n",
    "    day_one_hot = pd.get_dummies(data['day_of_week'], prefix='Day')\n",
    "    week_one_hot = pd.get_dummies(data['week_of_month'], prefix='Week')\n",
    "    month_one_hot = pd.get_dummies(data['month'], prefix='Month')\n",
    "    return pd.concat([data, hour_one_hot, day_one_hot, week_one_hot, month_one_hot], axis=1)\n",
    "\n",
    "\n",
    "#    \n",
    "data = pd.read_csv(\"BTC_upbit_KRW_min5.csv\", index_col=0)\n",
    "data.columns = ['open', 'high', 'low', 'close', 'volume', 'value']\n",
    "data.index = pd.to_datetime(data.index)\n",
    "data = calculate_indicators(data)\n",
    "data = encode_datetime_features(data)\n",
    "\n",
    "# Feature   \n",
    "features = ['open', 'high', 'low', 'close', 'volume', 'value', 'William_R',\n",
    "            'ATR', 'OBV', 'Z_Score', 'Entropy', 'SMA_5', 'SMA_10', 'SMA_20', 'SMA_60', 'SMA_120', 'RSI', \n",
    "            'BB_Upper', 'BB_Middle', 'BB_Lower', 'MACD', 'Stochastic'] + \\\n",
    "            list(data.filter(regex='Hour_').columns) + list(data.filter(regex='Day_').columns) + \\\n",
    "            list(data.filter(regex='Week_').columns) + list(data.filter(regex='Month_').columns)\n",
    "\n",
    "data = data[features].dropna()\n",
    "scaler = MinMaxScaler()\n",
    "data[features] = scaler.fit_transform(data[features])\n",
    "\n",
    "def bin_and_encode(data, features, bins=100):\n",
    "    for feature in features:\n",
    "        # \n",
    "        data[f'{feature}_Bin'] = pd.cut(data[feature], bins=bins, labels=False)\n",
    "        # One-Hot Encoding\n",
    "        one_hot = pd.get_dummies(data[f'{feature}_Bin'], prefix=f'{feature}_Bin')\n",
    "        #   \n",
    "        data = pd.concat([data, one_hot], axis=1)\n",
    "        #   column \n",
    "        data.drop(columns=[f'{feature}_Bin'], inplace=True)\n",
    "    #    float32 \n",
    "    data = data.astype(np.float32)\n",
    "    return data\n",
    "\n",
    "\n",
    "#   One-Hot Encoding \n",
    "data = bin_and_encode(data, features)\n",
    "\n",
    "# Dataset \n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, lookback=15, target_idx=-1):\n",
    "        self.data = data\n",
    "        self.lookback = lookback\n",
    "        self.target_idx = target_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.lookback\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.lookback, :]\n",
    "        y = self.data[idx + self.lookback, self.target_idx]\n",
    "        y_target = 1 if y > self.data[idx + self.lookback - 1, self.target_idx] else 0\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y_target, dtype=torch.long)\n",
    "\n",
    "# Transformer  \n",
    "class EncoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=512, num_heads=8, num_layers=4, ffn_dim=1024, num_classes=2):\n",
    "        super(EncoderOnlyTransformer, self).__init__()\n",
    "        self.token_embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(512, embedding_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim, nhead=num_heads, dim_feedforward=ffn_dim\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, feature_dim = x.shape\n",
    "        x = self.token_embedding(x)\n",
    "        positions = torch.arange(seq_len).unsqueeze(0).repeat(batch_size, 1).to(x.device)\n",
    "        x += self.position_embedding(positions)\n",
    "        x = x.permute(1, 0, 2)  # Convert to (seq_len, batch_size, embedding_dim)\n",
    "        x = self.encoder(x)\n",
    "        x = x[-1]  # Take the last token's representation\n",
    "        return self.fc(x)\n",
    "\n",
    "#    \n",
    "def train_and_evaluate(data, num_experiments=21, lookback=15, num_epochs=10):\n",
    "    input_dim = len(features)\n",
    "    step_size = 25000  #   \n",
    "\n",
    "    for exp in range(num_experiments):\n",
    "        train_start = exp * step_size\n",
    "        train_end = train_start + step_size * 8  #   (8 )\n",
    "        val_end = train_end + step_size  #   (1 )\n",
    "        test_end = val_end + step_size  #   (1 )\n",
    "        print(data.head())\n",
    "\n",
    "        if test_end > len(data):\n",
    "            break\n",
    "\n",
    "        train_data = data[train_start:train_end]\n",
    "        val_data = data[train_end:val_end]\n",
    "        test_data = data[val_end:test_end]\n",
    "\n",
    "        train_dataset = TimeSeriesDataset(train_data.values, lookback=lookback, target_idx=features.index('close'))\n",
    "        val_dataset = TimeSeriesDataset(val_data.values, lookback=lookback, target_idx=features.index('close'))\n",
    "        test_dataset = TimeSeriesDataset(test_data.values, lookback=lookback, target_idx=features.index('close'))\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "        #  \n",
    "        model = EncoderOnlyTransformer(input_dim=input_dim).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Fine-tuning:    (exp > 0 )\n",
    "        model_path = f\"model_experiment_{exp}.pth\"\n",
    "        if exp > 0:\n",
    "            try:\n",
    "                model.load_state_dict(torch.load(f\"model_experiment_{exp - 1}.pth\"))  #    \n",
    "                print(f\"Loaded model from experiment {exp - 1}.\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Model file for experiment {exp - 1} not found. Starting fresh training.\")\n",
    "\n",
    "        def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-4):\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            model.to(device)\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                model.train()\n",
    "                total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "                for x, y in train_loader:\n",
    "                    x, y = x.to(device), y.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(x)\n",
    "                    loss = criterion(outputs, y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    total_loss += loss.item()\n",
    "                    correct += (outputs.argmax(1) == y).sum().item()\n",
    "                    total += y.size(0)\n",
    "\n",
    "                print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {total_loss:.4f}, Train Accuracy: {correct / total:.4f}\")\n",
    "\n",
    "        #  \n",
    "        train_model(model, train_loader, val_loader, num_epochs)\n",
    "\n",
    "        #  \n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Saved model for experiment {exp}.\")\n",
    "\n",
    "        # \n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                x, y = x.to('cuda' if torch.cuda.is_available() else 'cpu'), y.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                outputs = model(x)\n",
    "                correct += (outputs.argmax(1) == y).sum().item()\n",
    "                total += y.size(0)\n",
    "\n",
    "        print(f\"Experiment {exp + 1}, Test Accuracy: {correct / total:.4f}\")\n",
    "\n",
    "train_and_evaluate(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 356.0320, Train Accuracy: 0.4985\n",
      "Epoch 2/10, Train Loss: 349.0838, Train Accuracy: 0.5025\n",
      "Epoch 3/10, Train Loss: 348.5795, Train Accuracy: 0.4978\n",
      "Epoch 4/10, Train Loss: 347.4646, Train Accuracy: 0.5051\n",
      "Epoch 5/10, Train Loss: 347.2377, Train Accuracy: 0.5049\n",
      "Epoch 6/10, Train Loss: 347.5045, Train Accuracy: 0.5065\n",
      "Epoch 7/10, Train Loss: 347.7260, Train Accuracy: 0.5050\n",
      "Epoch 8/10, Train Loss: 346.2214, Train Accuracy: 0.5218\n",
      "Epoch 9/10, Train Loss: 345.5038, Train Accuracy: 0.5272\n",
      "Epoch 10/10, Train Loss: 344.9985, Train Accuracy: 0.5371\n",
      "Saved model for experiment 0.\n",
      "Experiment 1, Test Accuracy: 0.5471\n",
      "Loaded model from experiment 0.\n",
      "Epoch 1/10, Train Loss: 345.3740, Train Accuracy: 0.5331\n",
      "Epoch 2/10, Train Loss: 345.6932, Train Accuracy: 0.5272\n",
      "Epoch 3/10, Train Loss: 344.3195, Train Accuracy: 0.5407\n",
      "Epoch 4/10, Train Loss: 344.2962, Train Accuracy: 0.5416\n",
      "Epoch 5/10, Train Loss: 344.3541, Train Accuracy: 0.5456\n",
      "Epoch 6/10, Train Loss: 344.1669, Train Accuracy: 0.5442\n",
      "Epoch 7/10, Train Loss: 343.8638, Train Accuracy: 0.5427\n",
      "Epoch 8/10, Train Loss: 343.2254, Train Accuracy: 0.5469\n",
      "Epoch 9/10, Train Loss: 343.0732, Train Accuracy: 0.5506\n",
      "Epoch 10/10, Train Loss: 343.9873, Train Accuracy: 0.5433\n",
      "Saved model for experiment 1.\n",
      "Experiment 2, Test Accuracy: 0.5295\n",
      "Loaded model from experiment 1.\n",
      "Epoch 1/10, Train Loss: 344.4609, Train Accuracy: 0.5403\n",
      "Epoch 2/10, Train Loss: 344.4992, Train Accuracy: 0.5421\n",
      "Epoch 3/10, Train Loss: 343.9096, Train Accuracy: 0.5414\n",
      "Epoch 4/10, Train Loss: 344.4852, Train Accuracy: 0.5324\n",
      "Epoch 5/10, Train Loss: 343.1911, Train Accuracy: 0.5528\n",
      "Epoch 6/10, Train Loss: 344.2309, Train Accuracy: 0.5429\n",
      "Epoch 7/10, Train Loss: 344.1772, Train Accuracy: 0.5447\n",
      "Epoch 8/10, Train Loss: 343.7610, Train Accuracy: 0.5444\n",
      "Epoch 9/10, Train Loss: 344.5792, Train Accuracy: 0.5399\n",
      "Epoch 10/10, Train Loss: 344.3947, Train Accuracy: 0.5386\n",
      "Saved model for experiment 2.\n",
      "Experiment 3, Test Accuracy: 0.5416\n",
      "Loaded model from experiment 2.\n",
      "Epoch 1/10, Train Loss: 343.6076, Train Accuracy: 0.5439\n",
      "Epoch 2/10, Train Loss: 344.2165, Train Accuracy: 0.5423\n",
      "Epoch 3/10, Train Loss: 345.2556, Train Accuracy: 0.5344\n",
      "Epoch 4/10, Train Loss: 343.5305, Train Accuracy: 0.5465\n",
      "Epoch 5/10, Train Loss: 343.3498, Train Accuracy: 0.5510\n",
      "Epoch 6/10, Train Loss: 343.8117, Train Accuracy: 0.5490\n",
      "Epoch 7/10, Train Loss: 343.6665, Train Accuracy: 0.5486\n",
      "Epoch 8/10, Train Loss: 343.4784, Train Accuracy: 0.5488\n",
      "Epoch 9/10, Train Loss: 343.0517, Train Accuracy: 0.5558\n",
      "Epoch 10/10, Train Loss: 343.0305, Train Accuracy: 0.5491\n",
      "Saved model for experiment 3.\n",
      "Experiment 4, Test Accuracy: 0.5531\n",
      "Loaded model from experiment 3.\n",
      "Epoch 1/10, Train Loss: 343.2539, Train Accuracy: 0.5518\n",
      "Epoch 2/10, Train Loss: 343.2502, Train Accuracy: 0.5540\n",
      "Epoch 3/10, Train Loss: 343.3516, Train Accuracy: 0.5488\n",
      "Epoch 4/10, Train Loss: 342.7112, Train Accuracy: 0.5555\n",
      "Epoch 5/10, Train Loss: 343.2215, Train Accuracy: 0.5475\n",
      "Epoch 6/10, Train Loss: 343.5355, Train Accuracy: 0.5430\n",
      "Epoch 7/10, Train Loss: 343.0161, Train Accuracy: 0.5522\n",
      "Epoch 8/10, Train Loss: 343.4600, Train Accuracy: 0.5470\n",
      "Epoch 9/10, Train Loss: 344.0145, Train Accuracy: 0.5384\n",
      "Epoch 10/10, Train Loss: 343.9926, Train Accuracy: 0.5369\n",
      "Saved model for experiment 4.\n",
      "Experiment 5, Test Accuracy: 0.5395\n",
      "Loaded model from experiment 4.\n",
      "Epoch 1/10, Train Loss: 344.7226, Train Accuracy: 0.5406\n",
      "Epoch 2/10, Train Loss: 343.7908, Train Accuracy: 0.5468\n",
      "Epoch 3/10, Train Loss: 343.7606, Train Accuracy: 0.5468\n",
      "Epoch 4/10, Train Loss: 344.1989, Train Accuracy: 0.5378\n",
      "Epoch 5/10, Train Loss: 344.3544, Train Accuracy: 0.5344\n",
      "Epoch 6/10, Train Loss: 344.5017, Train Accuracy: 0.5381\n",
      "Epoch 7/10, Train Loss: 343.7005, Train Accuracy: 0.5474\n",
      "Epoch 8/10, Train Loss: 343.3316, Train Accuracy: 0.5536\n",
      "Epoch 9/10, Train Loss: 343.5427, Train Accuracy: 0.5456\n",
      "Epoch 10/10, Train Loss: 343.4799, Train Accuracy: 0.5474\n",
      "Saved model for experiment 5.\n",
      "Experiment 6, Test Accuracy: 0.5068\n",
      "Loaded model from experiment 5.\n",
      "Epoch 1/10, Train Loss: 344.1993, Train Accuracy: 0.5439\n",
      "Epoch 2/10, Train Loss: 344.9708, Train Accuracy: 0.5275\n",
      "Epoch 3/10, Train Loss: 344.0612, Train Accuracy: 0.5440\n",
      "Epoch 4/10, Train Loss: 344.7984, Train Accuracy: 0.5294\n",
      "Epoch 5/10, Train Loss: 346.8719, Train Accuracy: 0.5056\n",
      "Epoch 6/10, Train Loss: 346.8377, Train Accuracy: 0.5003\n",
      "Epoch 7/10, Train Loss: 346.9468, Train Accuracy: 0.4968\n",
      "Epoch 8/10, Train Loss: 346.8123, Train Accuracy: 0.5088\n",
      "Epoch 9/10, Train Loss: 346.3958, Train Accuracy: 0.5105\n",
      "Epoch 10/10, Train Loss: 345.5365, Train Accuracy: 0.5239\n",
      "Saved model for experiment 6.\n",
      "Experiment 7, Test Accuracy: 0.5053\n",
      "Loaded model from experiment 6.\n",
      "Epoch 1/10, Train Loss: 346.3834, Train Accuracy: 0.5105\n",
      "Epoch 2/10, Train Loss: 346.7986, Train Accuracy: 0.4929\n",
      "Epoch 3/10, Train Loss: 346.7960, Train Accuracy: 0.5039\n",
      "Epoch 4/10, Train Loss: 346.9920, Train Accuracy: 0.4896\n",
      "Epoch 5/10, Train Loss: 346.8315, Train Accuracy: 0.4964\n",
      "Epoch 6/10, Train Loss: 346.8284, Train Accuracy: 0.5007\n",
      "Epoch 7/10, Train Loss: 346.6650, Train Accuracy: 0.5047\n",
      "Epoch 8/10, Train Loss: 346.8027, Train Accuracy: 0.5000\n",
      "Epoch 9/10, Train Loss: 346.8594, Train Accuracy: 0.4930\n",
      "Epoch 10/10, Train Loss: 346.6837, Train Accuracy: 0.5057\n",
      "Saved model for experiment 7.\n",
      "Experiment 8, Test Accuracy: 0.4967\n",
      "Loaded model from experiment 7.\n",
      "Epoch 1/10, Train Loss: 346.8836, Train Accuracy: 0.4975\n",
      "Epoch 2/10, Train Loss: 346.8429, Train Accuracy: 0.5005\n",
      "Epoch 3/10, Train Loss: 346.7667, Train Accuracy: 0.4963\n",
      "Epoch 4/10, Train Loss: 346.8473, Train Accuracy: 0.4943\n",
      "Epoch 5/10, Train Loss: 346.7893, Train Accuracy: 0.4985\n",
      "Epoch 6/10, Train Loss: 346.7919, Train Accuracy: 0.4973\n",
      "Epoch 7/10, Train Loss: 346.6725, Train Accuracy: 0.5050\n",
      "Epoch 8/10, Train Loss: 346.6722, Train Accuracy: 0.5032\n",
      "Epoch 9/10, Train Loss: 346.8335, Train Accuracy: 0.4994\n",
      "Epoch 10/10, Train Loss: 346.8079, Train Accuracy: 0.4944\n",
      "Saved model for experiment 8.\n",
      "Experiment 9, Test Accuracy: 0.5053\n",
      "Loaded model from experiment 8.\n",
      "Epoch 1/10, Train Loss: 346.8369, Train Accuracy: 0.4988\n",
      "Epoch 2/10, Train Loss: 346.7294, Train Accuracy: 0.5032\n",
      "Epoch 3/10, Train Loss: 346.7800, Train Accuracy: 0.5033\n",
      "Epoch 4/10, Train Loss: 346.7896, Train Accuracy: 0.4943\n",
      "Epoch 5/10, Train Loss: 346.7289, Train Accuracy: 0.5016\n",
      "Epoch 6/10, Train Loss: 346.8920, Train Accuracy: 0.4940\n",
      "Epoch 7/10, Train Loss: 346.9013, Train Accuracy: 0.4934\n",
      "Epoch 8/10, Train Loss: 346.7991, Train Accuracy: 0.4937\n",
      "Epoch 9/10, Train Loss: 346.7716, Train Accuracy: 0.5090\n",
      "Epoch 10/10, Train Loss: 346.7817, Train Accuracy: 0.4945\n",
      "Saved model for experiment 9.\n",
      "Experiment 10, Test Accuracy: 0.4776\n",
      "Loaded model from experiment 9.\n",
      "Epoch 1/10, Train Loss: 346.8090, Train Accuracy: 0.5007\n",
      "Epoch 2/10, Train Loss: 346.8391, Train Accuracy: 0.4953\n",
      "Epoch 3/10, Train Loss: 346.7233, Train Accuracy: 0.4965\n",
      "Epoch 4/10, Train Loss: 346.8383, Train Accuracy: 0.4985\n",
      "Epoch 5/10, Train Loss: 346.8645, Train Accuracy: 0.4935\n",
      "Epoch 6/10, Train Loss: 346.8038, Train Accuracy: 0.4997\n",
      "Epoch 7/10, Train Loss: 346.8288, Train Accuracy: 0.5015\n",
      "Epoch 8/10, Train Loss: 346.8834, Train Accuracy: 0.4998\n",
      "Epoch 9/10, Train Loss: 346.8043, Train Accuracy: 0.5008\n",
      "Epoch 10/10, Train Loss: 346.8251, Train Accuracy: 0.4930\n",
      "Saved model for experiment 10.\n",
      "Experiment 11, Test Accuracy: 0.4952\n",
      "Loaded model from experiment 10.\n",
      "Epoch 1/10, Train Loss: 346.7505, Train Accuracy: 0.4972\n",
      "Epoch 2/10, Train Loss: 346.7453, Train Accuracy: 0.4995\n",
      "Epoch 3/10, Train Loss: 346.8720, Train Accuracy: 0.4952\n",
      "Epoch 4/10, Train Loss: 346.7441, Train Accuracy: 0.5003\n",
      "Epoch 5/10, Train Loss: 346.8315, Train Accuracy: 0.5002\n",
      "Epoch 6/10, Train Loss: 346.7905, Train Accuracy: 0.5004\n",
      "Epoch 7/10, Train Loss: 346.7837, Train Accuracy: 0.4983\n",
      "Epoch 8/10, Train Loss: 346.7466, Train Accuracy: 0.4962\n",
      "Epoch 9/10, Train Loss: 346.8312, Train Accuracy: 0.4950\n",
      "Epoch 10/10, Train Loss: 346.7632, Train Accuracy: 0.4978\n",
      "Saved model for experiment 11.\n",
      "Experiment 12, Test Accuracy: 0.5149\n",
      "Loaded model from experiment 11.\n",
      "Epoch 1/10, Train Loss: 346.8064, Train Accuracy: 0.4953\n",
      "Epoch 2/10, Train Loss: 346.8202, Train Accuracy: 0.4976\n",
      "Epoch 3/10, Train Loss: 346.7414, Train Accuracy: 0.5002\n",
      "Epoch 4/10, Train Loss: 346.7243, Train Accuracy: 0.4980\n",
      "Epoch 5/10, Train Loss: 346.8843, Train Accuracy: 0.5003\n",
      "Epoch 6/10, Train Loss: 346.6702, Train Accuracy: 0.5010\n",
      "Epoch 7/10, Train Loss: 346.7638, Train Accuracy: 0.5001\n",
      "Epoch 8/10, Train Loss: 346.9284, Train Accuracy: 0.4975\n",
      "Epoch 9/10, Train Loss: 346.6723, Train Accuracy: 0.5069\n",
      "Epoch 10/10, Train Loss: 346.7039, Train Accuracy: 0.5013\n",
      "Saved model for experiment 12.\n",
      "Experiment 13, Test Accuracy: 0.5068\n",
      "Loaded model from experiment 12.\n",
      "Epoch 1/10, Train Loss: 346.8107, Train Accuracy: 0.4920\n",
      "Epoch 2/10, Train Loss: 346.7562, Train Accuracy: 0.5012\n",
      "Epoch 3/10, Train Loss: 346.6826, Train Accuracy: 0.5052\n",
      "Epoch 4/10, Train Loss: 346.8334, Train Accuracy: 0.4963\n",
      "Epoch 5/10, Train Loss: 346.8312, Train Accuracy: 0.5003\n",
      "Epoch 6/10, Train Loss: 346.7327, Train Accuracy: 0.5019\n",
      "Epoch 7/10, Train Loss: 346.6843, Train Accuracy: 0.4938\n",
      "Epoch 8/10, Train Loss: 346.9075, Train Accuracy: 0.4958\n",
      "Epoch 9/10, Train Loss: 346.7444, Train Accuracy: 0.5017\n",
      "Epoch 10/10, Train Loss: 346.8504, Train Accuracy: 0.5023\n",
      "Saved model for experiment 13.\n",
      "Experiment 14, Test Accuracy: 0.5144\n",
      "Loaded model from experiment 13.\n",
      "Epoch 1/10, Train Loss: 346.7018, Train Accuracy: 0.4983\n",
      "Epoch 2/10, Train Loss: 346.7931, Train Accuracy: 0.4996\n",
      "Epoch 3/10, Train Loss: 346.8941, Train Accuracy: 0.4995\n",
      "Epoch 4/10, Train Loss: 346.8042, Train Accuracy: 0.4959\n",
      "Epoch 5/10, Train Loss: 346.7910, Train Accuracy: 0.4942\n",
      "Epoch 6/10, Train Loss: 346.7883, Train Accuracy: 0.4965\n",
      "Epoch 7/10, Train Loss: 346.6700, Train Accuracy: 0.5053\n",
      "Epoch 8/10, Train Loss: 346.8496, Train Accuracy: 0.5002\n",
      "Epoch 9/10, Train Loss: 346.7177, Train Accuracy: 0.4993\n",
      "Epoch 10/10, Train Loss: 346.7735, Train Accuracy: 0.5012\n",
      "Saved model for experiment 14.\n",
      "Experiment 15, Test Accuracy: 0.5214\n",
      "Loaded model from experiment 14.\n",
      "Epoch 1/10, Train Loss: 346.9517, Train Accuracy: 0.4966\n",
      "Epoch 2/10, Train Loss: 346.7218, Train Accuracy: 0.5037\n",
      "Epoch 3/10, Train Loss: 346.6966, Train Accuracy: 0.5028\n",
      "Epoch 4/10, Train Loss: 346.5533, Train Accuracy: 0.5065\n",
      "Epoch 5/10, Train Loss: 346.6831, Train Accuracy: 0.5050\n",
      "Epoch 6/10, Train Loss: 346.7307, Train Accuracy: 0.4987\n",
      "Epoch 7/10, Train Loss: 346.7318, Train Accuracy: 0.5046\n",
      "Epoch 8/10, Train Loss: 346.7216, Train Accuracy: 0.5069\n",
      "Epoch 9/10, Train Loss: 346.6910, Train Accuracy: 0.5013\n",
      "Epoch 10/10, Train Loss: 346.7999, Train Accuracy: 0.5046\n",
      "Saved model for experiment 15.\n",
      "Experiment 16, Test Accuracy: 0.5224\n",
      "Loaded model from experiment 15.\n",
      "Epoch 1/10, Train Loss: 346.6969, Train Accuracy: 0.5018\n",
      "Epoch 2/10, Train Loss: 346.7071, Train Accuracy: 0.5050\n",
      "Epoch 3/10, Train Loss: 346.7422, Train Accuracy: 0.5005\n",
      "Epoch 4/10, Train Loss: 346.7447, Train Accuracy: 0.4952\n",
      "Epoch 5/10, Train Loss: 346.7182, Train Accuracy: 0.4990\n",
      "Epoch 6/10, Train Loss: 346.6970, Train Accuracy: 0.5055\n",
      "Epoch 7/10, Train Loss: 346.7498, Train Accuracy: 0.4999\n",
      "Epoch 8/10, Train Loss: 346.7769, Train Accuracy: 0.4998\n",
      "Epoch 9/10, Train Loss: 346.7218, Train Accuracy: 0.4982\n",
      "Epoch 10/10, Train Loss: 346.7891, Train Accuracy: 0.4968\n",
      "Saved model for experiment 16.\n",
      "Experiment 17, Test Accuracy: 0.4997\n",
      "Loaded model from experiment 16.\n",
      "Epoch 1/10, Train Loss: 346.6583, Train Accuracy: 0.5075\n",
      "Epoch 2/10, Train Loss: 346.5254, Train Accuracy: 0.5071\n",
      "Epoch 3/10, Train Loss: 346.8737, Train Accuracy: 0.4997\n",
      "Epoch 4/10, Train Loss: 346.7637, Train Accuracy: 0.4991\n",
      "Epoch 5/10, Train Loss: 346.6537, Train Accuracy: 0.5062\n",
      "Epoch 6/10, Train Loss: 346.7374, Train Accuracy: 0.5017\n",
      "Epoch 7/10, Train Loss: 346.6842, Train Accuracy: 0.5035\n",
      "Epoch 8/10, Train Loss: 346.7251, Train Accuracy: 0.5008\n",
      "Epoch 9/10, Train Loss: 346.6757, Train Accuracy: 0.4995\n",
      "Epoch 10/10, Train Loss: 346.5468, Train Accuracy: 0.5090\n",
      "Saved model for experiment 17.\n",
      "Experiment 18, Test Accuracy: 0.4846\n",
      "Loaded model from experiment 17.\n",
      "Epoch 1/10, Train Loss: 346.6854, Train Accuracy: 0.5047\n",
      "Epoch 2/10, Train Loss: 346.8377, Train Accuracy: 0.4992\n",
      "Epoch 3/10, Train Loss: 346.6293, Train Accuracy: 0.5049\n",
      "Epoch 4/10, Train Loss: 346.8176, Train Accuracy: 0.4985\n",
      "Epoch 5/10, Train Loss: 346.7010, Train Accuracy: 0.5010\n",
      "Epoch 6/10, Train Loss: 346.7270, Train Accuracy: 0.5041\n",
      "Epoch 7/10, Train Loss: 346.7897, Train Accuracy: 0.5039\n",
      "Epoch 8/10, Train Loss: 346.6061, Train Accuracy: 0.5050\n",
      "Epoch 9/10, Train Loss: 346.7117, Train Accuracy: 0.4987\n",
      "Epoch 10/10, Train Loss: 346.6850, Train Accuracy: 0.5050\n",
      "Saved model for experiment 18.\n",
      "Experiment 19, Test Accuracy: 0.4922\n",
      "Loaded model from experiment 18.\n",
      "Epoch 1/10, Train Loss: 346.6834, Train Accuracy: 0.5007\n",
      "Epoch 2/10, Train Loss: 346.7556, Train Accuracy: 0.5021\n",
      "Epoch 3/10, Train Loss: 346.7179, Train Accuracy: 0.4992\n",
      "Epoch 4/10, Train Loss: 346.7333, Train Accuracy: 0.5027\n",
      "Epoch 5/10, Train Loss: 346.8090, Train Accuracy: 0.5017\n",
      "Epoch 6/10, Train Loss: 346.6992, Train Accuracy: 0.5023\n",
      "Epoch 7/10, Train Loss: 346.8035, Train Accuracy: 0.4936\n",
      "Epoch 8/10, Train Loss: 346.6987, Train Accuracy: 0.4992\n",
      "Epoch 9/10, Train Loss: 346.8137, Train Accuracy: 0.4924\n",
      "Epoch 10/10, Train Loss: 346.7115, Train Accuracy: 0.4995\n",
      "Saved model for experiment 19.\n",
      "Experiment 20, Test Accuracy: 0.4892\n",
      "Loaded model from experiment 19.\n",
      "Epoch 1/10, Train Loss: 346.7419, Train Accuracy: 0.4945\n",
      "Epoch 2/10, Train Loss: 346.7253, Train Accuracy: 0.4977\n",
      "Epoch 3/10, Train Loss: 346.7845, Train Accuracy: 0.4969\n",
      "Epoch 4/10, Train Loss: 346.7659, Train Accuracy: 0.4974\n",
      "Epoch 5/10, Train Loss: 346.6836, Train Accuracy: 0.5065\n",
      "Epoch 6/10, Train Loss: 346.8012, Train Accuracy: 0.4983\n",
      "Epoch 7/10, Train Loss: 346.7320, Train Accuracy: 0.4997\n",
      "Epoch 8/10, Train Loss: 346.8134, Train Accuracy: 0.5013\n",
      "Epoch 9/10, Train Loss: 346.7006, Train Accuracy: 0.4978\n",
      "Epoch 10/10, Train Loss: 346.7295, Train Accuracy: 0.4993\n",
      "Saved model for experiment 20.\n",
      "Experiment 21, Test Accuracy: 0.5043\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta as ta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#    \n",
    "def calculate_indicators(data):\n",
    "    data['William_R'] = ta.willr(data['high'], data['low'], data['close'])\n",
    "    data['ATR'] = ta.atr(data['high'], data['low'], data['close'])\n",
    "    data['OBV'] = ta.obv(data['close'], data['volume'])\n",
    "    data['Z_Score'] = (data['close'] - data['close'].rolling(window=20).mean()) / data['close'].rolling(window=20).std()\n",
    "    data['Entropy'] = ta.entropy(data['close'], length=14)\n",
    "    data['SMA_5'] = data['close'].rolling(window=5).mean()\n",
    "    data['SMA_10'] = data['close'].rolling(window=10).mean()\n",
    "    data['SMA_20'] = data['close'].rolling(window=20).mean()\n",
    "    data['SMA_60'] = data['close'].rolling(window=60).mean()\n",
    "    data['SMA_120'] = data['close'].rolling(window=120).mean()\n",
    "    data['RSI'] = ta.rsi(data['close'])\n",
    "    bb = ta.bbands(data['close'])\n",
    "    data['BB_Upper'], data['BB_Middle'], data['BB_Lower'] = bb.iloc[:, 0], bb.iloc[:, 1], bb.iloc[:, 2]\n",
    "    macd = ta.macd(data['close'])\n",
    "    data['MACD'] = macd.iloc[:, 0]\n",
    "    data['Stochastic'] = ta.stoch(data['high'], data['low'], data['close']).iloc[:, 0]\n",
    "    return data.dropna()\n",
    "\n",
    "# Datetime Feature One-Hot Encoding\n",
    "def encode_datetime_features(data):\n",
    "    if 'datetime' not in data.columns:\n",
    "        data['datetime'] = pd.to_datetime(data.index)\n",
    "    data['hour_of_day'] = data['datetime'].dt.hour\n",
    "    data['day_of_week'] = data['datetime'].dt.dayofweek\n",
    "    data['week_of_month'] = (data['datetime'].dt.day - 1) // 7 + 1\n",
    "    data['month'] = data['datetime'].dt.month\n",
    "    hour_one_hot = pd.get_dummies(data['hour_of_day'], prefix='Hour')\n",
    "    day_one_hot = pd.get_dummies(data['day_of_week'], prefix='Day')\n",
    "    week_one_hot = pd.get_dummies(data['week_of_month'], prefix='Week')\n",
    "    month_one_hot = pd.get_dummies(data['month'], prefix='Month')\n",
    "    return pd.concat([data, hour_one_hot, day_one_hot, week_one_hot, month_one_hot], axis=1)\n",
    "\n",
    "#    \n",
    "data = pd.read_csv(\"BTC_upbit_KRW_min60.csv\", index_col=0)\n",
    "data.columns = ['open', 'high', 'low', 'close', 'volume', 'value']\n",
    "data.index = pd.to_datetime(data.index)\n",
    "data = calculate_indicators(data)\n",
    "data = encode_datetime_features(data)\n",
    "\n",
    "# Feature   \n",
    "features = ['open', 'high', 'low', 'close', 'volume', 'value', 'William_R',\n",
    "            'ATR', 'OBV', 'Z_Score', 'Entropy', 'SMA_5', 'SMA_10', 'SMA_20', 'SMA_60', 'SMA_120', 'RSI', \n",
    "            'BB_Upper', 'BB_Middle', 'BB_Lower', 'MACD', 'Stochastic'] + \\\n",
    "            list(data.filter(regex='Hour_').columns) + list(data.filter(regex='Day_').columns) + \\\n",
    "            list(data.filter(regex='Week_').columns) + list(data.filter(regex='Month_').columns)\n",
    "\n",
    "data = data[features].dropna()\n",
    "scaler = MinMaxScaler()\n",
    "data[features] = scaler.fit_transform(data[features])\n",
    "\n",
    "# Dataset \n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, lookback=15, target_idx=-1):\n",
    "        self.data = data\n",
    "        self.lookback = lookback\n",
    "        self.target_idx = target_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.lookback\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.lookback, :]\n",
    "        y = self.data[idx + self.lookback, self.target_idx]\n",
    "        y_target = 1 if y > self.data[idx + self.lookback - 1, self.target_idx] else 0\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y_target, dtype=torch.long)\n",
    "\n",
    "# Transformer  \n",
    "class EncoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=512, num_heads=8, num_layers=4, ffn_dim=1024, num_classes=2):\n",
    "        super(EncoderOnlyTransformer, self).__init__()\n",
    "        self.token_embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(512, embedding_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim, nhead=num_heads, dim_feedforward=ffn_dim\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, feature_dim = x.shape\n",
    "        x = self.token_embedding(x)\n",
    "        positions = torch.arange(seq_len).unsqueeze(0).repeat(batch_size, 1).to(x.device)\n",
    "        x += self.position_embedding(positions)\n",
    "        x = x.permute(1, 0, 2)  # Convert to (seq_len, batch_size, embedding_dim)\n",
    "        x = self.encoder(x)\n",
    "        x = x[-1]  # Take the last token's representation\n",
    "        return self.fc(x)\n",
    "\n",
    "#    \n",
    "def train_and_evaluate(data, num_experiments=21, lookback=15, num_epochs=10):\n",
    "    input_dim = len(features)\n",
    "    step_size = 2000  #   \n",
    "\n",
    "    for exp in range(num_experiments):\n",
    "        train_start = exp * step_size\n",
    "        train_end = train_start + step_size * 8  #   (8 )\n",
    "        val_end = train_end + step_size  #   (1 )\n",
    "        test_end = val_end + step_size  #   (1 )\n",
    "\n",
    "        if test_end > len(data):\n",
    "            break\n",
    "\n",
    "        train_data = data[train_start:train_end]\n",
    "        val_data = data[train_end:val_end]\n",
    "        test_data = data[val_end:test_end]\n",
    "\n",
    "        train_dataset = TimeSeriesDataset(train_data.values, lookback=lookback, target_idx=features.index('close'))\n",
    "        val_dataset = TimeSeriesDataset(val_data.values, lookback=lookback, target_idx=features.index('close'))\n",
    "        test_dataset = TimeSeriesDataset(test_data.values, lookback=lookback, target_idx=features.index('close'))\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "        #  \n",
    "        model = EncoderOnlyTransformer(input_dim=input_dim).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Fine-tuning:    (exp > 0 )\n",
    "        model_path = f\"model_experiment_{exp}.pth\"\n",
    "        if exp > 0:\n",
    "            try:\n",
    "                model.load_state_dict(torch.load(f\"model_experiment_{exp - 1}.pth\"))  #    \n",
    "                print(f\"Loaded model from experiment {exp - 1}.\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Model file for experiment {exp - 1} not found. Starting fresh training.\")\n",
    "\n",
    "        def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-4):\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            model.to(device)\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                model.train()\n",
    "                total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "                for x, y in train_loader:\n",
    "                    x, y = x.to(device), y.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(x)\n",
    "                    loss = criterion(outputs, y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    total_loss += loss.item()\n",
    "                    correct += (outputs.argmax(1) == y).sum().item()\n",
    "                    total += y.size(0)\n",
    "\n",
    "                print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {total_loss:.4f}, Train Accuracy: {correct / total:.4f}\")\n",
    "\n",
    "        #  \n",
    "        train_model(model, train_loader, val_loader, num_epochs)\n",
    "\n",
    "        #  \n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Saved model for experiment {exp}.\")\n",
    "\n",
    "        # \n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                x, y = x.to('cuda' if torch.cuda.is_available() else 'cpu'), y.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                outputs = model(x)\n",
    "                correct += (outputs.argmax(1) == y).sum().item()\n",
    "                total += y.size(0)\n",
    "\n",
    "        print(f\"Experiment {exp + 1}, Test Accuracy: {correct / total:.4f}\")\n",
    "\n",
    "train_and_evaluate(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 4344.5988, Train Accuracy: 0.5177\n",
      "Epoch 2/10, Train Loss: 4326.8134, Train Accuracy: 0.5259\n",
      "Epoch 3/10, Train Loss: 4324.5882, Train Accuracy: 0.5277\n",
      "Epoch 4/10, Train Loss: 4323.8848, Train Accuracy: 0.5288\n",
      "Epoch 5/10, Train Loss: 4323.5218, Train Accuracy: 0.5284\n",
      "Epoch 6/10, Train Loss: 4323.8476, Train Accuracy: 0.5286\n",
      "Epoch 7/10, Train Loss: 4323.5611, Train Accuracy: 0.5286\n",
      "Epoch 8/10, Train Loss: 4323.2599, Train Accuracy: 0.5289\n",
      "Epoch 9/10, Train Loss: 4323.4070, Train Accuracy: 0.5287\n",
      "Epoch 10/10, Train Loss: 4323.1307, Train Accuracy: 0.5285\n",
      "Saved model for experiment 0.\n",
      "Experiment 1, Test Accuracy: 0.5297\n",
      "Loaded model from experiment 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 4320.7808, Train Accuracy: 0.5331\n",
      "Epoch 2/10, Train Loss: 4320.2245, Train Accuracy: 0.5327\n",
      "Epoch 3/10, Train Loss: 4320.3175, Train Accuracy: 0.5328\n",
      "Epoch 4/10, Train Loss: 4320.3216, Train Accuracy: 0.5331\n",
      "Epoch 5/10, Train Loss: 4320.6044, Train Accuracy: 0.5332\n",
      "Epoch 6/10, Train Loss: 4320.3406, Train Accuracy: 0.5331\n",
      "Epoch 7/10, Train Loss: 4319.8986, Train Accuracy: 0.5328\n",
      "Epoch 8/10, Train Loss: 4320.4472, Train Accuracy: 0.5332\n",
      "Epoch 9/10, Train Loss: 4320.2451, Train Accuracy: 0.5333\n",
      "Epoch 10/10, Train Loss: 4320.1179, Train Accuracy: 0.5329\n",
      "Saved model for experiment 1.\n",
      "Experiment 2, Test Accuracy: 0.5212\n",
      "Loaded model from experiment 1.\n",
      "Epoch 1/10, Train Loss: 4316.1846, Train Accuracy: 0.5371\n",
      "Epoch 2/10, Train Loss: 4316.2848, Train Accuracy: 0.5373\n",
      "Epoch 3/10, Train Loss: 4316.4303, Train Accuracy: 0.5372\n",
      "Epoch 4/10, Train Loss: 4316.3318, Train Accuracy: 0.5373\n",
      "Epoch 5/10, Train Loss: 4316.0879, Train Accuracy: 0.5374\n",
      "Epoch 6/10, Train Loss: 4316.3289, Train Accuracy: 0.5373\n",
      "Epoch 7/10, Train Loss: 4316.1593, Train Accuracy: 0.5374\n",
      "Epoch 8/10, Train Loss: 4316.1592, Train Accuracy: 0.5374\n",
      "Epoch 9/10, Train Loss: 4315.9055, Train Accuracy: 0.5374\n",
      "Epoch 10/10, Train Loss: 4315.8659, Train Accuracy: 0.5374\n",
      "Saved model for experiment 2.\n",
      "Experiment 3, Test Accuracy: 0.5342\n",
      "Loaded model from experiment 2.\n",
      "Epoch 1/10, Train Loss: 4315.8654, Train Accuracy: 0.5375\n",
      "Epoch 2/10, Train Loss: 4316.0674, Train Accuracy: 0.5375\n",
      "Epoch 3/10, Train Loss: 4315.5956, Train Accuracy: 0.5374\n",
      "Epoch 4/10, Train Loss: 4315.8840, Train Accuracy: 0.5375\n",
      "Epoch 5/10, Train Loss: 4315.2539, Train Accuracy: 0.5375\n",
      "Epoch 6/10, Train Loss: 4316.0028, Train Accuracy: 0.5375\n",
      "Epoch 7/10, Train Loss: 4315.8175, Train Accuracy: 0.5375\n",
      "Epoch 8/10, Train Loss: 4315.7674, Train Accuracy: 0.5375\n",
      "Epoch 9/10, Train Loss: 4315.7533, Train Accuracy: 0.5374\n",
      "Epoch 10/10, Train Loss: 4315.6454, Train Accuracy: 0.5375\n",
      "Saved model for experiment 3.\n",
      "Experiment 4, Test Accuracy: 0.5326\n",
      "Loaded model from experiment 3.\n",
      "Epoch 1/10, Train Loss: 4315.5062, Train Accuracy: 0.5378\n",
      "Epoch 2/10, Train Loss: 4315.3780, Train Accuracy: 0.5377\n",
      "Epoch 3/10, Train Loss: 4315.4687, Train Accuracy: 0.5377\n",
      "Epoch 4/10, Train Loss: 4315.0705, Train Accuracy: 0.5378\n",
      "Epoch 5/10, Train Loss: 4315.5637, Train Accuracy: 0.5377\n",
      "Epoch 6/10, Train Loss: 4315.2453, Train Accuracy: 0.5378\n",
      "Epoch 7/10, Train Loss: 4315.2078, Train Accuracy: 0.5378\n",
      "Epoch 8/10, Train Loss: 4315.4218, Train Accuracy: 0.5378\n",
      "Epoch 9/10, Train Loss: 4315.1726, Train Accuracy: 0.5377\n",
      "Epoch 10/10, Train Loss: 4315.1888, Train Accuracy: 0.5378\n",
      "Saved model for experiment 4.\n",
      "Experiment 5, Test Accuracy: 0.5046\n",
      "Loaded model from experiment 4.\n",
      "Epoch 1/10, Train Loss: 4317.7973, Train Accuracy: 0.5351\n",
      "Epoch 2/10, Train Loss: 4317.5523, Train Accuracy: 0.5351\n",
      "Epoch 3/10, Train Loss: 4317.7966, Train Accuracy: 0.5351\n",
      "Epoch 4/10, Train Loss: 4317.7478, Train Accuracy: 0.5351\n",
      "Epoch 5/10, Train Loss: 4317.9339, Train Accuracy: 0.5351\n",
      "Epoch 6/10, Train Loss: 4317.7394, Train Accuracy: 0.5352\n",
      "Epoch 7/10, Train Loss: 4317.6321, Train Accuracy: 0.5351\n",
      "Epoch 8/10, Train Loss: 4317.5319, Train Accuracy: 0.5351\n",
      "Epoch 9/10, Train Loss: 4317.4290, Train Accuracy: 0.5352\n",
      "Epoch 10/10, Train Loss: 4317.5621, Train Accuracy: 0.5351\n",
      "Saved model for experiment 5.\n",
      "Experiment 6, Test Accuracy: 0.4992\n",
      "Loaded model from experiment 5.\n",
      "Epoch 1/10, Train Loss: 4323.3281, Train Accuracy: 0.5279\n",
      "Epoch 2/10, Train Loss: 4323.3133, Train Accuracy: 0.5277\n",
      "Epoch 3/10, Train Loss: 4323.3304, Train Accuracy: 0.5278\n",
      "Epoch 4/10, Train Loss: 4323.6654, Train Accuracy: 0.5278\n",
      "Epoch 5/10, Train Loss: 4323.2647, Train Accuracy: 0.5278\n",
      "Epoch 6/10, Train Loss: 4323.2131, Train Accuracy: 0.5277\n",
      "Epoch 7/10, Train Loss: 4323.0816, Train Accuracy: 0.5277\n",
      "Epoch 8/10, Train Loss: 4323.2758, Train Accuracy: 0.5278\n",
      "Epoch 9/10, Train Loss: 4323.2367, Train Accuracy: 0.5278\n",
      "Epoch 10/10, Train Loss: 4323.3942, Train Accuracy: 0.5279\n",
      "Saved model for experiment 6.\n",
      "Experiment 7, Test Accuracy: 0.5105\n",
      "Loaded model from experiment 6.\n",
      "Epoch 1/10, Train Loss: 4326.1971, Train Accuracy: 0.5232\n",
      "Epoch 2/10, Train Loss: 4325.8477, Train Accuracy: 0.5229\n",
      "Epoch 3/10, Train Loss: 4326.1922, Train Accuracy: 0.5231\n",
      "Epoch 4/10, Train Loss: 4326.0550, Train Accuracy: 0.5230\n",
      "Epoch 5/10, Train Loss: 4326.2194, Train Accuracy: 0.5232\n",
      "Epoch 6/10, Train Loss: 4326.0125, Train Accuracy: 0.5232\n",
      "Epoch 7/10, Train Loss: 4326.2321, Train Accuracy: 0.5233\n",
      "Epoch 8/10, Train Loss: 4326.0939, Train Accuracy: 0.5232\n",
      "Epoch 9/10, Train Loss: 4326.0671, Train Accuracy: 0.5232\n",
      "Epoch 10/10, Train Loss: 4325.8745, Train Accuracy: 0.5231\n",
      "Saved model for experiment 7.\n",
      "Experiment 8, Test Accuracy: 0.5011\n",
      "Loaded model from experiment 7.\n",
      "Epoch 1/10, Train Loss: 4326.6219, Train Accuracy: 0.5223\n",
      "Epoch 2/10, Train Loss: 4326.9041, Train Accuracy: 0.5224\n",
      "Epoch 3/10, Train Loss: 4326.5732, Train Accuracy: 0.5224\n",
      "Epoch 4/10, Train Loss: 4326.3019, Train Accuracy: 0.5224\n",
      "Epoch 5/10, Train Loss: 4326.2910, Train Accuracy: 0.5223\n",
      "Epoch 6/10, Train Loss: 4326.3949, Train Accuracy: 0.5223\n",
      "Epoch 7/10, Train Loss: 4326.5612, Train Accuracy: 0.5223\n",
      "Epoch 8/10, Train Loss: 4326.5496, Train Accuracy: 0.5224\n",
      "Epoch 9/10, Train Loss: 4326.3195, Train Accuracy: 0.5223\n",
      "Epoch 10/10, Train Loss: 4326.5736, Train Accuracy: 0.5224\n",
      "Saved model for experiment 8.\n",
      "Experiment 9, Test Accuracy: 0.5150\n",
      "Loaded model from experiment 8.\n",
      "Epoch 1/10, Train Loss: 4329.4856, Train Accuracy: 0.5165\n",
      "Epoch 2/10, Train Loss: 4329.1353, Train Accuracy: 0.5158\n",
      "Epoch 3/10, Train Loss: 4329.4893, Train Accuracy: 0.5164\n",
      "Epoch 4/10, Train Loss: 4329.3931, Train Accuracy: 0.5165\n",
      "Epoch 5/10, Train Loss: 4329.1198, Train Accuracy: 0.5162\n",
      "Epoch 6/10, Train Loss: 4329.4229, Train Accuracy: 0.5160\n",
      "Epoch 7/10, Train Loss: 4329.4154, Train Accuracy: 0.5163\n",
      "Epoch 8/10, Train Loss: 4329.0762, Train Accuracy: 0.5164\n",
      "Epoch 9/10, Train Loss: 4329.4431, Train Accuracy: 0.5162\n",
      "Epoch 10/10, Train Loss: 4329.4584, Train Accuracy: 0.5165\n",
      "Saved model for experiment 9.\n",
      "Experiment 10, Test Accuracy: 0.5041\n",
      "Loaded model from experiment 9.\n",
      "Epoch 1/10, Train Loss: 4329.8603, Train Accuracy: 0.5143\n",
      "Epoch 2/10, Train Loss: 4330.1136, Train Accuracy: 0.5143\n",
      "Epoch 3/10, Train Loss: 4330.0544, Train Accuracy: 0.5145\n",
      "Epoch 4/10, Train Loss: 4329.9235, Train Accuracy: 0.5142\n",
      "Epoch 5/10, Train Loss: 4329.9787, Train Accuracy: 0.5147\n",
      "Epoch 6/10, Train Loss: 4329.9097, Train Accuracy: 0.5141\n",
      "Epoch 7/10, Train Loss: 4329.8291, Train Accuracy: 0.5145\n",
      "Epoch 8/10, Train Loss: 4329.9706, Train Accuracy: 0.5143\n",
      "Epoch 9/10, Train Loss: 4330.1128, Train Accuracy: 0.5141\n",
      "Epoch 10/10, Train Loss: 4330.0478, Train Accuracy: 0.5144\n",
      "Saved model for experiment 10.\n",
      "Experiment 11, Test Accuracy: 0.5159\n",
      "Loaded model from experiment 10.\n",
      "Epoch 1/10, Train Loss: 4330.7892, Train Accuracy: 0.5112\n",
      "Epoch 2/10, Train Loss: 4330.9132, Train Accuracy: 0.5122\n",
      "Epoch 3/10, Train Loss: 4330.6869, Train Accuracy: 0.5115\n",
      "Epoch 4/10, Train Loss: 4330.8009, Train Accuracy: 0.5122\n",
      "Epoch 5/10, Train Loss: 4330.4815, Train Accuracy: 0.5121\n",
      "Epoch 6/10, Train Loss: 4330.6408, Train Accuracy: 0.5121\n",
      "Epoch 7/10, Train Loss: 4330.7088, Train Accuracy: 0.5114\n",
      "Epoch 8/10, Train Loss: 4330.5469, Train Accuracy: 0.5123\n",
      "Epoch 9/10, Train Loss: 4330.7050, Train Accuracy: 0.5117\n",
      "Epoch 10/10, Train Loss: 4330.8268, Train Accuracy: 0.5119\n",
      "Saved model for experiment 11.\n",
      "Experiment 12, Test Accuracy: 0.5067\n",
      "Loaded model from experiment 11.\n",
      "Epoch 1/10, Train Loss: 4331.1879, Train Accuracy: 0.5090\n",
      "Epoch 2/10, Train Loss: 4331.2084, Train Accuracy: 0.5092\n",
      "Epoch 3/10, Train Loss: 4331.3301, Train Accuracy: 0.5091\n",
      "Epoch 4/10, Train Loss: 4331.4992, Train Accuracy: 0.5089\n",
      "Epoch 5/10, Train Loss: 4331.3723, Train Accuracy: 0.5087\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 186\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExperiment \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Accuracy across all experiments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(accuracies)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 186\u001b[0m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 163\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(data, num_experiments, lookback, num_epochs)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mtotal\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    162\u001b[0m \u001b[38;5;66;03m#  \u001b[39;00m\n\u001b[1;32m--> 163\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;66;03m#  \u001b[39;00m\n\u001b[0;32m    166\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), model_path)\n",
      "Cell \u001b[1;32mIn[1], line 151\u001b[0m, in \u001b[0;36mtrain_and_evaluate.<locals>.train_model\u001b[1;34m(model, train_loader, val_loader, num_epochs, lr)\u001b[0m\n\u001b[0;32m    149\u001b[0m x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    150\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 151\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y)\n\u001b[0;32m    153\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 91\u001b[0m, in \u001b[0;36mEncoderOnlyTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     90\u001b[0m     batch_size, seq_len, feature_dim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m---> 91\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m     positions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(seq_len)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(batch_size, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     93\u001b[0m     x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding(positions)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta as ta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#    \n",
    "def calculate_indicators(data):\n",
    "    data['William_R'] = ta.willr(data['high'], data['low'], data['close'])\n",
    "    data['ATR'] = ta.atr(data['high'], data['low'], data['close'])\n",
    "    data['OBV'] = ta.obv(data['close'], data['volume'])\n",
    "    data['Z_Score'] = (data['close'] - data['close'].rolling(window=20).mean()) / data['close'].rolling(window=20).std()\n",
    "    data['Entropy'] = ta.entropy(data['close'], length=14)\n",
    "    data['SMA_5'] = data['close'].rolling(window=5).mean()\n",
    "    data['SMA_10'] = data['close'].rolling(window=10).mean()\n",
    "    data['SMA_20'] = data['close'].rolling(window=20).mean()\n",
    "    data['SMA_60'] = data['close'].rolling(window=60).mean()\n",
    "    data['SMA_120'] = data['close'].rolling(window=120).mean()\n",
    "    data['RSI'] = ta.rsi(data['close'])\n",
    "    bb = ta.bbands(data['close'])\n",
    "    data['BB_Upper'], data['BB_Middle'], data['BB_Lower'] = bb.iloc[:, 0], bb.iloc[:, 1], bb.iloc[:, 2]\n",
    "    macd = ta.macd(data['close'])\n",
    "    data['MACD'] = macd.iloc[:, 0]\n",
    "    data['Stochastic'] = ta.stoch(data['high'], data['low'], data['close']).iloc[:, 0]\n",
    "    return data.dropna()\n",
    "\n",
    "# Datetime Feature One-Hot Encoding\n",
    "def encode_datetime_features(data):\n",
    "    if 'datetime' not in data.columns:\n",
    "        data['datetime'] = pd.to_datetime(data.index)\n",
    "    data['hour_of_day'] = data['datetime'].dt.hour\n",
    "    data['day_of_week'] = data['datetime'].dt.dayofweek\n",
    "    data['week_of_month'] = (data['datetime'].dt.day - 1) // 7 + 1\n",
    "    data['month'] = data['datetime'].dt.month\n",
    "    hour_one_hot = pd.get_dummies(data['hour_of_day'], prefix='Hour')\n",
    "    day_one_hot = pd.get_dummies(data['day_of_week'], prefix='Day')\n",
    "    week_one_hot = pd.get_dummies(data['week_of_month'], prefix='Week')\n",
    "    month_one_hot = pd.get_dummies(data['month'], prefix='Month')\n",
    "    return pd.concat([data, hour_one_hot, day_one_hot, week_one_hot, month_one_hot], axis=1)\n",
    "\n",
    "#    \n",
    "data = pd.read_csv(\"BTC_upbit_KRW_min5.csv\", index_col=0)\n",
    "data.columns = ['open', 'high', 'low', 'close', 'volume', 'value']\n",
    "data.index = pd.to_datetime(data.index)\n",
    "data = calculate_indicators(data)\n",
    "data = encode_datetime_features(data)\n",
    "\n",
    "# Feature   \n",
    "features = ['open', 'high', 'low', 'close', 'volume', 'value', 'William_R',\n",
    "            'ATR', 'OBV', 'Z_Score', 'Entropy', 'SMA_5', 'SMA_10', 'SMA_20', 'SMA_60', 'SMA_120', 'RSI', \n",
    "            'BB_Upper', 'BB_Middle', 'BB_Lower', 'MACD', 'Stochastic'] + \\\n",
    "            list(data.filter(regex='Hour_').columns) + list(data.filter(regex='Day_').columns) + \\\n",
    "            list(data.filter(regex='Week_').columns) + list(data.filter(regex='Month_').columns)\n",
    "\n",
    "data = data[features].dropna()\n",
    "scaler = MinMaxScaler()\n",
    "data[features] = scaler.fit_transform(data[features])\n",
    "\n",
    "# Dataset \n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, lookback=36, target_idx=-1):\n",
    "        self.data = data\n",
    "        self.lookback = lookback\n",
    "        self.target_idx = target_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.lookback\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.lookback, :]\n",
    "        y = self.data[idx + self.lookback, self.target_idx]\n",
    "        y_target = 1 if y > self.data[idx + self.lookback - 1, self.target_idx] else 0\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y_target, dtype=torch.long)\n",
    "\n",
    "# Transformer  \n",
    "class EncoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=512, num_heads=1, num_layers=6, ffn_dim=2048, num_classes=2):\n",
    "        super(EncoderOnlyTransformer, self).__init__()\n",
    "        self.token_embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(512, embedding_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim, nhead=num_heads, dim_feedforward=ffn_dim\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, feature_dim = x.shape\n",
    "        x = self.token_embedding(x)\n",
    "        positions = torch.arange(seq_len).unsqueeze(0).repeat(batch_size, 1).to(x.device)\n",
    "        x += self.position_embedding(positions)\n",
    "        x = x.permute(1, 0, 2)  # Convert to (seq_len, batch_size, embedding_dim)\n",
    "        x = self.encoder(x)\n",
    "        x = x[-1]  # Take the last token's representation\n",
    "        return self.fc(x)\n",
    "\n",
    "#    \n",
    "def train_and_evaluate(data, num_experiments=21, lookback=36, num_epochs=10):\n",
    "    input_dim = len(features)\n",
    "    step_size = 25000  #   \n",
    "    accuracies = []\n",
    "\n",
    "    for exp in range(num_experiments):\n",
    "        train_start = exp * step_size\n",
    "        train_end = train_start + step_size * 8  #   (8 )\n",
    "        val_end = train_end + step_size  #   (1 )\n",
    "        test_end = val_end + step_size  #   (1 )\n",
    "\n",
    "        if test_end > len(data):\n",
    "            break\n",
    "\n",
    "        train_data = data[train_start:train_end]\n",
    "        val_data = data[train_end:val_end]\n",
    "        test_data = data[val_end:test_end]\n",
    "\n",
    "        train_dataset = TimeSeriesDataset(train_data.values, lookback=lookback, target_idx=features.index('close'))\n",
    "        val_dataset = TimeSeriesDataset(val_data.values, lookback=lookback, target_idx=features.index('close'))\n",
    "        test_dataset = TimeSeriesDataset(test_data.values, lookback=lookback, target_idx=features.index('close'))\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "        #  \n",
    "        model = EncoderOnlyTransformer(input_dim=input_dim).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Fine-tuning:    (exp > 0 )\n",
    "        model_path = f\"model_experiment_{exp}.pth\"\n",
    "        if exp > 0:\n",
    "            try:\n",
    "                model.load_state_dict(torch.load(f\"model_experiment_{exp - 1}.pth\"))  #    \n",
    "                print(f\"Loaded model from experiment {exp - 1}.\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Model file for experiment {exp - 1} not found. Starting fresh training.\")\n",
    "\n",
    "        def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-4):\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            model.to(device)\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                model.train()\n",
    "                total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "                for x, y in train_loader:\n",
    "                    x, y = x.to(device), y.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(x)\n",
    "                    loss = criterion(outputs, y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    total_loss += loss.item()\n",
    "                    correct += (outputs.argmax(1) == y).sum().item()\n",
    "                    total += y.size(0)\n",
    "\n",
    "                print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {total_loss:.4f}, Train Accuracy: {correct / total:.4f}\")\n",
    "\n",
    "        #  \n",
    "        train_model(model, train_loader, val_loader, num_epochs)\n",
    "\n",
    "        #  \n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Saved model for experiment {exp}.\")\n",
    "\n",
    "        # \n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                x, y = x.to('cuda' if torch.cuda.is_available() else 'cpu'), y.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                outputs = model(x)\n",
    "                correct += (outputs.argmax(1) == y).sum().item()\n",
    "                total += y.size(0)\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        accuracies.append(accuracy)\n",
    "        print(f\"Experiment {exp + 1}, Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    print(f\"Average Accuracy across all experiments: {np.mean(accuracies):.4f}\")\n",
    "\n",
    "\n",
    "train_and_evaluate(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Model(Priceformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 4347.4453, Train Accuracy: 0.5182\n",
      "Epoch 2/10, Train Loss: 4324.7100, Train Accuracy: 0.5263\n",
      "Epoch 3/10, Train Loss: 4323.2969, Train Accuracy: 0.5288\n",
      "Epoch 4/10, Train Loss: 4323.0919, Train Accuracy: 0.5288\n",
      "Epoch 5/10, Train Loss: 4322.1220, Train Accuracy: 0.5291\n",
      "Epoch 6/10, Train Loss: 4322.2598, Train Accuracy: 0.5293\n",
      "Epoch 7/10, Train Loss: 4321.9022, Train Accuracy: 0.5291\n",
      "Epoch 8/10, Train Loss: 4321.8538, Train Accuracy: 0.5294\n",
      "Epoch 9/10, Train Loss: 4321.5105, Train Accuracy: 0.5293\n",
      "Epoch 10/10, Train Loss: 4321.7050, Train Accuracy: 0.5294\n",
      "Saved model for experiment 0.\n",
      "Experiment 1, Test Accuracy: 0.5290\n",
      "Loaded model from experiment 0.\n",
      "Epoch 1/10, Train Loss: 4318.5513, Train Accuracy: 0.5335\n",
      "Epoch 2/10, Train Loss: 4318.3723, Train Accuracy: 0.5335\n",
      "Epoch 3/10, Train Loss: 4318.2940, Train Accuracy: 0.5335\n",
      "Epoch 4/10, Train Loss: 4318.0873, Train Accuracy: 0.5335\n",
      "Epoch 5/10, Train Loss: 4318.1402, Train Accuracy: 0.5335\n",
      "Epoch 6/10, Train Loss: 4318.1876, Train Accuracy: 0.5334\n",
      "Epoch 7/10, Train Loss: 4318.1814, Train Accuracy: 0.5333\n",
      "Epoch 8/10, Train Loss: 4318.1453, Train Accuracy: 0.5334\n",
      "Epoch 9/10, Train Loss: 4317.9423, Train Accuracy: 0.5334\n",
      "Epoch 10/10, Train Loss: 4317.9697, Train Accuracy: 0.5335\n",
      "Saved model for experiment 1.\n",
      "Experiment 2, Test Accuracy: 0.5201\n",
      "Loaded model from experiment 1.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 306\u001b[0m\n\u001b[0;32m    303\u001b[0m                 total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    304\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExperiment \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mtotal\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 306\u001b[0m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 292\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(data, num_experiments, lookback, num_epochs)\u001b[0m\n\u001b[0;32m    289\u001b[0m             total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mtotal\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 292\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    293\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), model_path)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved model for experiment \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 280\u001b[0m, in \u001b[0;36mtrain_and_evaluate.<locals>.train_model\u001b[1;34m(model, train_loader, val_loader, num_epochs, lr)\u001b[0m\n\u001b[0;32m    278\u001b[0m x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    279\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 280\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y)\n\u001b[0;32m    282\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 218\u001b[0m, in \u001b[0;36mEncoderOnlyTransformerCustom.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    215\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding(positions)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 218\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;66;03m#     \u001b[39;00m\n\u001b[0;32m    221\u001b[0m x \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# (batch_size, embedding_dim)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 193\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    191\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(x)\n\u001b[0;32m    192\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m attn_out)\n\u001b[1;32m--> 193\u001b[0m ffn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m ffn_out)\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 179\u001b[0m, in \u001b[0;36mFeedForward.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta as ta\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from math import sqrt\n",
    "\n",
    "####################################\n",
    "# 1.     ( )\n",
    "####################################\n",
    "def calculate_indicators(data):\n",
    "    data['William_R'] = ta.willr(data['high'], data['low'], data['close'])\n",
    "    data['ATR'] = ta.atr(data['high'], data['low'], data['close'])\n",
    "    data['OBV'] = ta.obv(data['close'], data['volume'])\n",
    "    data['Z_Score'] = (data['close'] - data['close'].rolling(window=20).mean()) / data['close'].rolling(window=20).std()\n",
    "    data['Entropy'] = ta.entropy(data['close'], length=14)\n",
    "    data['SMA_5'] = data['close'].rolling(window=5).mean()\n",
    "    data['SMA_10'] = data['close'].rolling(window=10).mean()\n",
    "    data['SMA_20'] = data['close'].rolling(window=20).mean()\n",
    "    data['SMA_60'] = data['close'].rolling(window=60).mean()\n",
    "    data['SMA_120'] = data['close'].rolling(window=120).mean()\n",
    "    data['SMA_250'] = data['close'].rolling(window=250).mean()\n",
    "    data['RSI'] = ta.rsi(data['close'])\n",
    "    bb = ta.bbands(data['close'])\n",
    "    data['BB_Upper'], data['BB_Middle'], data['BB_Lower'] = bb.iloc[:, 0], bb.iloc[:, 1], bb.iloc[:, 2]\n",
    "    macd = ta.macd(data['close'])\n",
    "    data['MACD'] = macd.iloc[:, 0]\n",
    "    data['Stochastic'] = ta.stoch(data['high'], data['low'], data['close']).iloc[:, 0]\n",
    "    return data.dropna()\n",
    "\n",
    "####################################\n",
    "# 2. Datetime   Positional (Cyclical) \n",
    "####################################\n",
    "def encode_datetime_features_positional(data):\n",
    "    if 'datetime' not in data.columns:\n",
    "        data['datetime'] = pd.to_datetime(data.index)\n",
    "    data['hour_sin'] = np.sin(2 * np.pi * data['datetime'].dt.hour / 24)\n",
    "    data['hour_cos'] = np.cos(2 * np.pi * data['datetime'].dt.hour / 24)\n",
    "    data['day_sin'] = np.sin(2 * np.pi * data['datetime'].dt.dayofweek / 7)\n",
    "    data['day_cos'] = np.cos(2 * np.pi * data['datetime'].dt.dayofweek / 7)\n",
    "    data['week_of_month'] = (data['datetime'].dt.day - 1) // 7 + 1\n",
    "    data['week_sin'] = np.sin(2 * np.pi * data['week_of_month'] / 5)\n",
    "    data['week_cos'] = np.cos(2 * np.pi * data['week_of_month'] / 5)\n",
    "    data['month_sin'] = np.sin(2 * np.pi * data['datetime'].dt.month / 12)\n",
    "    data['month_cos'] = np.cos(2 * np.pi * data['datetime'].dt.month / 12)\n",
    "    return data\n",
    "\n",
    "####################################\n",
    "# 3.  50  rolling minmax scaling ( 0 )\n",
    "####################################\n",
    "def rolling_minmax_scale(series, window=50):\n",
    "    roll_min = series.rolling(window=window, min_periods=window).min()\n",
    "    roll_max = series.rolling(window=window, min_periods=window).max()\n",
    "    scaled = (series - roll_min) / ((roll_max - roll_min) + 1e-8)\n",
    "    return scaled\n",
    "\n",
    "####################################\n",
    "# 4. Binning  One-Hot  ( )\n",
    "####################################\n",
    "def bin_and_encode(data, features, bins=10, drop_original=True):\n",
    "    for feature in features:\n",
    "        data[f'{feature}_Bin'] = pd.cut(data[feature], bins=bins, labels=False)\n",
    "        one_hot = pd.get_dummies(data[f'{feature}_Bin'], prefix=f'{feature}_Bin')\n",
    "        expected_columns = [f'{feature}_Bin_{i}' for i in range(bins)]\n",
    "        one_hot = one_hot.reindex(columns=expected_columns, fill_value=0)\n",
    "        data = pd.concat([data, one_hot], axis=1)\n",
    "        if drop_original:\n",
    "            data.drop(columns=[f'{feature}_Bin'], inplace=True)\n",
    "    data = data.astype(np.float32)\n",
    "    return data\n",
    "\n",
    "####################################\n",
    "# 5.    \n",
    "####################################\n",
    "data = pd.read_csv(\"BTC_upbit_KRW_min5.csv\", index_col=0)\n",
    "data.columns = ['open', 'high', 'low', 'close', 'volume', 'value']\n",
    "data.index = pd.to_datetime(data.index)\n",
    "data = calculate_indicators(data)\n",
    "data = encode_datetime_features_positional(data)\n",
    "\n",
    "features_to_bin = ['open', 'high', 'low', 'volume', 'value', 'William_R',\n",
    "                   'ATR', 'OBV', 'Z_Score', 'Entropy', 'SMA_5', 'SMA_10', \n",
    "                   'SMA_20', 'SMA_60', 'SMA_120', 'SMA_250', 'RSI', 'BB_Upper', 'BB_Middle', \n",
    "                   'BB_Lower', 'MACD', 'Stochastic']\n",
    "datetime_features = ['hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'week_sin', 'week_cos', 'month_sin', 'month_cos']\n",
    "\n",
    "# close_target  close \n",
    "data['close_target'] = data['close']\n",
    "\n",
    "selected_features = features_to_bin + ['close_target'] + datetime_features\n",
    "data = data[selected_features].dropna()\n",
    "\n",
    "for feature in features_to_bin:\n",
    "    data[feature] = rolling_minmax_scale(data[feature], window=512)\n",
    "data = data.dropna()\n",
    "\n",
    "data = bin_and_encode(data, features_to_bin, bins=10, drop_original=True)\n",
    "data['close_for_binning'] = data['close_target']\n",
    "data = bin_and_encode(data, ['close_for_binning'], bins=10, drop_original=False)\n",
    "data.drop(columns=['close_for_binning'], inplace=True)\n",
    "\n",
    "final_columns = []\n",
    "for feature in features_to_bin:\n",
    "    final_columns.extend([f'{feature}_Bin_{i}' for i in range(10)])\n",
    "final_columns.append('close_target')\n",
    "final_columns.extend([f'close_for_binning_Bin_{i}' for i in range(10)])\n",
    "final_columns.extend(datetime_features)\n",
    "data = data[final_columns]\n",
    "\n",
    "####################################\n",
    "# 6. Dataset \n",
    "####################################\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, lookback=12, target_col='close_target'):\n",
    "        self.data = data.values\n",
    "        self.lookback = lookback\n",
    "        self.target_idx = list(data.columns).index(target_col)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.lookback\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.lookback, :]\n",
    "        y = self.data[idx + self.lookback, self.target_idx]\n",
    "        y_prev = self.data[idx + self.lookback - 1, self.target_idx]\n",
    "        y_target = 1 if y > y_prev else 0\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y_target, dtype=torch.long)\n",
    "\n",
    "####################################\n",
    "# 7. Transformer Encoder  \n",
    "####################################\n",
    "# 7-1. Multi-Head Self-Attention\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == embed_dim, \"embed_dim num_heads  .\"\n",
    "        \n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key   = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out   = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, embed_dim)\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        \n",
    "        Q = self.query(x)  # (batch_size, seq_len, embed_dim)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        \n",
    "        #   : (batch_size, num_heads, seq_len, head_dim)\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Scaled Dot-Product Attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / sqrt(self.head_dim)  # (batch_size, num_heads, seq_len, seq_len)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, V)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "        \n",
    "        #    projection\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "\n",
    "# 7-2. Feed-Forward Network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ffn_dim):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, ffn_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(ffn_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "# 7-3. Transformer Encoder Layer (Self-Attention + FeedForward + Residual + LayerNorm)\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = FeedForward(embed_dim, ffn_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attn_out = self.self_attn(x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_out)\n",
    "        return x\n",
    "\n",
    "# 7-4. Encoder-Only Transformer  \n",
    "class EncoderOnlyTransformerCustom(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=512, num_heads=8, num_layers=4, ffn_dim=1024, num_classes=2, max_seq_len=512):\n",
    "        super(EncoderOnlyTransformerCustom, self).__init__()\n",
    "        self.token_embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embedding_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(embedding_dim, num_heads, ffn_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_dim)\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x = self.token_embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        x = x + self.position_embedding(positions)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        #     \n",
    "        x = x[:, -1, :]  # (batch_size, embedding_dim)\n",
    "        return self.fc(x)\n",
    "\n",
    "####################################\n",
    "# 8.     (Gradient Clipping )\n",
    "####################################\n",
    "def train_and_evaluate(data, num_experiments=21, lookback=50, num_epochs=10):\n",
    "    #   :\n",
    "    #   binned: len(features_to_bin)*50\n",
    "    # close_target: 1\n",
    "    # close one-hot: 50\n",
    "    # datetime positional: 8\n",
    "    input_dim = data.shape[1]\n",
    "    step_size = 25000  #  \n",
    "\n",
    "    for exp in range(num_experiments):\n",
    "        train_start = exp * step_size\n",
    "        train_end = train_start + step_size * 8   #  \n",
    "        val_end = train_end + step_size            #  \n",
    "        test_end = val_end + step_size             #  \n",
    "\n",
    "        if test_end > len(data):\n",
    "            break\n",
    "\n",
    "        train_data = data.iloc[train_start:train_end]\n",
    "        val_data = data.iloc[train_end:val_end]\n",
    "        test_data = data.iloc[val_end:test_end]\n",
    "\n",
    "        train_dataset = TimeSeriesDataset(train_data, lookback=lookback, target_col='close_target')\n",
    "        val_dataset = TimeSeriesDataset(val_data, lookback=lookback, target_col='close_target')\n",
    "        test_dataset = TimeSeriesDataset(test_data, lookback=lookback, target_col='close_target')\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        #   nn.TransformerEncoder  ,   EncoderOnlyTransformerCustom \n",
    "        model = EncoderOnlyTransformerCustom(input_dim=input_dim, embedding_dim=512, num_heads=8, \n",
    "                                               num_layers=4, ffn_dim=1024, num_classes=2).to(device)\n",
    "\n",
    "        #     (fine-tuning)\n",
    "        model_path = f\"model_experiment_{exp}.pth\"\n",
    "        if exp > 0:\n",
    "            try:\n",
    "                model.load_state_dict(torch.load(f\"model_experiment_{exp - 1}.pth\"))\n",
    "                print(f\"Loaded model from experiment {exp - 1}.\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Model file for experiment {exp - 1} not found. Starting fresh training.\")\n",
    "\n",
    "        def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-4):\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "            for epoch in range(num_epochs):\n",
    "                model.train()\n",
    "                total_loss, correct, total = 0, 0, 0\n",
    "                for x, y in train_loader:\n",
    "                    x, y = x.to(device), y.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(x)\n",
    "                    loss = criterion(outputs, y)\n",
    "                    loss.backward()\n",
    "                    # Gradient clipping \n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "\n",
    "                    total_loss += loss.item()\n",
    "                    correct += (outputs.argmax(1) == y).sum().item()\n",
    "                    total += y.size(0)\n",
    "                print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {total_loss:.4f}, Train Accuracy: {correct / total:.4f}\")\n",
    "\n",
    "        train_model(model, train_loader, val_loader, num_epochs)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Saved model for experiment {exp}.\")\n",
    "\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                outputs = model(x)\n",
    "                correct += (outputs.argmax(1) == y).sum().item()\n",
    "                total += y.size(0)\n",
    "        print(f\"Experiment {exp + 1}, Test Accuracy: {correct / total:.4f}\")\n",
    "\n",
    "train_and_evaluate(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base(fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 0: Training with lr=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.6932, Train Acc: 0.5405 | Val Loss: 0.6874, Val Acc: 0.5580\n",
      "Epoch 2/10 | Train Loss: 0.6893, Train Acc: 0.5468 | Val Loss: 0.6864, Val Acc: 0.5580\n",
      "Epoch 3/10 | Train Loss: 0.6890, Train Acc: 0.5475 | Val Loss: 0.6876, Val Acc: 0.5580\n",
      "Epoch 4/10 | Train Loss: 0.6890, Train Acc: 0.5473 | Val Loss: 0.6875, Val Acc: 0.5580\n",
      "Epoch 5/10 | Train Loss: 0.6889, Train Acc: 0.5472 | Val Loss: 0.6869, Val Acc: 0.5580\n",
      "Epoch 6/10 | Train Loss: 0.6888, Train Acc: 0.5475 | Val Loss: 0.6868, Val Acc: 0.5580\n",
      "Epoch 7/10 | Train Loss: 0.6888, Train Acc: 0.5475 | Val Loss: 0.6868, Val Acc: 0.5580\n",
      "Epoch 8/10 | Train Loss: 0.6888, Train Acc: 0.5474 | Val Loss: 0.6868, Val Acc: 0.5580\n",
      "Epoch 9/10 | Train Loss: 0.6887, Train Acc: 0.5475 | Val Loss: 0.6874, Val Acc: 0.5580\n",
      "Epoch 10/10 | Train Loss: 0.6887, Train Acc: 0.5475 | Val Loss: 0.6864, Val Acc: 0.5580\n",
      "Saved model for experiment 0.\n",
      "Experiment 0: Final Validation Accuracy: 0.5580\n",
      "Experiment 0: Test Accuracy: 0.5614\n",
      "Loaded model from experiment 0.\n",
      "Experiment 1: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6885, Train Acc: 0.5481 | Val Loss: 0.6858, Val Acc: 0.5614\n",
      "Epoch 2/10 | Train Loss: 0.6886, Train Acc: 0.5481 | Val Loss: 0.6858, Val Acc: 0.5614\n",
      "Epoch 3/10 | Train Loss: 0.6886, Train Acc: 0.5481 | Val Loss: 0.6861, Val Acc: 0.5614\n",
      "Epoch 4/10 | Train Loss: 0.6886, Train Acc: 0.5481 | Val Loss: 0.6861, Val Acc: 0.5614\n",
      "Epoch 5/10 | Train Loss: 0.6885, Train Acc: 0.5481 | Val Loss: 0.6859, Val Acc: 0.5614\n",
      "Epoch 6/10 | Train Loss: 0.6885, Train Acc: 0.5481 | Val Loss: 0.6858, Val Acc: 0.5614\n",
      "Epoch 7/10 | Train Loss: 0.6885, Train Acc: 0.5481 | Val Loss: 0.6863, Val Acc: 0.5614\n",
      "Epoch 8/10 | Train Loss: 0.6885, Train Acc: 0.5481 | Val Loss: 0.6859, Val Acc: 0.5614\n",
      "Epoch 9/10 | Train Loss: 0.6885, Train Acc: 0.5481 | Val Loss: 0.6859, Val Acc: 0.5614\n",
      "Epoch 10/10 | Train Loss: 0.6885, Train Acc: 0.5481 | Val Loss: 0.6859, Val Acc: 0.5614\n",
      "Saved model for experiment 1.\n",
      "Experiment 1: Final Validation Accuracy: 0.5614\n",
      "Experiment 1: Test Accuracy: 0.5319\n",
      "Loaded model from experiment 1.\n",
      "Experiment 2: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6877, Train Acc: 0.5521 | Val Loss: 0.6915, Val Acc: 0.5319\n",
      "Epoch 2/10 | Train Loss: 0.6877, Train Acc: 0.5521 | Val Loss: 0.6923, Val Acc: 0.5319\n",
      "Epoch 3/10 | Train Loss: 0.6877, Train Acc: 0.5521 | Val Loss: 0.6919, Val Acc: 0.5319\n",
      "Epoch 4/10 | Train Loss: 0.6877, Train Acc: 0.5521 | Val Loss: 0.6925, Val Acc: 0.5319\n",
      "Epoch 5/10 | Train Loss: 0.6877, Train Acc: 0.5521 | Val Loss: 0.6919, Val Acc: 0.5319\n",
      "Epoch 6/10 | Train Loss: 0.6877, Train Acc: 0.5521 | Val Loss: 0.6921, Val Acc: 0.5319\n",
      "Epoch 7/10 | Train Loss: 0.6877, Train Acc: 0.5521 | Val Loss: 0.6919, Val Acc: 0.5319\n",
      "Epoch 8/10 | Train Loss: 0.6877, Train Acc: 0.5521 | Val Loss: 0.6920, Val Acc: 0.5319\n",
      "Epoch 9/10 | Train Loss: 0.6877, Train Acc: 0.5521 | Val Loss: 0.6919, Val Acc: 0.5319\n",
      "Epoch 10/10 | Train Loss: 0.6877, Train Acc: 0.5521 | Val Loss: 0.6920, Val Acc: 0.5319\n",
      "Saved model for experiment 2.\n",
      "Experiment 2: Final Validation Accuracy: 0.5319\n",
      "Experiment 2: Test Accuracy: 0.5466\n",
      "Loaded model from experiment 2.\n",
      "Experiment 3: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6876, Train Acc: 0.5529 | Val Loss: 0.6890, Val Acc: 0.5466\n",
      "Epoch 2/10 | Train Loss: 0.6876, Train Acc: 0.5529 | Val Loss: 0.6888, Val Acc: 0.5466\n",
      "Epoch 3/10 | Train Loss: 0.6876, Train Acc: 0.5529 | Val Loss: 0.6892, Val Acc: 0.5466\n",
      "Epoch 4/10 | Train Loss: 0.6876, Train Acc: 0.5529 | Val Loss: 0.6891, Val Acc: 0.5466\n",
      "Epoch 5/10 | Train Loss: 0.6876, Train Acc: 0.5529 | Val Loss: 0.6888, Val Acc: 0.5466\n",
      "Epoch 6/10 | Train Loss: 0.6876, Train Acc: 0.5529 | Val Loss: 0.6890, Val Acc: 0.5466\n",
      "Epoch 7/10 | Train Loss: 0.6876, Train Acc: 0.5529 | Val Loss: 0.6890, Val Acc: 0.5466\n",
      "Epoch 8/10 | Train Loss: 0.6876, Train Acc: 0.5529 | Val Loss: 0.6889, Val Acc: 0.5466\n",
      "Epoch 9/10 | Train Loss: 0.6875, Train Acc: 0.5529 | Val Loss: 0.6889, Val Acc: 0.5466\n",
      "Epoch 10/10 | Train Loss: 0.6875, Train Acc: 0.5529 | Val Loss: 0.6890, Val Acc: 0.5466\n",
      "Saved model for experiment 3.\n",
      "Experiment 3: Final Validation Accuracy: 0.5466\n",
      "Experiment 3: Test Accuracy: 0.5347\n",
      "Loaded model from experiment 3.\n",
      "Experiment 4: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6877, Train Acc: 0.5525 | Val Loss: 0.6912, Val Acc: 0.5347\n",
      "Epoch 2/10 | Train Loss: 0.6877, Train Acc: 0.5525 | Val Loss: 0.6918, Val Acc: 0.5347\n",
      "Epoch 3/10 | Train Loss: 0.6877, Train Acc: 0.5525 | Val Loss: 0.6917, Val Acc: 0.5347\n",
      "Epoch 4/10 | Train Loss: 0.6877, Train Acc: 0.5525 | Val Loss: 0.6912, Val Acc: 0.5347\n",
      "Epoch 5/10 | Train Loss: 0.6876, Train Acc: 0.5525 | Val Loss: 0.6915, Val Acc: 0.5347\n",
      "Epoch 6/10 | Train Loss: 0.6877, Train Acc: 0.5525 | Val Loss: 0.6914, Val Acc: 0.5347\n",
      "Epoch 7/10 | Train Loss: 0.6876, Train Acc: 0.5525 | Val Loss: 0.6914, Val Acc: 0.5347\n",
      "Epoch 8/10 | Train Loss: 0.6876, Train Acc: 0.5525 | Val Loss: 0.6915, Val Acc: 0.5347\n",
      "Epoch 9/10 | Train Loss: 0.6876, Train Acc: 0.5525 | Val Loss: 0.6916, Val Acc: 0.5347\n",
      "Epoch 10/10 | Train Loss: 0.6876, Train Acc: 0.5525 | Val Loss: 0.6915, Val Acc: 0.5347\n",
      "Saved model for experiment 4.\n",
      "Experiment 4: Final Validation Accuracy: 0.5347\n",
      "Experiment 4: Test Accuracy: 0.5175\n",
      "Loaded model from experiment 4.\n",
      "Experiment 5: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6884, Train Acc: 0.5487 | Val Loss: 0.6957, Val Acc: 0.5175\n",
      "Epoch 2/10 | Train Loss: 0.6884, Train Acc: 0.5487 | Val Loss: 0.6950, Val Acc: 0.5175\n",
      "Epoch 3/10 | Train Loss: 0.6884, Train Acc: 0.5487 | Val Loss: 0.6939, Val Acc: 0.5175\n",
      "Epoch 4/10 | Train Loss: 0.6884, Train Acc: 0.5487 | Val Loss: 0.6947, Val Acc: 0.5175\n",
      "Epoch 5/10 | Train Loss: 0.6884, Train Acc: 0.5487 | Val Loss: 0.6949, Val Acc: 0.5175\n",
      "Epoch 6/10 | Train Loss: 0.6884, Train Acc: 0.5487 | Val Loss: 0.6947, Val Acc: 0.5175\n",
      "Epoch 7/10 | Train Loss: 0.6884, Train Acc: 0.5487 | Val Loss: 0.6942, Val Acc: 0.5175\n",
      "Epoch 8/10 | Train Loss: 0.6884, Train Acc: 0.5487 | Val Loss: 0.6951, Val Acc: 0.5175\n",
      "Epoch 9/10 | Train Loss: 0.6884, Train Acc: 0.5487 | Val Loss: 0.6945, Val Acc: 0.5175\n",
      "Epoch 10/10 | Train Loss: 0.6884, Train Acc: 0.5487 | Val Loss: 0.6948, Val Acc: 0.5175\n",
      "Saved model for experiment 5.\n",
      "Experiment 5: Final Validation Accuracy: 0.5175\n",
      "Experiment 5: Test Accuracy: 0.5292\n",
      "Loaded model from experiment 5.\n",
      "Experiment 6: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6895, Train Acc: 0.5429 | Val Loss: 0.6917, Val Acc: 0.5292\n",
      "Epoch 2/10 | Train Loss: 0.6895, Train Acc: 0.5429 | Val Loss: 0.6917, Val Acc: 0.5292\n",
      "Epoch 3/10 | Train Loss: 0.6895, Train Acc: 0.5429 | Val Loss: 0.6915, Val Acc: 0.5292\n",
      "Epoch 4/10 | Train Loss: 0.6895, Train Acc: 0.5429 | Val Loss: 0.6920, Val Acc: 0.5292\n",
      "Epoch 5/10 | Train Loss: 0.6895, Train Acc: 0.5429 | Val Loss: 0.6916, Val Acc: 0.5292\n",
      "Epoch 6/10 | Train Loss: 0.6895, Train Acc: 0.5429 | Val Loss: 0.6916, Val Acc: 0.5292\n",
      "Epoch 7/10 | Train Loss: 0.6895, Train Acc: 0.5429 | Val Loss: 0.6919, Val Acc: 0.5292\n",
      "Epoch 8/10 | Train Loss: 0.6895, Train Acc: 0.5429 | Val Loss: 0.6924, Val Acc: 0.5292\n",
      "Epoch 9/10 | Train Loss: 0.6895, Train Acc: 0.5429 | Val Loss: 0.6916, Val Acc: 0.5292\n",
      "Epoch 10/10 | Train Loss: 0.6895, Train Acc: 0.5429 | Val Loss: 0.6917, Val Acc: 0.5292\n",
      "Saved model for experiment 6.\n",
      "Experiment 6: Final Validation Accuracy: 0.5292\n",
      "Experiment 6: Test Accuracy: 0.5331\n",
      "Loaded model from experiment 6.\n",
      "Experiment 7: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6899, Train Acc: 0.5402 | Val Loss: 0.6910, Val Acc: 0.5331\n",
      "Epoch 2/10 | Train Loss: 0.6899, Train Acc: 0.5402 | Val Loss: 0.6911, Val Acc: 0.5331\n",
      "Epoch 3/10 | Train Loss: 0.6899, Train Acc: 0.5402 | Val Loss: 0.6913, Val Acc: 0.5331\n",
      "Epoch 4/10 | Train Loss: 0.6899, Train Acc: 0.5402 | Val Loss: 0.6910, Val Acc: 0.5331\n",
      "Epoch 5/10 | Train Loss: 0.6899, Train Acc: 0.5402 | Val Loss: 0.6910, Val Acc: 0.5331\n",
      "Epoch 6/10 | Train Loss: 0.6899, Train Acc: 0.5402 | Val Loss: 0.6910, Val Acc: 0.5331\n",
      "Epoch 7/10 | Train Loss: 0.6899, Train Acc: 0.5402 | Val Loss: 0.6911, Val Acc: 0.5331\n",
      "Epoch 8/10 | Train Loss: 0.6899, Train Acc: 0.5402 | Val Loss: 0.6910, Val Acc: 0.5331\n",
      "Epoch 9/10 | Train Loss: 0.6899, Train Acc: 0.5402 | Val Loss: 0.6911, Val Acc: 0.5331\n",
      "Epoch 10/10 | Train Loss: 0.6899, Train Acc: 0.5402 | Val Loss: 0.6910, Val Acc: 0.5331\n",
      "Saved model for experiment 7.\n",
      "Experiment 7: Final Validation Accuracy: 0.5331\n",
      "Experiment 7: Test Accuracy: 0.5321\n",
      "Loaded model from experiment 7.\n",
      "Experiment 8: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6901, Train Acc: 0.5390 | Val Loss: 0.6913, Val Acc: 0.5321\n",
      "Epoch 2/10 | Train Loss: 0.6901, Train Acc: 0.5390 | Val Loss: 0.6911, Val Acc: 0.5321\n",
      "Epoch 3/10 | Train Loss: 0.6901, Train Acc: 0.5390 | Val Loss: 0.6912, Val Acc: 0.5321\n",
      "Epoch 4/10 | Train Loss: 0.6901, Train Acc: 0.5390 | Val Loss: 0.6912, Val Acc: 0.5321\n",
      "Epoch 5/10 | Train Loss: 0.6901, Train Acc: 0.5390 | Val Loss: 0.6912, Val Acc: 0.5321\n",
      "Epoch 6/10 | Train Loss: 0.6901, Train Acc: 0.5390 | Val Loss: 0.6912, Val Acc: 0.5321\n",
      "Epoch 7/10 | Train Loss: 0.6901, Train Acc: 0.5390 | Val Loss: 0.6912, Val Acc: 0.5321\n",
      "Epoch 8/10 | Train Loss: 0.6901, Train Acc: 0.5390 | Val Loss: 0.6912, Val Acc: 0.5321\n",
      "Epoch 9/10 | Train Loss: 0.6901, Train Acc: 0.5390 | Val Loss: 0.6912, Val Acc: 0.5321\n",
      "Epoch 10/10 | Train Loss: 0.6901, Train Acc: 0.5390 | Val Loss: 0.6911, Val Acc: 0.5321\n",
      "Saved model for experiment 8.\n",
      "Experiment 8: Final Validation Accuracy: 0.5321\n",
      "Experiment 8: Test Accuracy: 0.5352\n",
      "Loaded model from experiment 8.\n",
      "Experiment 9: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6906, Train Acc: 0.5358 | Val Loss: 0.6907, Val Acc: 0.5352\n",
      "Epoch 2/10 | Train Loss: 0.6906, Train Acc: 0.5358 | Val Loss: 0.6906, Val Acc: 0.5352\n",
      "Epoch 3/10 | Train Loss: 0.6906, Train Acc: 0.5358 | Val Loss: 0.6907, Val Acc: 0.5352\n",
      "Epoch 4/10 | Train Loss: 0.6906, Train Acc: 0.5358 | Val Loss: 0.6906, Val Acc: 0.5352\n",
      "Epoch 5/10 | Train Loss: 0.6906, Train Acc: 0.5358 | Val Loss: 0.6906, Val Acc: 0.5352\n",
      "Epoch 6/10 | Train Loss: 0.6906, Train Acc: 0.5358 | Val Loss: 0.6906, Val Acc: 0.5352\n",
      "Epoch 7/10 | Train Loss: 0.6906, Train Acc: 0.5358 | Val Loss: 0.6907, Val Acc: 0.5352\n",
      "Epoch 8/10 | Train Loss: 0.6906, Train Acc: 0.5358 | Val Loss: 0.6906, Val Acc: 0.5352\n",
      "Epoch 9/10 | Train Loss: 0.6906, Train Acc: 0.5358 | Val Loss: 0.6906, Val Acc: 0.5352\n",
      "Epoch 10/10 | Train Loss: 0.6906, Train Acc: 0.5358 | Val Loss: 0.6906, Val Acc: 0.5352\n",
      "Saved model for experiment 9.\n",
      "Experiment 9: Final Validation Accuracy: 0.5352\n",
      "Experiment 9: Test Accuracy: 0.5356\n",
      "Loaded model from experiment 9.\n",
      "Experiment 10: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6910, Train Acc: 0.5325 | Val Loss: 0.6907, Val Acc: 0.5356\n",
      "Epoch 2/10 | Train Loss: 0.6910, Train Acc: 0.5325 | Val Loss: 0.6906, Val Acc: 0.5356\n",
      "Epoch 3/10 | Train Loss: 0.6911, Train Acc: 0.5325 | Val Loss: 0.6907, Val Acc: 0.5356\n",
      "Epoch 4/10 | Train Loss: 0.6911, Train Acc: 0.5325 | Val Loss: 0.6907, Val Acc: 0.5356\n",
      "Epoch 5/10 | Train Loss: 0.6910, Train Acc: 0.5325 | Val Loss: 0.6906, Val Acc: 0.5356\n",
      "Epoch 6/10 | Train Loss: 0.6910, Train Acc: 0.5325 | Val Loss: 0.6906, Val Acc: 0.5356\n",
      "Epoch 7/10 | Train Loss: 0.6910, Train Acc: 0.5325 | Val Loss: 0.6906, Val Acc: 0.5356\n",
      "Epoch 8/10 | Train Loss: 0.6910, Train Acc: 0.5325 | Val Loss: 0.6906, Val Acc: 0.5356\n",
      "Epoch 9/10 | Train Loss: 0.6910, Train Acc: 0.5325 | Val Loss: 0.6906, Val Acc: 0.5356\n",
      "Epoch 10/10 | Train Loss: 0.6910, Train Acc: 0.5325 | Val Loss: 0.6906, Val Acc: 0.5356\n",
      "Saved model for experiment 10.\n",
      "Experiment 10: Final Validation Accuracy: 0.5356\n",
      "Experiment 10: Test Accuracy: 0.5533\n",
      "Loaded model from experiment 10.\n",
      "Experiment 11: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6910, Train Acc: 0.5330 | Val Loss: 0.6887, Val Acc: 0.5533\n",
      "Epoch 2/10 | Train Loss: 0.6910, Train Acc: 0.5330 | Val Loss: 0.6885, Val Acc: 0.5533\n",
      "Epoch 3/10 | Train Loss: 0.6910, Train Acc: 0.5330 | Val Loss: 0.6882, Val Acc: 0.5533\n",
      "Epoch 4/10 | Train Loss: 0.6910, Train Acc: 0.5330 | Val Loss: 0.6886, Val Acc: 0.5533\n",
      "Epoch 5/10 | Train Loss: 0.6910, Train Acc: 0.5330 | Val Loss: 0.6882, Val Acc: 0.5533\n",
      "Epoch 6/10 | Train Loss: 0.6910, Train Acc: 0.5330 | Val Loss: 0.6883, Val Acc: 0.5533\n",
      "Epoch 7/10 | Train Loss: 0.6910, Train Acc: 0.5330 | Val Loss: 0.6885, Val Acc: 0.5533\n",
      "Epoch 8/10 | Train Loss: 0.6910, Train Acc: 0.5330 | Val Loss: 0.6883, Val Acc: 0.5533\n",
      "Epoch 9/10 | Train Loss: 0.6910, Train Acc: 0.5330 | Val Loss: 0.6885, Val Acc: 0.5533\n",
      "Epoch 10/10 | Train Loss: 0.6910, Train Acc: 0.5330 | Val Loss: 0.6881, Val Acc: 0.5533\n",
      "Saved model for experiment 11.\n",
      "Experiment 11: Final Validation Accuracy: 0.5533\n",
      "Experiment 11: Test Accuracy: 0.5505\n",
      "Loaded model from experiment 11.\n",
      "Experiment 12: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6909, Train Acc: 0.5338 | Val Loss: 0.6891, Val Acc: 0.5505\n",
      "Epoch 2/10 | Train Loss: 0.6909, Train Acc: 0.5338 | Val Loss: 0.6889, Val Acc: 0.5505\n",
      "Epoch 3/10 | Train Loss: 0.6909, Train Acc: 0.5338 | Val Loss: 0.6884, Val Acc: 0.5505\n",
      "Epoch 4/10 | Train Loss: 0.6909, Train Acc: 0.5338 | Val Loss: 0.6884, Val Acc: 0.5505\n",
      "Epoch 5/10 | Train Loss: 0.6909, Train Acc: 0.5338 | Val Loss: 0.6885, Val Acc: 0.5505\n",
      "Epoch 6/10 | Train Loss: 0.6909, Train Acc: 0.5338 | Val Loss: 0.6884, Val Acc: 0.5505\n",
      "Epoch 7/10 | Train Loss: 0.6909, Train Acc: 0.5338 | Val Loss: 0.6886, Val Acc: 0.5505\n",
      "Epoch 8/10 | Train Loss: 0.6909, Train Acc: 0.5338 | Val Loss: 0.6886, Val Acc: 0.5505\n",
      "Epoch 9/10 | Train Loss: 0.6909, Train Acc: 0.5338 | Val Loss: 0.6886, Val Acc: 0.5505\n",
      "Epoch 10/10 | Train Loss: 0.6909, Train Acc: 0.5338 | Val Loss: 0.6887, Val Acc: 0.5505\n",
      "Saved model for experiment 12.\n",
      "Experiment 12: Final Validation Accuracy: 0.5505\n",
      "Experiment 12: Test Accuracy: 0.5760\n",
      "Loaded model from experiment 12.\n",
      "Experiment 13: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6906, Train Acc: 0.5358 | Val Loss: 0.6859, Val Acc: 0.5760\n",
      "Epoch 2/10 | Train Loss: 0.6906, Train Acc: 0.5358 | Val Loss: 0.6848, Val Acc: 0.5760\n",
      "Epoch 3/10 | Train Loss: 0.6906, Train Acc: 0.5358 | Val Loss: 0.6843, Val Acc: 0.5760\n",
      "Epoch 4/10 | Train Loss: 0.6906, Train Acc: 0.5358 | Val Loss: 0.6845, Val Acc: 0.5760\n",
      "Epoch 5/10 | Train Loss: 0.6906, Train Acc: 0.5358 | Val Loss: 0.6850, Val Acc: 0.5760\n",
      "Epoch 6/10 | Train Loss: 0.6906, Train Acc: 0.5358 | Val Loss: 0.6847, Val Acc: 0.5760\n",
      "Epoch 7/10 | Train Loss: 0.6906, Train Acc: 0.5358 | Val Loss: 0.6849, Val Acc: 0.5760\n",
      "Epoch 8/10 | Train Loss: 0.6906, Train Acc: 0.5358 | Val Loss: 0.6844, Val Acc: 0.5760\n",
      "Epoch 9/10 | Train Loss: 0.6906, Train Acc: 0.5358 | Val Loss: 0.6849, Val Acc: 0.5760\n",
      "Epoch 10/10 | Train Loss: 0.6906, Train Acc: 0.5358 | Val Loss: 0.6848, Val Acc: 0.5760\n",
      "Saved model for experiment 13.\n",
      "Experiment 13: Final Validation Accuracy: 0.5760\n",
      "Experiment 13: Test Accuracy: 0.5799\n",
      "Loaded model from experiment 13.\n",
      "Experiment 14: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6894, Train Acc: 0.5431 | Val Loss: 0.6833, Val Acc: 0.5799\n",
      "Epoch 2/10 | Train Loss: 0.6894, Train Acc: 0.5431 | Val Loss: 0.6839, Val Acc: 0.5799\n",
      "Epoch 3/10 | Train Loss: 0.6895, Train Acc: 0.5431 | Val Loss: 0.6833, Val Acc: 0.5799\n",
      "Epoch 4/10 | Train Loss: 0.6895, Train Acc: 0.5431 | Val Loss: 0.6824, Val Acc: 0.5799\n",
      "Epoch 5/10 | Train Loss: 0.6895, Train Acc: 0.5431 | Val Loss: 0.6826, Val Acc: 0.5799\n",
      "Epoch 6/10 | Train Loss: 0.6895, Train Acc: 0.5431 | Val Loss: 0.6838, Val Acc: 0.5799\n",
      "Epoch 7/10 | Train Loss: 0.6894, Train Acc: 0.5431 | Val Loss: 0.6826, Val Acc: 0.5799\n",
      "Epoch 8/10 | Train Loss: 0.6894, Train Acc: 0.5431 | Val Loss: 0.6827, Val Acc: 0.5799\n",
      "Epoch 9/10 | Train Loss: 0.6894, Train Acc: 0.5431 | Val Loss: 0.6836, Val Acc: 0.5799\n",
      "Epoch 10/10 | Train Loss: 0.6894, Train Acc: 0.5431 | Val Loss: 0.6831, Val Acc: 0.5799\n",
      "Saved model for experiment 14.\n",
      "Experiment 14: Final Validation Accuracy: 0.5799\n",
      "Experiment 14: Test Accuracy: 0.6019\n",
      "Loaded model from experiment 14.\n",
      "Experiment 15: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6883, Train Acc: 0.5495 | Val Loss: 0.6777, Val Acc: 0.6019\n",
      "Epoch 2/10 | Train Loss: 0.6883, Train Acc: 0.5495 | Val Loss: 0.6778, Val Acc: 0.6019\n",
      "Epoch 3/10 | Train Loss: 0.6883, Train Acc: 0.5495 | Val Loss: 0.6783, Val Acc: 0.6019\n",
      "Epoch 4/10 | Train Loss: 0.6883, Train Acc: 0.5495 | Val Loss: 0.6771, Val Acc: 0.6019\n",
      "Epoch 5/10 | Train Loss: 0.6883, Train Acc: 0.5495 | Val Loss: 0.6772, Val Acc: 0.6019\n",
      "Epoch 6/10 | Train Loss: 0.6883, Train Acc: 0.5495 | Val Loss: 0.6771, Val Acc: 0.6019\n",
      "Epoch 7/10 | Train Loss: 0.6883, Train Acc: 0.5495 | Val Loss: 0.6770, Val Acc: 0.6019\n",
      "Epoch 8/10 | Train Loss: 0.6883, Train Acc: 0.5495 | Val Loss: 0.6788, Val Acc: 0.6019\n",
      "Epoch 9/10 | Train Loss: 0.6883, Train Acc: 0.5495 | Val Loss: 0.6779, Val Acc: 0.6019\n",
      "Epoch 10/10 | Train Loss: 0.6883, Train Acc: 0.5495 | Val Loss: 0.6781, Val Acc: 0.6019\n",
      "Saved model for experiment 15.\n",
      "Experiment 15: Final Validation Accuracy: 0.6019\n",
      "Experiment 15: Test Accuracy: 0.6290\n",
      "Loaded model from experiment 15.\n",
      "Experiment 16: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6864, Train Acc: 0.5581 | Val Loss: 0.6702, Val Acc: 0.6290\n",
      "Epoch 2/10 | Train Loss: 0.6864, Train Acc: 0.5581 | Val Loss: 0.6701, Val Acc: 0.6290\n",
      "Epoch 3/10 | Train Loss: 0.6864, Train Acc: 0.5581 | Val Loss: 0.6703, Val Acc: 0.6290\n",
      "Epoch 4/10 | Train Loss: 0.6864, Train Acc: 0.5581 | Val Loss: 0.6697, Val Acc: 0.6290\n",
      "Epoch 5/10 | Train Loss: 0.6864, Train Acc: 0.5581 | Val Loss: 0.6686, Val Acc: 0.6290\n",
      "Epoch 6/10 | Train Loss: 0.6864, Train Acc: 0.5581 | Val Loss: 0.6686, Val Acc: 0.6290\n",
      "Epoch 7/10 | Train Loss: 0.6864, Train Acc: 0.5581 | Val Loss: 0.6679, Val Acc: 0.6290\n",
      "Epoch 8/10 | Train Loss: 0.6864, Train Acc: 0.5581 | Val Loss: 0.6717, Val Acc: 0.6290\n",
      "Epoch 9/10 | Train Loss: 0.6864, Train Acc: 0.5581 | Val Loss: 0.6705, Val Acc: 0.6290\n",
      "Epoch 10/10 | Train Loss: 0.6864, Train Acc: 0.5581 | Val Loss: 0.6691, Val Acc: 0.6290\n",
      "Saved model for experiment 16.\n",
      "Experiment 16: Final Validation Accuracy: 0.6290\n",
      "Experiment 16: Test Accuracy: 0.5892\n",
      "Loaded model from experiment 16.\n",
      "Experiment 17: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6833, Train Acc: 0.5702 | Val Loss: 0.6783, Val Acc: 0.5892\n",
      "Epoch 2/10 | Train Loss: 0.6833, Train Acc: 0.5702 | Val Loss: 0.6780, Val Acc: 0.5892\n",
      "Epoch 3/10 | Train Loss: 0.6833, Train Acc: 0.5702 | Val Loss: 0.6779, Val Acc: 0.5892\n",
      "Epoch 4/10 | Train Loss: 0.6833, Train Acc: 0.5702 | Val Loss: 0.6783, Val Acc: 0.5892\n",
      "Epoch 5/10 | Train Loss: 0.6833, Train Acc: 0.5702 | Val Loss: 0.6777, Val Acc: 0.5892\n",
      "Epoch 6/10 | Train Loss: 0.6833, Train Acc: 0.5702 | Val Loss: 0.6781, Val Acc: 0.5892\n",
      "Epoch 7/10 | Train Loss: 0.6833, Train Acc: 0.5702 | Val Loss: 0.6777, Val Acc: 0.5892\n",
      "Epoch 8/10 | Train Loss: 0.6833, Train Acc: 0.5702 | Val Loss: 0.6782, Val Acc: 0.5892\n",
      "Epoch 9/10 | Train Loss: 0.6833, Train Acc: 0.5702 | Val Loss: 0.6780, Val Acc: 0.5892\n",
      "Epoch 10/10 | Train Loss: 0.6833, Train Acc: 0.5702 | Val Loss: 0.6781, Val Acc: 0.5892\n",
      "Saved model for experiment 17.\n",
      "Experiment 17: Final Validation Accuracy: 0.5892\n",
      "Experiment 17: Test Accuracy: 0.5489\n",
      "Loaded model from experiment 17.\n",
      "Experiment 18: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6813, Train Acc: 0.5770 | Val Loss: 0.6900, Val Acc: 0.5489\n",
      "Epoch 2/10 | Train Loss: 0.6813, Train Acc: 0.5770 | Val Loss: 0.6896, Val Acc: 0.5489\n",
      "Epoch 3/10 | Train Loss: 0.6813, Train Acc: 0.5770 | Val Loss: 0.6902, Val Acc: 0.5489\n",
      "Epoch 4/10 | Train Loss: 0.6813, Train Acc: 0.5770 | Val Loss: 0.6898, Val Acc: 0.5489\n",
      "Epoch 5/10 | Train Loss: 0.6813, Train Acc: 0.5770 | Val Loss: 0.6892, Val Acc: 0.5489\n",
      "Epoch 6/10 | Train Loss: 0.6813, Train Acc: 0.5770 | Val Loss: 0.6897, Val Acc: 0.5489\n",
      "Epoch 7/10 | Train Loss: 0.6813, Train Acc: 0.5770 | Val Loss: 0.6899, Val Acc: 0.5489\n",
      "Epoch 8/10 | Train Loss: 0.6813, Train Acc: 0.5770 | Val Loss: 0.6905, Val Acc: 0.5489\n",
      "Epoch 9/10 | Train Loss: 0.6813, Train Acc: 0.5770 | Val Loss: 0.6897, Val Acc: 0.5489\n",
      "Epoch 10/10 | Train Loss: 0.6813, Train Acc: 0.5770 | Val Loss: 0.6899, Val Acc: 0.5489\n",
      "Saved model for experiment 18.\n",
      "Experiment 18: Final Validation Accuracy: 0.5489\n",
      "Experiment 18: Test Accuracy: 0.5421\n",
      "Loaded model from experiment 18.\n",
      "Experiment 19: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6808, Train Acc: 0.5786 | Val Loss: 0.6918, Val Acc: 0.5421\n",
      "Epoch 2/10 | Train Loss: 0.6808, Train Acc: 0.5786 | Val Loss: 0.6918, Val Acc: 0.5421\n",
      "Epoch 3/10 | Train Loss: 0.6808, Train Acc: 0.5786 | Val Loss: 0.6919, Val Acc: 0.5421\n",
      "Epoch 4/10 | Train Loss: 0.6808, Train Acc: 0.5786 | Val Loss: 0.6926, Val Acc: 0.5421\n",
      "Epoch 5/10 | Train Loss: 0.6807, Train Acc: 0.5786 | Val Loss: 0.6925, Val Acc: 0.5421\n",
      "Epoch 6/10 | Train Loss: 0.6807, Train Acc: 0.5786 | Val Loss: 0.6926, Val Acc: 0.5421\n",
      "Epoch 7/10 | Train Loss: 0.6808, Train Acc: 0.5786 | Val Loss: 0.6925, Val Acc: 0.5421\n",
      "Epoch 8/10 | Train Loss: 0.6807, Train Acc: 0.5786 | Val Loss: 0.6925, Val Acc: 0.5421\n",
      "Epoch 9/10 | Train Loss: 0.6807, Train Acc: 0.5786 | Val Loss: 0.6926, Val Acc: 0.5421\n",
      "Epoch 10/10 | Train Loss: 0.6807, Train Acc: 0.5786 | Val Loss: 0.6924, Val Acc: 0.5421\n",
      "Saved model for experiment 19.\n",
      "Experiment 19: Final Validation Accuracy: 0.5421\n",
      "Experiment 19: Test Accuracy: 0.5424\n",
      "Loaded model from experiment 19.\n",
      "Experiment 20: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6812, Train Acc: 0.5772 | Val Loss: 0.6918, Val Acc: 0.5424\n",
      "Epoch 2/10 | Train Loss: 0.6812, Train Acc: 0.5772 | Val Loss: 0.6909, Val Acc: 0.5424\n",
      "Epoch 3/10 | Train Loss: 0.6812, Train Acc: 0.5772 | Val Loss: 0.6919, Val Acc: 0.5424\n",
      "Epoch 4/10 | Train Loss: 0.6812, Train Acc: 0.5772 | Val Loss: 0.6921, Val Acc: 0.5424\n",
      "Epoch 5/10 | Train Loss: 0.6812, Train Acc: 0.5772 | Val Loss: 0.6913, Val Acc: 0.5424\n",
      "Epoch 6/10 | Train Loss: 0.6812, Train Acc: 0.5772 | Val Loss: 0.6919, Val Acc: 0.5424\n",
      "Epoch 7/10 | Train Loss: 0.6812, Train Acc: 0.5772 | Val Loss: 0.6915, Val Acc: 0.5424\n",
      "Epoch 8/10 | Train Loss: 0.6812, Train Acc: 0.5772 | Val Loss: 0.6919, Val Acc: 0.5424\n",
      "Epoch 9/10 | Train Loss: 0.6812, Train Acc: 0.5772 | Val Loss: 0.6920, Val Acc: 0.5424\n",
      "Epoch 10/10 | Train Loss: 0.6812, Train Acc: 0.5772 | Val Loss: 0.6919, Val Acc: 0.5424\n",
      "Saved model for experiment 20.\n",
      "Experiment 20: Final Validation Accuracy: 0.5424\n",
      "Experiment 20: Test Accuracy: 0.5467\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta as ta\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from math import sqrt\n",
    "\n",
    "####################################\n",
    "# 1.     ( )\n",
    "####################################\n",
    "def calculate_indicators(data):\n",
    "    data['William_R'] = ta.willr(data['high'], data['low'], data['close'])\n",
    "    data['ATR'] = ta.atr(data['high'], data['low'], data['close'])\n",
    "    data['OBV'] = ta.obv(data['close'], data['volume'])\n",
    "    data['Z_Score'] = (data['close'] - data['close'].rolling(window=20).mean()) / data['close'].rolling(window=20).std()\n",
    "    data['Entropy'] = ta.entropy(data['close'], length=14)\n",
    "    data['SMA_5'] = data['close'].rolling(window=5).mean()\n",
    "    data['SMA_10'] = data['close'].rolling(window=10).mean()\n",
    "    data['SMA_20'] = data['close'].rolling(window=20).mean()\n",
    "    data['SMA_60'] = data['close'].rolling(window=60).mean()\n",
    "    data['SMA_120'] = data['close'].rolling(window=120).mean()\n",
    "    data['SMA_250'] = data['close'].rolling(window=250).mean()\n",
    "    data['RSI'] = ta.rsi(data['close'])\n",
    "    bb = ta.bbands(data['close'])\n",
    "    data['BB_Upper'], data['BB_Middle'], data['BB_Lower'] = bb.iloc[:, 0], bb.iloc[:, 1], bb.iloc[:, 2]\n",
    "    macd = ta.macd(data['close'])\n",
    "    data['MACD'] = macd.iloc[:, 0]\n",
    "    data['Stochastic'] = ta.stoch(data['high'], data['low'], data['close']).iloc[:, 0]\n",
    "    return data.dropna()\n",
    "\n",
    "####################################\n",
    "# 2. Datetime   Positional (Cyclical) \n",
    "####################################\n",
    "def encode_datetime_features_positional(data):\n",
    "    if 'datetime' not in data.columns:\n",
    "        data['datetime'] = pd.to_datetime(data.index)\n",
    "    data['hour_sin'] = np.sin(2 * np.pi * data['datetime'].dt.hour / 24)\n",
    "    data['hour_cos'] = np.cos(2 * np.pi * data['datetime'].dt.hour / 24)\n",
    "    data['day_sin'] = np.sin(2 * np.pi * data['datetime'].dt.dayofweek / 7)\n",
    "    data['day_cos'] = np.cos(2 * np.pi * data['datetime'].dt.dayofweek / 7)\n",
    "    data['week_of_month'] = (data['datetime'].dt.day - 1) // 7 + 1\n",
    "    data['week_sin'] = np.sin(2 * np.pi * data['week_of_month'] / 5)\n",
    "    data['week_cos'] = np.cos(2 * np.pi * data['week_of_month'] / 5)\n",
    "    data['month_sin'] = np.sin(2 * np.pi * data['datetime'].dt.month / 12)\n",
    "    data['month_cos'] = np.cos(2 * np.pi * data['datetime'].dt.month / 12)\n",
    "    return data\n",
    "\n",
    "####################################\n",
    "# 3.  50  rolling minmax scaling ( 0 )\n",
    "####################################\n",
    "def rolling_minmax_scale(series, window=12):\n",
    "    roll_min = series.rolling(window=window, min_periods=window).min()\n",
    "    roll_max = series.rolling(window=window, min_periods=window).max()\n",
    "    scaled = (series - roll_min) / ((roll_max - roll_min) + 1e-8)\n",
    "    return scaled\n",
    "\n",
    "####################################\n",
    "# 4. Binning  One-Hot  ( )\n",
    "####################################\n",
    "def bin_and_encode(data, features, bins=10, drop_original=True):\n",
    "    for feature in features:\n",
    "        data[f'{feature}_Bin'] = pd.cut(data[feature], bins=bins, labels=False)\n",
    "        one_hot = pd.get_dummies(data[f'{feature}_Bin'], prefix=f'{feature}_Bin')\n",
    "        expected_columns = [f'{feature}_Bin_{i}' for i in range(bins)]\n",
    "        one_hot = one_hot.reindex(columns=expected_columns, fill_value=0)\n",
    "        data = pd.concat([data, one_hot], axis=1)\n",
    "        if drop_original:\n",
    "            data.drop(columns=[f'{feature}_Bin'], inplace=True)\n",
    "    data = data.astype(np.float32)\n",
    "    return data\n",
    "\n",
    "####################################\n",
    "# 5.    \n",
    "####################################\n",
    "data = pd.read_csv(\"ETH_upbit_KRW_min5.csv\", index_col=0)\n",
    "data.columns = ['open', 'high', 'low', 'close', 'volume', 'value']\n",
    "data.index = pd.to_datetime(data.index)\n",
    "data = calculate_indicators(data)\n",
    "data = encode_datetime_features_positional(data)\n",
    "\n",
    "features_to_bin = ['open', 'high', 'low', 'volume', 'value', 'William_R',\n",
    "                   'ATR', 'OBV', 'Z_Score', 'Entropy', 'SMA_5', 'SMA_10', \n",
    "                   'SMA_20', 'SMA_60', 'SMA_120', 'SMA_250', 'RSI', 'BB_Upper', 'BB_Middle', \n",
    "                   'BB_Lower', 'MACD', 'Stochastic']\n",
    "datetime_features = ['hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'week_sin', 'week_cos', 'month_sin', 'month_cos']\n",
    "\n",
    "# close_target  close \n",
    "data['close_target'] = data['close']\n",
    "\n",
    "selected_features = features_to_bin + ['close_target'] + datetime_features\n",
    "data = data[selected_features].dropna()\n",
    "\n",
    "# rolling scaling  (window=512:   )\n",
    "for feature in features_to_bin:\n",
    "    data[feature] = rolling_minmax_scale(data[feature], window=32)\n",
    "data = data.dropna()\n",
    "\n",
    "data = bin_and_encode(data, features_to_bin, bins=10, drop_original=True)\n",
    "data['close_for_binning'] = data['close_target']\n",
    "data = bin_and_encode(data, ['close_for_binning'], bins=10, drop_original=False)\n",
    "data.drop(columns=['close_for_binning'], inplace=True)\n",
    "\n",
    "final_columns = []\n",
    "for feature in features_to_bin:\n",
    "    final_columns.extend([f'{feature}_Bin_{i}' for i in range(10)])\n",
    "final_columns.append('close_target')\n",
    "final_columns.extend([f'close_for_binning_Bin_{i}' for i in range(10)])\n",
    "final_columns.extend(datetime_features)\n",
    "data = data[final_columns]\n",
    "\n",
    "####################################\n",
    "# 6. Dataset \n",
    "####################################\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, lookback=32, target_col='close_target'):\n",
    "        self.data = data.values\n",
    "        self.lookback = lookback\n",
    "        self.target_idx = list(data.columns).index(target_col)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.lookback\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.lookback, :]\n",
    "        y = self.data[idx + self.lookback, self.target_idx]\n",
    "        y_prev = self.data[idx + self.lookback - 1, self.target_idx]\n",
    "        y_target = 1 if y > y_prev else 0\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y_target, dtype=torch.long)\n",
    "\n",
    "####################################\n",
    "# 7. Transformer Encoder  \n",
    "####################################\n",
    "# 7-1. Multi-Head Self-Attention\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == embed_dim, \"embed_dim num_heads  .\"\n",
    "        \n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key   = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out   = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        \n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / sqrt(self.head_dim)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, V)\n",
    "        \n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "\n",
    "# 7-2. Feed-Forward Network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ffn_dim):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, ffn_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(ffn_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "# 7-3. Transformer Encoder Layer (Self-Attention + FFN + Residual + LayerNorm)\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = FeedForward(embed_dim, ffn_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attn_out = self.self_attn(x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_out)\n",
    "        return x\n",
    "\n",
    "# 7-4. Encoder-Only Transformer  \n",
    "class EncoderOnlyTransformerCustom(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=512, num_heads=8, num_layers=6, ffn_dim=1024, num_classes=2, max_seq_len=32):\n",
    "        super(EncoderOnlyTransformerCustom, self).__init__()\n",
    "        self.token_embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embedding_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(embedding_dim, num_heads, ffn_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x = self.token_embedding(x)\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        x = x + self.position_embedding(positions)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x[:, -1, :]\n",
    "        return self.fc(x)\n",
    "\n",
    "####################################\n",
    "# 8.     (Fine-tuning  Validation Accuracy )\n",
    "####################################\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return total_loss / len(data_loader), correct / total\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, lr, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_state = None\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, device)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = model.state_dict()\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(data, num_experiments=21, lookback=32, num_epochs=10):\n",
    "    input_dim = data.shape[1]\n",
    "    step_size = 25000  #  \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    for exp in range(num_experiments):\n",
    "        train_start = exp * step_size\n",
    "        train_end = train_start + step_size * 8   #  \n",
    "        val_end = train_end + step_size            #  \n",
    "        test_end = val_end + step_size             #  \n",
    "        if test_end > len(data):\n",
    "            break\n",
    "        \n",
    "        train_data = data.iloc[train_start:train_end]\n",
    "        val_data = data.iloc[train_end:val_end]\n",
    "        test_data = data.iloc[val_end:test_end]\n",
    "        \n",
    "        train_dataset = TimeSeriesDataset(train_data, lookback=lookback, target_col='close_target')\n",
    "        val_dataset = TimeSeriesDataset(val_data, lookback=lookback, target_col='close_target')\n",
    "        test_dataset = TimeSeriesDataset(test_data, lookback=lookback, target_col='close_target')\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        # Fine-tuning:    lr,    lr \n",
    "        if exp == 0:\n",
    "            lr = 1e-4\n",
    "        else:\n",
    "            lr = 1e-5\n",
    "        model = EncoderOnlyTransformerCustom(input_dim=input_dim, embedding_dim=512, num_heads=8, \n",
    "                                               num_layers=6, ffn_dim=1024, num_classes=2, max_seq_len=32).to(device)\n",
    "        model_path = f\"model_experiment_{exp}.pth\"\n",
    "        if exp > 0:\n",
    "            try:\n",
    "                model.load_state_dict(torch.load(f\"model_experiment_{exp - 1}.pth\"))\n",
    "                print(f\"Loaded model from experiment {exp - 1}.\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Model file for experiment {exp - 1} not found. Starting fresh training.\")\n",
    "        \n",
    "        print(f\"Experiment {exp}: Training with lr={lr}\")\n",
    "        model = train_model(model, train_loader, val_loader, num_epochs, lr, device)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Saved model for experiment {exp}.\")\n",
    "        \n",
    "        #    \n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, device)\n",
    "        print(f\"Experiment {exp}: Final Validation Accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "        #   \n",
    "        test_loss, test_acc = evaluate_model(model, test_loader, device)\n",
    "        print(f\"Experiment {exp}: Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "train_and_evaluate(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base(cold start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 0: Training with lr=0.0001 (Cold Start)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.6953, Train Acc: 0.5174 | Val Loss: 0.6893, Val Acc: 0.5483\n",
      "Epoch 2/10 | Train Loss: 0.6919, Train Acc: 0.5273 | Val Loss: 0.6890, Val Acc: 0.5483\n",
      "Epoch 3/10 | Train Loss: 0.6918, Train Acc: 0.5290 | Val Loss: 0.6887, Val Acc: 0.5483\n",
      "Epoch 4/10 | Train Loss: 0.6917, Train Acc: 0.5288 | Val Loss: 0.6885, Val Acc: 0.5483\n",
      "Epoch 5/10 | Train Loss: 0.6917, Train Acc: 0.5293 | Val Loss: 0.6901, Val Acc: 0.5483\n",
      "Epoch 6/10 | Train Loss: 0.6916, Train Acc: 0.5294 | Val Loss: 0.6891, Val Acc: 0.5483\n",
      "Epoch 7/10 | Train Loss: 0.6916, Train Acc: 0.5290 | Val Loss: 0.6887, Val Acc: 0.5483\n",
      "Epoch 8/10 | Train Loss: 0.6915, Train Acc: 0.5295 | Val Loss: 0.6892, Val Acc: 0.5483\n",
      "Epoch 9/10 | Train Loss: 0.6915, Train Acc: 0.5295 | Val Loss: 0.6892, Val Acc: 0.5483\n",
      "Epoch 10/10 | Train Loss: 0.6915, Train Acc: 0.5295 | Val Loss: 0.6891, Val Acc: 0.5483\n",
      "Saved model for experiment 0.\n",
      "Experiment 0: Final Validation Accuracy: 0.5483\n",
      "Experiment 0: Test Accuracy: 0.5290\n",
      "Experiment 1: Training with lr=0.0001 (Cold Start)\n",
      "Epoch 1/10 | Train Loss: 0.6946, Train Acc: 0.5229 | Val Loss: 0.6922, Val Acc: 0.5290\n",
      "Epoch 2/10 | Train Loss: 0.6915, Train Acc: 0.5312 | Val Loss: 0.6940, Val Acc: 0.5290\n",
      "Epoch 3/10 | Train Loss: 0.6913, Train Acc: 0.5331 | Val Loss: 0.6918, Val Acc: 0.5290\n",
      "Epoch 4/10 | Train Loss: 0.6912, Train Acc: 0.5335 | Val Loss: 0.6917, Val Acc: 0.5290\n",
      "Epoch 5/10 | Train Loss: 0.6911, Train Acc: 0.5335 | Val Loss: 0.6922, Val Acc: 0.5290\n",
      "Epoch 6/10 | Train Loss: 0.6912, Train Acc: 0.5335 | Val Loss: 0.6915, Val Acc: 0.5290\n",
      "Epoch 7/10 | Train Loss: 0.6911, Train Acc: 0.5332 | Val Loss: 0.6915, Val Acc: 0.5290\n",
      "Epoch 8/10 | Train Loss: 0.6911, Train Acc: 0.5335 | Val Loss: 0.6915, Val Acc: 0.5290\n",
      "Epoch 9/10 | Train Loss: 0.6911, Train Acc: 0.5335 | Val Loss: 0.6918, Val Acc: 0.5290\n",
      "Epoch 10/10 | Train Loss: 0.6910, Train Acc: 0.5335 | Val Loss: 0.6915, Val Acc: 0.5290\n",
      "Saved model for experiment 1.\n",
      "Experiment 1: Final Validation Accuracy: 0.5290\n",
      "Experiment 1: Test Accuracy: 0.5201\n",
      "Experiment 2: Training with lr=0.0001 (Cold Start)\n",
      "Epoch 1/10 | Train Loss: 0.6945, Train Acc: 0.5275 | Val Loss: 0.6924, Val Acc: 0.5201\n",
      "Epoch 2/10 | Train Loss: 0.6909, Train Acc: 0.5360 | Val Loss: 0.6930, Val Acc: 0.5201\n",
      "Epoch 3/10 | Train Loss: 0.6908, Train Acc: 0.5372 | Val Loss: 0.6947, Val Acc: 0.5201\n",
      "Epoch 4/10 | Train Loss: 0.6907, Train Acc: 0.5368 | Val Loss: 0.6946, Val Acc: 0.5201\n",
      "Epoch 5/10 | Train Loss: 0.6905, Train Acc: 0.5375 | Val Loss: 0.6938, Val Acc: 0.5201\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 311\u001b[0m\n\u001b[0;32m    308\u001b[0m         test_loss, test_acc \u001b[38;5;241m=\u001b[39m evaluate_model(model, test_loader, device)\n\u001b[0;32m    309\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExperiment \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 311\u001b[0m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 301\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(data, num_experiments, lookback, num_epochs)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m#      (Cold Start )\u001b[39;00m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExperiment \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Training with lr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Cold Start)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 301\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), model_path)\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved model for experiment \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 251\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, num_epochs, lr, device)\u001b[0m\n\u001b[0;32m    248\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m    249\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 251\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (outputs\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m y)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    253\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta as ta\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from math import sqrt\n",
    "\n",
    "####################################\n",
    "# 1.     ( )\n",
    "####################################\n",
    "def calculate_indicators(data):\n",
    "    data['William_R'] = ta.willr(data['high'], data['low'], data['close'])\n",
    "    data['ATR'] = ta.atr(data['high'], data['low'], data['close'])\n",
    "    data['OBV'] = ta.obv(data['close'], data['volume'])\n",
    "    data['Z_Score'] = (data['close'] - data['close'].rolling(window=20).mean()) / data['close'].rolling(window=20).std()\n",
    "    data['Entropy'] = ta.entropy(data['close'], length=14)\n",
    "    data['SMA_5'] = data['close'].rolling(window=5).mean()\n",
    "    data['SMA_10'] = data['close'].rolling(window=10).mean()\n",
    "    data['SMA_20'] = data['close'].rolling(window=20).mean()\n",
    "    data['SMA_60'] = data['close'].rolling(window=60).mean()\n",
    "    data['SMA_120'] = data['close'].rolling(window=120).mean()\n",
    "    data['SMA_250'] = data['close'].rolling(window=250).mean()\n",
    "    data['RSI'] = ta.rsi(data['close'])\n",
    "    bb = ta.bbands(data['close'])\n",
    "    data['BB_Upper'], data['BB_Middle'], data['BB_Lower'] = bb.iloc[:, 0], bb.iloc[:, 1], bb.iloc[:, 2]\n",
    "    macd = ta.macd(data['close'])\n",
    "    data['MACD'] = macd.iloc[:, 0]\n",
    "    data['Stochastic'] = ta.stoch(data['high'], data['low'], data['close']).iloc[:, 0]\n",
    "    return data.dropna()\n",
    "\n",
    "####################################\n",
    "# 2. Datetime   Positional (Cyclical) \n",
    "####################################\n",
    "def encode_datetime_features_positional(data):\n",
    "    if 'datetime' not in data.columns:\n",
    "        data['datetime'] = pd.to_datetime(data.index)\n",
    "    data['hour_sin'] = np.sin(2 * np.pi * data['datetime'].dt.hour / 24)\n",
    "    data['hour_cos'] = np.cos(2 * np.pi * data['datetime'].dt.hour / 24)\n",
    "    data['day_sin'] = np.sin(2 * np.pi * data['datetime'].dt.dayofweek / 7)\n",
    "    data['day_cos'] = np.cos(2 * np.pi * data['datetime'].dt.dayofweek / 7)\n",
    "    data['week_of_month'] = (data['datetime'].dt.day - 1) // 7 + 1\n",
    "    data['week_sin'] = np.sin(2 * np.pi * data['week_of_month'] / 5)\n",
    "    data['week_cos'] = np.cos(2 * np.pi * data['week_of_month'] / 5)\n",
    "    data['month_sin'] = np.sin(2 * np.pi * data['datetime'].dt.month / 12)\n",
    "    data['month_cos'] = np.cos(2 * np.pi * data['datetime'].dt.month / 12)\n",
    "    return data\n",
    "\n",
    "####################################\n",
    "# 3.  50  rolling minmax scaling ( 0 )\n",
    "####################################\n",
    "def rolling_minmax_scale(series, window=50):\n",
    "    roll_min = series.rolling(window=window, min_periods=window).min()\n",
    "    roll_max = series.rolling(window=window, min_periods=window).max()\n",
    "    scaled = (series - roll_min) / ((roll_max - roll_min) + 1e-8)\n",
    "    return scaled\n",
    "\n",
    "####################################\n",
    "# 4. Binning  One-Hot  ( )\n",
    "####################################\n",
    "def bin_and_encode(data, features, bins=10, drop_original=True):\n",
    "    for feature in features:\n",
    "        data[f'{feature}_Bin'] = pd.cut(data[feature], bins=bins, labels=False)\n",
    "        one_hot = pd.get_dummies(data[f'{feature}_Bin'], prefix=f'{feature}_Bin')\n",
    "        expected_columns = [f'{feature}_Bin_{i}' for i in range(bins)]\n",
    "        one_hot = one_hot.reindex(columns=expected_columns, fill_value=0)\n",
    "        data = pd.concat([data, one_hot], axis=1)\n",
    "        if drop_original:\n",
    "            data.drop(columns=[f'{feature}_Bin'], inplace=True)\n",
    "    data = data.astype(np.float32)\n",
    "    return data\n",
    "\n",
    "####################################\n",
    "# 5.    \n",
    "####################################\n",
    "data = pd.read_csv(\"BTC_upbit_KRW_min5.csv\", index_col=0)\n",
    "data.columns = ['open', 'high', 'low', 'close', 'volume', 'value']\n",
    "data.index = pd.to_datetime(data.index)\n",
    "data = calculate_indicators(data)\n",
    "data = encode_datetime_features_positional(data)\n",
    "\n",
    "features_to_bin = ['open', 'high', 'low', 'volume', 'value', 'William_R',\n",
    "                   'ATR', 'OBV', 'Z_Score', 'Entropy', 'SMA_5', 'SMA_10', \n",
    "                   'SMA_20', 'SMA_60', 'SMA_120', 'SMA_250', 'RSI', 'BB_Upper', 'BB_Middle', \n",
    "                   'BB_Lower', 'MACD', 'Stochastic']\n",
    "datetime_features = ['hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'week_sin', 'week_cos', 'month_sin', 'month_cos']\n",
    "\n",
    "# close_target  close \n",
    "data['close_target'] = data['close']\n",
    "\n",
    "selected_features = features_to_bin + ['close_target'] + datetime_features\n",
    "data = data[selected_features].dropna()\n",
    "\n",
    "#      window \n",
    "for feature in features_to_bin:\n",
    "    data[feature] = rolling_minmax_scale(data[feature], window=512)\n",
    "data = data.dropna()\n",
    "\n",
    "data = bin_and_encode(data, features_to_bin, bins=10, drop_original=True)\n",
    "data['close_for_binning'] = data['close_target']\n",
    "data = bin_and_encode(data, ['close_for_binning'], bins=10, drop_original=False)\n",
    "data.drop(columns=['close_for_binning'], inplace=True)\n",
    "\n",
    "final_columns = []\n",
    "for feature in features_to_bin:\n",
    "    final_columns.extend([f'{feature}_Bin_{i}' for i in range(10)])\n",
    "final_columns.append('close_target')\n",
    "final_columns.extend([f'close_for_binning_Bin_{i}' for i in range(10)])\n",
    "final_columns.extend(datetime_features)\n",
    "data = data[final_columns]\n",
    "\n",
    "####################################\n",
    "# 6. Dataset \n",
    "####################################\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, lookback=12, target_col='close_target'):\n",
    "        self.data = data.values\n",
    "        self.lookback = lookback\n",
    "        self.target_idx = list(data.columns).index(target_col)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.lookback\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.lookback, :]\n",
    "        y = self.data[idx + self.lookback, self.target_idx]\n",
    "        y_prev = self.data[idx + self.lookback - 1, self.target_idx]\n",
    "        y_target = 1 if y > y_prev else 0\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y_target, dtype=torch.long)\n",
    "\n",
    "####################################\n",
    "# 7. Transformer Encoder  \n",
    "####################################\n",
    "# 7-1. Multi-Head Self-Attention\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == embed_dim, \"embed_dim num_heads  .\"\n",
    "        \n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key   = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out   = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        \n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / sqrt(self.head_dim)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, V)\n",
    "        \n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "\n",
    "# 7-2. Feed-Forward Network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ffn_dim):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, ffn_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(ffn_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "# 7-3. Transformer Encoder Layer (Self-Attention + FFN + Residual + LayerNorm)\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = FeedForward(embed_dim, ffn_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attn_out = self.self_attn(x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_out)\n",
    "        return x\n",
    "\n",
    "# 7-4. Encoder-Only Transformer   (Cold Start )\n",
    "class EncoderOnlyTransformerCustom(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=512, num_heads=8, num_layers=4, ffn_dim=1024, num_classes=2, max_seq_len=512):\n",
    "        super(EncoderOnlyTransformerCustom, self).__init__()\n",
    "        self.token_embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embedding_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(embedding_dim, num_heads, ffn_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x = self.token_embedding(x)\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        x = x + self.position_embedding(positions)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x[:, -1, :]\n",
    "        return self.fc(x)\n",
    "\n",
    "####################################\n",
    "# 8.     (Cold Start , Validation Accuracy )\n",
    "####################################\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return total_loss / len(data_loader), correct / total\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, lr, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_state = None\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, device)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = model.state_dict()\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(data, num_experiments=21, lookback=50, num_epochs=10):\n",
    "    input_dim = data.shape[1]\n",
    "    step_size = 25000  #  \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Cold Start :       (    )\n",
    "    for exp in range(num_experiments):\n",
    "        train_start = exp * step_size\n",
    "        train_end = train_start + step_size * 8   #  \n",
    "        val_end = train_end + step_size            #  \n",
    "        test_end = val_end + step_size             #  \n",
    "        if test_end > len(data):\n",
    "            break\n",
    "        \n",
    "        train_data = data.iloc[train_start:train_end]\n",
    "        val_data = data.iloc[train_end:val_end]\n",
    "        test_data = data.iloc[val_end:test_end]\n",
    "        \n",
    "        train_dataset = TimeSeriesDataset(train_data, lookback=lookback, target_col='close_target')\n",
    "        val_dataset = TimeSeriesDataset(val_data, lookback=lookback, target_col='close_target')\n",
    "        test_dataset = TimeSeriesDataset(test_data, lookback=lookback, target_col='close_target')\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        #      (Cold Start)\n",
    "        lr = 1e-4\n",
    "        model = EncoderOnlyTransformerCustom(input_dim=input_dim, embedding_dim=512, num_heads=8, \n",
    "                                               num_layers=4, ffn_dim=1024, num_classes=2, max_seq_len=512).to(device)\n",
    "        model_path = f\"model_experiment_{exp}.pth\"\n",
    "        #      (Cold Start )\n",
    "        print(f\"Experiment {exp}: Training with lr={lr} (Cold Start)\")\n",
    "        model = train_model(model, train_loader, val_loader, num_epochs, lr, device)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Saved model for experiment {exp}.\")\n",
    "        \n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, device)\n",
    "        print(f\"Experiment {exp}: Final Validation Accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "        test_loss, test_acc = evaluate_model(model, test_loader, device)\n",
    "        print(f\"Experiment {exp}: Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "train_and_evaluate(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 0: Training with lr=0.0001 (Partial Transfer)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.6956, Train Acc: 0.5190 | Val Loss: 0.6886, Val Acc: 0.5483\n",
      "Epoch 2/10 | Train Loss: 0.6921, Train Acc: 0.5268 | Val Loss: 0.6917, Val Acc: 0.5483\n",
      "Epoch 3/10 | Train Loss: 0.6919, Train Acc: 0.5286 | Val Loss: 0.6886, Val Acc: 0.5483\n",
      "Epoch 4/10 | Train Loss: 0.6918, Train Acc: 0.5282 | Val Loss: 0.6913, Val Acc: 0.5483\n",
      "Epoch 5/10 | Train Loss: 0.6915, Train Acc: 0.5295 | Val Loss: 0.6895, Val Acc: 0.5483\n",
      "Epoch 6/10 | Train Loss: 0.6915, Train Acc: 0.5293 | Val Loss: 0.6892, Val Acc: 0.5483\n",
      "Epoch 7/10 | Train Loss: 0.6915, Train Acc: 0.5295 | Val Loss: 0.6896, Val Acc: 0.5483\n",
      "Epoch 8/10 | Train Loss: 0.6915, Train Acc: 0.5295 | Val Loss: 0.6896, Val Acc: 0.5483\n",
      "Epoch 9/10 | Train Loss: 0.6915, Train Acc: 0.5295 | Val Loss: 0.6889, Val Acc: 0.5483\n",
      "Epoch 10/10 | Train Loss: 0.6915, Train Acc: 0.5295 | Val Loss: 0.6895, Val Acc: 0.5483\n",
      "Saved model for experiment 0.\n",
      "Experiment 0: Final Validation Accuracy: 0.5483\n",
      "Experiment 0: Test Accuracy: 0.5290\n",
      "Loaded model from experiment 0 for partial transfer.\n",
      "Experiment 1: Training with lr=0.0001 (Partial Transfer)\n",
      "Epoch 1/10 | Train Loss: 0.6911, Train Acc: 0.5335 | Val Loss: 0.6916, Val Acc: 0.5290\n",
      "Epoch 2/10 | Train Loss: 0.6911, Train Acc: 0.5335 | Val Loss: 0.6915, Val Acc: 0.5290\n",
      "Epoch 3/10 | Train Loss: 0.6911, Train Acc: 0.5333 | Val Loss: 0.6915, Val Acc: 0.5290\n",
      "Epoch 4/10 | Train Loss: 0.6911, Train Acc: 0.5333 | Val Loss: 0.6915, Val Acc: 0.5290\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 328\u001b[0m\n\u001b[0;32m    325\u001b[0m         test_loss, test_acc \u001b[38;5;241m=\u001b[39m evaluate_model(model, test_loader, device)\n\u001b[0;32m    326\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExperiment \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 328\u001b[0m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 318\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(data, num_experiments, lookback, num_epochs)\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel file for experiment \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found. Starting fresh training.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExperiment \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Training with lr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Partial Transfer)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 318\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), model_path)\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved model for experiment \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 245\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, num_epochs, lr, device)\u001b[0m\n\u001b[0;32m    243\u001b[0m x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    244\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 245\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y)\n\u001b[0;32m    247\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 207\u001b[0m, in \u001b[0;36mEncoderOnlyTransformerCustom.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    206\u001b[0m     batch_size, seq_len, _ \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m--> 207\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    208\u001b[0m     positions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(seq_len, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(batch_size, seq_len)\n\u001b[0;32m    209\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding(positions)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta as ta\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from math import sqrt\n",
    "\n",
    "####################################\n",
    "# 1.     ( )\n",
    "####################################\n",
    "def calculate_indicators(data):\n",
    "    data['William_R'] = ta.willr(data['high'], data['low'], data['close'])\n",
    "    data['ATR'] = ta.atr(data['high'], data['low'], data['close'])\n",
    "    data['OBV'] = ta.obv(data['close'], data['volume'])\n",
    "    data['Z_Score'] = (data['close'] - data['close'].rolling(window=20).mean()) / data['close'].rolling(window=20).std()\n",
    "    data['Entropy'] = ta.entropy(data['close'], length=14)\n",
    "    data['SMA_5'] = data['close'].rolling(window=5).mean()\n",
    "    data['SMA_10'] = data['close'].rolling(window=10).mean()\n",
    "    data['SMA_20'] = data['close'].rolling(window=20).mean()\n",
    "    data['SMA_60'] = data['close'].rolling(window=60).mean()\n",
    "    data['SMA_120'] = data['close'].rolling(window=120).mean()\n",
    "    data['SMA_250'] = data['close'].rolling(window=250).mean()\n",
    "    data['RSI'] = ta.rsi(data['close'])\n",
    "    bb = ta.bbands(data['close'])\n",
    "    data['BB_Upper'], data['BB_Middle'], data['BB_Lower'] = bb.iloc[:, 0], bb.iloc[:, 1], bb.iloc[:, 2]\n",
    "    macd = ta.macd(data['close'])\n",
    "    data['MACD'] = macd.iloc[:, 0]\n",
    "    data['Stochastic'] = ta.stoch(data['high'], data['low'], data['close']).iloc[:, 0]\n",
    "    return data.dropna()\n",
    "\n",
    "####################################\n",
    "# 2. Datetime   Positional (Cyclical) \n",
    "####################################\n",
    "def encode_datetime_features_positional(data):\n",
    "    if 'datetime' not in data.columns:\n",
    "        data['datetime'] = pd.to_datetime(data.index)\n",
    "    data['hour_sin'] = np.sin(2 * np.pi * data['datetime'].dt.hour / 24)\n",
    "    data['hour_cos'] = np.cos(2 * np.pi * data['datetime'].dt.hour / 24)\n",
    "    data['day_sin'] = np.sin(2 * np.pi * data['datetime'].dt.dayofweek / 7)\n",
    "    data['day_cos'] = np.cos(2 * np.pi * data['datetime'].dt.dayofweek / 7)\n",
    "    data['week_of_month'] = (data['datetime'].dt.day - 1) // 7 + 1\n",
    "    data['week_sin'] = np.sin(2 * np.pi * data['week_of_month'] / 5)\n",
    "    data['week_cos'] = np.cos(2 * np.pi * data['week_of_month'] / 5)\n",
    "    data['month_sin'] = np.sin(2 * np.pi * data['datetime'].dt.month / 12)\n",
    "    data['month_cos'] = np.cos(2 * np.pi * data['datetime'].dt.month / 12)\n",
    "    return data\n",
    "\n",
    "####################################\n",
    "# 3.  50  rolling minmax scaling ( 0 )\n",
    "####################################\n",
    "def rolling_minmax_scale(series, window=50):\n",
    "    roll_min = series.rolling(window=window, min_periods=window).min()\n",
    "    roll_max = series.rolling(window=window, min_periods=window).max()\n",
    "    scaled = (series - roll_min) / ((roll_max - roll_min) + 1e-8)\n",
    "    return scaled\n",
    "\n",
    "####################################\n",
    "# 4. Binning  One-Hot  ( )\n",
    "####################################\n",
    "def bin_and_encode(data, features, bins=10, drop_original=True):\n",
    "    for feature in features:\n",
    "        data[f'{feature}_Bin'] = pd.cut(data[feature], bins=bins, labels=False)\n",
    "        one_hot = pd.get_dummies(data[f'{feature}_Bin'], prefix=f'{feature}_Bin')\n",
    "        expected_columns = [f'{feature}_Bin_{i}' for i in range(bins)]\n",
    "        one_hot = one_hot.reindex(columns=expected_columns, fill_value=0)\n",
    "        data = pd.concat([data, one_hot], axis=1)\n",
    "        if drop_original:\n",
    "            data.drop(columns=[f'{feature}_Bin'], inplace=True)\n",
    "    data = data.astype(np.float32)\n",
    "    return data\n",
    "\n",
    "####################################\n",
    "# 5.    \n",
    "####################################\n",
    "data = pd.read_csv(\"BTC_upbit_KRW_min5.csv\", index_col=0)\n",
    "data.columns = ['open', 'high', 'low', 'close', 'volume', 'value']\n",
    "data.index = pd.to_datetime(data.index)\n",
    "data = calculate_indicators(data)\n",
    "data = encode_datetime_features_positional(data)\n",
    "\n",
    "features_to_bin = ['open', 'high', 'low', 'volume', 'value', 'William_R',\n",
    "                   'ATR', 'OBV', 'Z_Score', 'Entropy', 'SMA_5', 'SMA_10', \n",
    "                   'SMA_20', 'SMA_60', 'SMA_120', 'SMA_250', 'RSI', 'BB_Upper', 'BB_Middle', \n",
    "                   'BB_Lower', 'MACD', 'Stochastic']\n",
    "datetime_features = ['hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'week_sin', 'week_cos', 'month_sin', 'month_cos']\n",
    "\n",
    "# close_target  close \n",
    "data['close_target'] = data['close']\n",
    "\n",
    "selected_features = features_to_bin + ['close_target'] + datetime_features\n",
    "data = data[selected_features].dropna()\n",
    "\n",
    "#      window  (: window=512)\n",
    "for feature in features_to_bin:\n",
    "    data[feature] = rolling_minmax_scale(data[feature], window=512)\n",
    "data = data.dropna()\n",
    "\n",
    "data = bin_and_encode(data, features_to_bin, bins=10, drop_original=True)\n",
    "data['close_for_binning'] = data['close_target']\n",
    "data = bin_and_encode(data, ['close_for_binning'], bins=10, drop_original=False)\n",
    "data.drop(columns=['close_for_binning'], inplace=True)\n",
    "\n",
    "final_columns = []\n",
    "for feature in features_to_bin:\n",
    "    final_columns.extend([f'{feature}_Bin_{i}' for i in range(10)])\n",
    "final_columns.append('close_target')\n",
    "final_columns.extend([f'close_for_binning_Bin_{i}' for i in range(10)])\n",
    "final_columns.extend(datetime_features)\n",
    "data = data[final_columns]\n",
    "\n",
    "####################################\n",
    "# 6. Dataset \n",
    "####################################\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, lookback=12, target_col='close_target'):\n",
    "        self.data = data.values\n",
    "        self.lookback = lookback\n",
    "        self.target_idx = list(data.columns).index(target_col)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.lookback\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.lookback, :]\n",
    "        y = self.data[idx + self.lookback, self.target_idx]\n",
    "        y_prev = self.data[idx + self.lookback - 1, self.target_idx]\n",
    "        y_target = 1 if y > y_prev else 0\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y_target, dtype=torch.long)\n",
    "\n",
    "####################################\n",
    "# 7. Transformer Encoder  \n",
    "####################################\n",
    "# 7-1. Multi-Head Self-Attention\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == embed_dim, \"embed_dim num_heads  .\"\n",
    "        \n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key   = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out   = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        \n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / sqrt(self.head_dim)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, V)\n",
    "        \n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "\n",
    "# 7-2. Feed-Forward Network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ffn_dim):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, ffn_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(ffn_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "# 7-3. Transformer Encoder Layer (Self-Attention + FFN + Residual + LayerNorm)\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = FeedForward(embed_dim, ffn_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attn_out = self.self_attn(x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_out)\n",
    "        return x\n",
    "\n",
    "# 7-4. Encoder-Only Transformer   (  )\n",
    "class EncoderOnlyTransformerCustom(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=512, num_heads=8, num_layers=4, ffn_dim=1024, num_classes=2, max_seq_len=512):\n",
    "        super(EncoderOnlyTransformerCustom, self).__init__()\n",
    "        self.token_embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embedding_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(embedding_dim, num_heads, ffn_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x = self.token_embedding(x)\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        x = x + self.position_embedding(positions)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x[:, -1, :]\n",
    "        return self.fc(x)\n",
    "\n",
    "####################################\n",
    "# 8.     (  , Validation Accuracy )\n",
    "####################################\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return total_loss / len(data_loader), correct / total\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, lr, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_state = None\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, device)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = model.state_dict()\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(data, num_experiments=21, lookback=50, num_epochs=10):\n",
    "    input_dim = data.shape[1]\n",
    "    step_size = 25000  #  \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    for exp in range(num_experiments):\n",
    "        train_start = exp * step_size\n",
    "        train_end = train_start + step_size * 8\n",
    "        val_end = train_end + step_size\n",
    "        test_end = val_end + step_size\n",
    "        if test_end > len(data):\n",
    "            break\n",
    "        \n",
    "        train_data = data.iloc[train_start:train_end]\n",
    "        val_data = data.iloc[train_end:val_end]\n",
    "        test_data = data.iloc[val_end:test_end]\n",
    "        \n",
    "        train_dataset = TimeSeriesDataset(train_data, lookback=lookback, target_col='close_target')\n",
    "        val_dataset = TimeSeriesDataset(val_data, lookback=lookback, target_col='close_target')\n",
    "        test_dataset = TimeSeriesDataset(test_data, lookback=lookback, target_col='close_target')\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        # Fine-tuning:      ( )\n",
    "        # exp == 0  cold start   \n",
    "        lr = 1e-4\n",
    "        model = EncoderOnlyTransformerCustom(input_dim=input_dim, embedding_dim=512, num_heads=8, \n",
    "                                               num_layers=4, ffn_dim=1024, num_classes=2, max_seq_len=512).to(device)\n",
    "        model_path = f\"model_experiment_{exp}.pth\"\n",
    "        if exp > 0:\n",
    "            try:\n",
    "                #      \n",
    "                model.load_state_dict(torch.load(f\"model_experiment_{exp - 1}.pth\"))\n",
    "                print(f\"Loaded model from experiment {exp - 1} for partial transfer.\")\n",
    "                #  : token_embedding, position_embedding,  encoder   \n",
    "                for param in model.token_embedding.parameters():\n",
    "                    param.requires_grad = False\n",
    "                for param in model.position_embedding.parameters():\n",
    "                    param.requires_grad = False\n",
    "                num_freeze = len(model.layers) // 2  # :    freeze\n",
    "                for i in range(num_freeze):\n",
    "                    for param in model.layers[i].parameters():\n",
    "                        param.requires_grad = False\n",
    "                #    \n",
    "            except FileNotFoundError:\n",
    "                print(f\"Model file for experiment {exp - 1} not found. Starting fresh training.\")\n",
    "        \n",
    "        print(f\"Experiment {exp}: Training with lr={lr} (Partial Transfer)\")\n",
    "        model = train_model(model, train_loader, val_loader, num_epochs, lr, device)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Saved model for experiment {exp}.\")\n",
    "        \n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, device)\n",
    "        print(f\"Experiment {exp}: Final Validation Accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "        test_loss, test_acc = evaluate_model(model, test_loader, device)\n",
    "        print(f\"Experiment {exp}: Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "train_and_evaluate(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 0: Training with lr=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.6951, Train Acc: 0.5213 | Val Loss: 0.6952, Val Acc: 0.4754\n",
      "Epoch 2/10 | Train Loss: 0.6918, Train Acc: 0.5295 | Val Loss: 0.6919, Val Acc: 0.5246\n",
      "Epoch 3/10 | Train Loss: 0.6915, Train Acc: 0.5312 | Val Loss: 0.6928, Val Acc: 0.5246\n",
      "Epoch 4/10 | Train Loss: 0.6914, Train Acc: 0.5313 | Val Loss: 0.6921, Val Acc: 0.5246\n",
      "Epoch 5/10 | Train Loss: 0.6914, Train Acc: 0.5314 | Val Loss: 0.6937, Val Acc: 0.5246\n",
      "Epoch 6/10 | Train Loss: 0.6913, Train Acc: 0.5316 | Val Loss: 0.6924, Val Acc: 0.5246\n",
      "Epoch 7/10 | Train Loss: 0.6913, Train Acc: 0.5316 | Val Loss: 0.6920, Val Acc: 0.5246\n",
      "Epoch 8/10 | Train Loss: 0.6913, Train Acc: 0.5316 | Val Loss: 0.6923, Val Acc: 0.5246\n",
      "Epoch 9/10 | Train Loss: 0.6912, Train Acc: 0.5316 | Val Loss: 0.6921, Val Acc: 0.5246\n",
      "Epoch 10/10 | Train Loss: 0.6912, Train Acc: 0.5316 | Val Loss: 0.6921, Val Acc: 0.5246\n",
      "Saved model for experiment 0.\n",
      "Experiment 0: Final Validation Accuracy: 0.5246\n",
      "Experiment 0: Test Accuracy: 0.5292\n",
      "Loaded model from experiment 0.\n",
      "Experiment 1: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6910, Train Acc: 0.5331 | Val Loss: 0.6916, Val Acc: 0.5292\n",
      "Epoch 2/10 | Train Loss: 0.6910, Train Acc: 0.5331 | Val Loss: 0.6915, Val Acc: 0.5292\n",
      "Epoch 3/10 | Train Loss: 0.6910, Train Acc: 0.5331 | Val Loss: 0.6915, Val Acc: 0.5292\n",
      "Epoch 4/10 | Train Loss: 0.6910, Train Acc: 0.5331 | Val Loss: 0.6915, Val Acc: 0.5292\n",
      "Epoch 5/10 | Train Loss: 0.6910, Train Acc: 0.5331 | Val Loss: 0.6915, Val Acc: 0.5292\n",
      "Epoch 6/10 | Train Loss: 0.6910, Train Acc: 0.5331 | Val Loss: 0.6914, Val Acc: 0.5292\n",
      "Epoch 7/10 | Train Loss: 0.6910, Train Acc: 0.5331 | Val Loss: 0.6914, Val Acc: 0.5292\n",
      "Epoch 8/10 | Train Loss: 0.6910, Train Acc: 0.5331 | Val Loss: 0.6914, Val Acc: 0.5292\n",
      "Epoch 9/10 | Train Loss: 0.6910, Train Acc: 0.5331 | Val Loss: 0.6915, Val Acc: 0.5292\n",
      "Epoch 10/10 | Train Loss: 0.6910, Train Acc: 0.5331 | Val Loss: 0.6915, Val Acc: 0.5292\n",
      "Saved model for experiment 1.\n",
      "Experiment 1: Final Validation Accuracy: 0.5292\n",
      "Experiment 1: Test Accuracy: 0.5286\n",
      "Loaded model from experiment 1.\n",
      "Experiment 2: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6905, Train Acc: 0.5363 | Val Loss: 0.6916, Val Acc: 0.5286\n",
      "Epoch 2/10 | Train Loss: 0.6905, Train Acc: 0.5363 | Val Loss: 0.6916, Val Acc: 0.5286\n",
      "Epoch 3/10 | Train Loss: 0.6905, Train Acc: 0.5363 | Val Loss: 0.6917, Val Acc: 0.5286\n",
      "Epoch 4/10 | Train Loss: 0.6905, Train Acc: 0.5363 | Val Loss: 0.6916, Val Acc: 0.5286\n",
      "Epoch 5/10 | Train Loss: 0.6905, Train Acc: 0.5363 | Val Loss: 0.6916, Val Acc: 0.5286\n",
      "Epoch 6/10 | Train Loss: 0.6905, Train Acc: 0.5363 | Val Loss: 0.6917, Val Acc: 0.5286\n",
      "Epoch 7/10 | Train Loss: 0.6905, Train Acc: 0.5363 | Val Loss: 0.6916, Val Acc: 0.5286\n",
      "Epoch 8/10 | Train Loss: 0.6905, Train Acc: 0.5363 | Val Loss: 0.6917, Val Acc: 0.5286\n",
      "Epoch 9/10 | Train Loss: 0.6905, Train Acc: 0.5363 | Val Loss: 0.6916, Val Acc: 0.5286\n",
      "Epoch 10/10 | Train Loss: 0.6905, Train Acc: 0.5363 | Val Loss: 0.6917, Val Acc: 0.5286\n",
      "Saved model for experiment 2.\n",
      "Experiment 2: Final Validation Accuracy: 0.5286\n",
      "Experiment 2: Test Accuracy: 0.5037\n",
      "Loaded model from experiment 2.\n",
      "Experiment 3: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6905, Train Acc: 0.5365 | Val Loss: 0.6960, Val Acc: 0.5037\n",
      "Epoch 2/10 | Train Loss: 0.6905, Train Acc: 0.5365 | Val Loss: 0.6954, Val Acc: 0.5037\n",
      "Epoch 3/10 | Train Loss: 0.6905, Train Acc: 0.5365 | Val Loss: 0.6961, Val Acc: 0.5037\n",
      "Epoch 4/10 | Train Loss: 0.6905, Train Acc: 0.5365 | Val Loss: 0.6948, Val Acc: 0.5037\n",
      "Epoch 5/10 | Train Loss: 0.6905, Train Acc: 0.5365 | Val Loss: 0.6946, Val Acc: 0.5037\n",
      "Epoch 6/10 | Train Loss: 0.6905, Train Acc: 0.5365 | Val Loss: 0.6952, Val Acc: 0.5037\n",
      "Epoch 7/10 | Train Loss: 0.6905, Train Acc: 0.5365 | Val Loss: 0.6952, Val Acc: 0.5037\n",
      "Epoch 8/10 | Train Loss: 0.6905, Train Acc: 0.5365 | Val Loss: 0.6950, Val Acc: 0.5037\n",
      "Epoch 9/10 | Train Loss: 0.6905, Train Acc: 0.5365 | Val Loss: 0.6953, Val Acc: 0.5037\n",
      "Epoch 10/10 | Train Loss: 0.6905, Train Acc: 0.5365 | Val Loss: 0.6952, Val Acc: 0.5037\n",
      "Saved model for experiment 3.\n",
      "Experiment 3: Final Validation Accuracy: 0.5037\n",
      "Experiment 3: Test Accuracy: 0.5033\n",
      "Loaded model from experiment 3.\n",
      "Experiment 4: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6913, Train Acc: 0.5308 | Val Loss: 0.6944, Val Acc: 0.5033\n",
      "Epoch 2/10 | Train Loss: 0.6913, Train Acc: 0.5308 | Val Loss: 0.6943, Val Acc: 0.5033\n",
      "Epoch 3/10 | Train Loss: 0.6913, Train Acc: 0.5308 | Val Loss: 0.6954, Val Acc: 0.5033\n",
      "Epoch 4/10 | Train Loss: 0.6913, Train Acc: 0.5308 | Val Loss: 0.6947, Val Acc: 0.5033\n",
      "Epoch 5/10 | Train Loss: 0.6913, Train Acc: 0.5308 | Val Loss: 0.6951, Val Acc: 0.5033\n",
      "Epoch 6/10 | Train Loss: 0.6913, Train Acc: 0.5308 | Val Loss: 0.6944, Val Acc: 0.5033\n",
      "Epoch 7/10 | Train Loss: 0.6913, Train Acc: 0.5308 | Val Loss: 0.6950, Val Acc: 0.5033\n",
      "Epoch 8/10 | Train Loss: 0.6913, Train Acc: 0.5308 | Val Loss: 0.6944, Val Acc: 0.5033\n",
      "Epoch 9/10 | Train Loss: 0.6913, Train Acc: 0.5308 | Val Loss: 0.6948, Val Acc: 0.5033\n",
      "Epoch 10/10 | Train Loss: 0.6913, Train Acc: 0.5308 | Val Loss: 0.6947, Val Acc: 0.5033\n",
      "Saved model for experiment 4.\n",
      "Experiment 4: Final Validation Accuracy: 0.5033\n",
      "Experiment 4: Test Accuracy: 0.5033\n",
      "Loaded model from experiment 4.\n",
      "Experiment 5: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6920, Train Acc: 0.5237 | Val Loss: 0.6940, Val Acc: 0.5033\n",
      "Epoch 2/10 | Train Loss: 0.6921, Train Acc: 0.5237 | Val Loss: 0.6940, Val Acc: 0.5033\n",
      "Epoch 3/10 | Train Loss: 0.6920, Train Acc: 0.5237 | Val Loss: 0.6943, Val Acc: 0.5033\n",
      "Epoch 4/10 | Train Loss: 0.6920, Train Acc: 0.5237 | Val Loss: 0.6941, Val Acc: 0.5033\n",
      "Epoch 5/10 | Train Loss: 0.6920, Train Acc: 0.5237 | Val Loss: 0.6940, Val Acc: 0.5033\n",
      "Epoch 6/10 | Train Loss: 0.6920, Train Acc: 0.5237 | Val Loss: 0.6941, Val Acc: 0.5033\n",
      "Epoch 7/10 | Train Loss: 0.6920, Train Acc: 0.5237 | Val Loss: 0.6940, Val Acc: 0.5033\n",
      "Epoch 8/10 | Train Loss: 0.6920, Train Acc: 0.5237 | Val Loss: 0.6941, Val Acc: 0.5033\n",
      "Epoch 9/10 | Train Loss: 0.6920, Train Acc: 0.5237 | Val Loss: 0.6940, Val Acc: 0.5033\n",
      "Epoch 10/10 | Train Loss: 0.6920, Train Acc: 0.5237 | Val Loss: 0.6939, Val Acc: 0.5033\n",
      "Saved model for experiment 5.\n",
      "Experiment 5: Final Validation Accuracy: 0.5033\n",
      "Experiment 5: Test Accuracy: 0.5132\n",
      "Loaded model from experiment 5.\n",
      "Experiment 6: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6924, Train Acc: 0.5200 | Val Loss: 0.6929, Val Acc: 0.5132\n",
      "Epoch 2/10 | Train Loss: 0.6924, Train Acc: 0.5200 | Val Loss: 0.6930, Val Acc: 0.5132\n",
      "Epoch 3/10 | Train Loss: 0.6924, Train Acc: 0.5200 | Val Loss: 0.6929, Val Acc: 0.5132\n",
      "Epoch 4/10 | Train Loss: 0.6924, Train Acc: 0.5200 | Val Loss: 0.6931, Val Acc: 0.5132\n",
      "Epoch 5/10 | Train Loss: 0.6924, Train Acc: 0.5200 | Val Loss: 0.6928, Val Acc: 0.5132\n",
      "Epoch 6/10 | Train Loss: 0.6924, Train Acc: 0.5200 | Val Loss: 0.6930, Val Acc: 0.5132\n",
      "Epoch 7/10 | Train Loss: 0.6924, Train Acc: 0.5200 | Val Loss: 0.6929, Val Acc: 0.5132\n",
      "Epoch 8/10 | Train Loss: 0.6924, Train Acc: 0.5200 | Val Loss: 0.6929, Val Acc: 0.5132\n",
      "Epoch 9/10 | Train Loss: 0.6924, Train Acc: 0.5200 | Val Loss: 0.6929, Val Acc: 0.5132\n",
      "Epoch 10/10 | Train Loss: 0.6924, Train Acc: 0.5200 | Val Loss: 0.6929, Val Acc: 0.5132\n",
      "Saved model for experiment 6.\n",
      "Experiment 6: Final Validation Accuracy: 0.5132\n",
      "Experiment 6: Test Accuracy: 0.5067\n",
      "Loaded model from experiment 6.\n",
      "Experiment 7: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6925, Train Acc: 0.5182 | Val Loss: 0.6937, Val Acc: 0.5067\n",
      "Epoch 2/10 | Train Loss: 0.6925, Train Acc: 0.5182 | Val Loss: 0.6932, Val Acc: 0.5067\n",
      "Epoch 3/10 | Train Loss: 0.6925, Train Acc: 0.5182 | Val Loss: 0.6935, Val Acc: 0.5067\n",
      "Epoch 4/10 | Train Loss: 0.6925, Train Acc: 0.5182 | Val Loss: 0.6934, Val Acc: 0.5067\n",
      "Epoch 5/10 | Train Loss: 0.6925, Train Acc: 0.5182 | Val Loss: 0.6934, Val Acc: 0.5067\n",
      "Epoch 6/10 | Train Loss: 0.6925, Train Acc: 0.5182 | Val Loss: 0.6934, Val Acc: 0.5067\n",
      "Epoch 7/10 | Train Loss: 0.6925, Train Acc: 0.5182 | Val Loss: 0.6934, Val Acc: 0.5067\n",
      "Epoch 8/10 | Train Loss: 0.6925, Train Acc: 0.5182 | Val Loss: 0.6933, Val Acc: 0.5067\n",
      "Epoch 9/10 | Train Loss: 0.6925, Train Acc: 0.5182 | Val Loss: 0.6934, Val Acc: 0.5067\n",
      "Epoch 10/10 | Train Loss: 0.6925, Train Acc: 0.5182 | Val Loss: 0.6933, Val Acc: 0.5067\n",
      "Saved model for experiment 7.\n",
      "Experiment 7: Final Validation Accuracy: 0.5067\n",
      "Experiment 7: Test Accuracy: 0.5098\n",
      "Loaded model from experiment 7.\n",
      "Experiment 8: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6928, Train Acc: 0.5141 | Val Loss: 0.6930, Val Acc: 0.5098\n",
      "Epoch 2/10 | Train Loss: 0.6928, Train Acc: 0.5141 | Val Loss: 0.6930, Val Acc: 0.5098\n",
      "Epoch 3/10 | Train Loss: 0.6928, Train Acc: 0.5141 | Val Loss: 0.6931, Val Acc: 0.5098\n",
      "Epoch 4/10 | Train Loss: 0.6928, Train Acc: 0.5141 | Val Loss: 0.6930, Val Acc: 0.5098\n",
      "Epoch 5/10 | Train Loss: 0.6928, Train Acc: 0.5141 | Val Loss: 0.6931, Val Acc: 0.5098\n",
      "Epoch 6/10 | Train Loss: 0.6928, Train Acc: 0.5141 | Val Loss: 0.6931, Val Acc: 0.5098\n",
      "Epoch 7/10 | Train Loss: 0.6928, Train Acc: 0.5141 | Val Loss: 0.6930, Val Acc: 0.5098\n",
      "Epoch 8/10 | Train Loss: 0.6928, Train Acc: 0.5141 | Val Loss: 0.6930, Val Acc: 0.5098\n",
      "Epoch 9/10 | Train Loss: 0.6928, Train Acc: 0.5141 | Val Loss: 0.6930, Val Acc: 0.5098\n",
      "Epoch 10/10 | Train Loss: 0.6928, Train Acc: 0.5141 | Val Loss: 0.6930, Val Acc: 0.5098\n",
      "Saved model for experiment 8.\n",
      "Experiment 8: Final Validation Accuracy: 0.5098\n",
      "Experiment 8: Test Accuracy: 0.5146\n",
      "Loaded model from experiment 8.\n",
      "Experiment 9: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6929, Train Acc: 0.5123 | Val Loss: 0.6927, Val Acc: 0.5146\n",
      "Epoch 2/10 | Train Loss: 0.6929, Train Acc: 0.5123 | Val Loss: 0.6928, Val Acc: 0.5146\n",
      "Epoch 3/10 | Train Loss: 0.6929, Train Acc: 0.5121 | Val Loss: 0.6930, Val Acc: 0.5146\n",
      "Epoch 4/10 | Train Loss: 0.6929, Train Acc: 0.5123 | Val Loss: 0.6927, Val Acc: 0.5146\n",
      "Epoch 5/10 | Train Loss: 0.6929, Train Acc: 0.5123 | Val Loss: 0.6928, Val Acc: 0.5146\n",
      "Epoch 6/10 | Train Loss: 0.6929, Train Acc: 0.5123 | Val Loss: 0.6928, Val Acc: 0.5146\n",
      "Epoch 7/10 | Train Loss: 0.6929, Train Acc: 0.5123 | Val Loss: 0.6928, Val Acc: 0.5146\n",
      "Epoch 8/10 | Train Loss: 0.6929, Train Acc: 0.5123 | Val Loss: 0.6928, Val Acc: 0.5146\n",
      "Epoch 9/10 | Train Loss: 0.6929, Train Acc: 0.5123 | Val Loss: 0.6928, Val Acc: 0.5146\n",
      "Epoch 10/10 | Train Loss: 0.6929, Train Acc: 0.5123 | Val Loss: 0.6928, Val Acc: 0.5146\n",
      "Saved model for experiment 9.\n",
      "Experiment 9: Final Validation Accuracy: 0.5146\n",
      "Experiment 9: Test Accuracy: 0.5180\n",
      "Loaded model from experiment 9.\n",
      "Experiment 10: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6930, Train Acc: 0.5104 | Val Loss: 0.6925, Val Acc: 0.5180\n",
      "Epoch 2/10 | Train Loss: 0.6930, Train Acc: 0.5104 | Val Loss: 0.6929, Val Acc: 0.5180\n",
      "Epoch 3/10 | Train Loss: 0.6930, Train Acc: 0.5104 | Val Loss: 0.6926, Val Acc: 0.5180\n",
      "Epoch 4/10 | Train Loss: 0.6929, Train Acc: 0.5103 | Val Loss: 0.6929, Val Acc: 0.5180\n",
      "Epoch 5/10 | Train Loss: 0.6929, Train Acc: 0.5104 | Val Loss: 0.6926, Val Acc: 0.5180\n",
      "Epoch 6/10 | Train Loss: 0.6929, Train Acc: 0.5104 | Val Loss: 0.6927, Val Acc: 0.5180\n",
      "Epoch 7/10 | Train Loss: 0.6929, Train Acc: 0.5104 | Val Loss: 0.6927, Val Acc: 0.5180\n",
      "Epoch 8/10 | Train Loss: 0.6929, Train Acc: 0.5104 | Val Loss: 0.6926, Val Acc: 0.5180\n",
      "Epoch 9/10 | Train Loss: 0.6929, Train Acc: 0.5104 | Val Loss: 0.6926, Val Acc: 0.5180\n",
      "Epoch 10/10 | Train Loss: 0.6929, Train Acc: 0.5104 | Val Loss: 0.6926, Val Acc: 0.5180\n",
      "Saved model for experiment 10.\n",
      "Experiment 10: Final Validation Accuracy: 0.5180\n",
      "Experiment 10: Test Accuracy: 0.5215\n",
      "Loaded model from experiment 10.\n",
      "Experiment 11: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6930, Train Acc: 0.5091 | Val Loss: 0.6926, Val Acc: 0.5215\n",
      "Epoch 2/10 | Train Loss: 0.6930, Train Acc: 0.5091 | Val Loss: 0.6923, Val Acc: 0.5215\n",
      "Epoch 3/10 | Train Loss: 0.6930, Train Acc: 0.5090 | Val Loss: 0.6925, Val Acc: 0.5215\n",
      "Epoch 4/10 | Train Loss: 0.6930, Train Acc: 0.5089 | Val Loss: 0.6924, Val Acc: 0.5215\n",
      "Epoch 5/10 | Train Loss: 0.6930, Train Acc: 0.5091 | Val Loss: 0.6924, Val Acc: 0.5215\n",
      "Epoch 6/10 | Train Loss: 0.6930, Train Acc: 0.5091 | Val Loss: 0.6925, Val Acc: 0.5215\n",
      "Epoch 7/10 | Train Loss: 0.6930, Train Acc: 0.5091 | Val Loss: 0.6927, Val Acc: 0.5215\n",
      "Epoch 8/10 | Train Loss: 0.6930, Train Acc: 0.5091 | Val Loss: 0.6924, Val Acc: 0.5215\n",
      "Epoch 9/10 | Train Loss: 0.6930, Train Acc: 0.5091 | Val Loss: 0.6926, Val Acc: 0.5215\n",
      "Epoch 10/10 | Train Loss: 0.6930, Train Acc: 0.5091 | Val Loss: 0.6925, Val Acc: 0.5215\n",
      "Saved model for experiment 11.\n",
      "Experiment 11: Final Validation Accuracy: 0.5215\n",
      "Experiment 11: Test Accuracy: 0.5255\n",
      "Loaded model from experiment 11.\n",
      "Experiment 12: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6929, Train Acc: 0.5113 | Val Loss: 0.6923, Val Acc: 0.5255\n",
      "Epoch 2/10 | Train Loss: 0.6929, Train Acc: 0.5113 | Val Loss: 0.6923, Val Acc: 0.5255\n",
      "Epoch 3/10 | Train Loss: 0.6929, Train Acc: 0.5113 | Val Loss: 0.6924, Val Acc: 0.5255\n",
      "Epoch 4/10 | Train Loss: 0.6929, Train Acc: 0.5113 | Val Loss: 0.6923, Val Acc: 0.5255\n",
      "Epoch 5/10 | Train Loss: 0.6929, Train Acc: 0.5113 | Val Loss: 0.6921, Val Acc: 0.5255\n",
      "Epoch 6/10 | Train Loss: 0.6929, Train Acc: 0.5113 | Val Loss: 0.6923, Val Acc: 0.5255\n",
      "Epoch 7/10 | Train Loss: 0.6929, Train Acc: 0.5113 | Val Loss: 0.6921, Val Acc: 0.5255\n",
      "Epoch 8/10 | Train Loss: 0.6929, Train Acc: 0.5113 | Val Loss: 0.6921, Val Acc: 0.5255\n",
      "Epoch 9/10 | Train Loss: 0.6929, Train Acc: 0.5113 | Val Loss: 0.6923, Val Acc: 0.5255\n",
      "Epoch 10/10 | Train Loss: 0.6929, Train Acc: 0.5113 | Val Loss: 0.6923, Val Acc: 0.5255\n",
      "Saved model for experiment 12.\n",
      "Experiment 12: Final Validation Accuracy: 0.5255\n",
      "Experiment 12: Test Accuracy: 0.5103\n",
      "Loaded model from experiment 12.\n",
      "Experiment 13: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6928, Train Acc: 0.5141 | Val Loss: 0.6930, Val Acc: 0.5103\n",
      "Epoch 2/10 | Train Loss: 0.6928, Train Acc: 0.5141 | Val Loss: 0.6930, Val Acc: 0.5103\n",
      "Epoch 3/10 | Train Loss: 0.6928, Train Acc: 0.5141 | Val Loss: 0.6930, Val Acc: 0.5103\n",
      "Epoch 4/10 | Train Loss: 0.6928, Train Acc: 0.5141 | Val Loss: 0.6929, Val Acc: 0.5103\n",
      "Epoch 5/10 | Train Loss: 0.6928, Train Acc: 0.5141 | Val Loss: 0.6930, Val Acc: 0.5103\n",
      "Epoch 6/10 | Train Loss: 0.6928, Train Acc: 0.5141 | Val Loss: 0.6929, Val Acc: 0.5103\n",
      "Epoch 7/10 | Train Loss: 0.6928, Train Acc: 0.5141 | Val Loss: 0.6930, Val Acc: 0.5103\n",
      "Epoch 8/10 | Train Loss: 0.6928, Train Acc: 0.5141 | Val Loss: 0.6930, Val Acc: 0.5103\n",
      "Epoch 9/10 | Train Loss: 0.6928, Train Acc: 0.5141 | Val Loss: 0.6930, Val Acc: 0.5103\n",
      "Epoch 10/10 | Train Loss: 0.6928, Train Acc: 0.5141 | Val Loss: 0.6930, Val Acc: 0.5103\n",
      "Saved model for experiment 13.\n",
      "Experiment 13: Final Validation Accuracy: 0.5103\n",
      "Experiment 13: Test Accuracy: 0.5062\n",
      "Loaded model from experiment 13.\n",
      "Experiment 14: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6927, Train Acc: 0.5150 | Val Loss: 0.6933, Val Acc: 0.5062\n",
      "Epoch 2/10 | Train Loss: 0.6927, Train Acc: 0.5150 | Val Loss: 0.6933, Val Acc: 0.5062\n",
      "Epoch 3/10 | Train Loss: 0.6927, Train Acc: 0.5150 | Val Loss: 0.6932, Val Acc: 0.5062\n",
      "Epoch 4/10 | Train Loss: 0.6927, Train Acc: 0.5150 | Val Loss: 0.6936, Val Acc: 0.5062\n",
      "Epoch 5/10 | Train Loss: 0.6927, Train Acc: 0.5150 | Val Loss: 0.6932, Val Acc: 0.5062\n",
      "Epoch 6/10 | Train Loss: 0.6927, Train Acc: 0.5150 | Val Loss: 0.6931, Val Acc: 0.5062\n",
      "Epoch 7/10 | Train Loss: 0.6927, Train Acc: 0.5150 | Val Loss: 0.6931, Val Acc: 0.5062\n",
      "Epoch 8/10 | Train Loss: 0.6927, Train Acc: 0.5150 | Val Loss: 0.6932, Val Acc: 0.5062\n",
      "Epoch 9/10 | Train Loss: 0.6927, Train Acc: 0.5150 | Val Loss: 0.6933, Val Acc: 0.5062\n",
      "Epoch 10/10 | Train Loss: 0.6927, Train Acc: 0.5150 | Val Loss: 0.6932, Val Acc: 0.5062\n",
      "Saved model for experiment 14.\n",
      "Experiment 14: Final Validation Accuracy: 0.5062\n",
      "Experiment 14: Test Accuracy: 0.5147\n",
      "Loaded model from experiment 14.\n",
      "Experiment 15: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6928, Train Acc: 0.5141 | Val Loss: 0.6927, Val Acc: 0.5147\n",
      "Epoch 2/10 | Train Loss: 0.6928, Train Acc: 0.5141 | Val Loss: 0.6928, Val Acc: 0.5147\n",
      "Epoch 3/10 | Train Loss: 0.6928, Train Acc: 0.5141 | Val Loss: 0.6927, Val Acc: 0.5147\n",
      "Epoch 4/10 | Train Loss: 0.6928, Train Acc: 0.5141 | Val Loss: 0.6928, Val Acc: 0.5147\n",
      "Epoch 5/10 | Train Loss: 0.6928, Train Acc: 0.5141 | Val Loss: 0.6927, Val Acc: 0.5147\n",
      "Epoch 6/10 | Train Loss: 0.6928, Train Acc: 0.5141 | Val Loss: 0.6927, Val Acc: 0.5147\n",
      "Epoch 7/10 | Train Loss: 0.6928, Train Acc: 0.5141 | Val Loss: 0.6927, Val Acc: 0.5147\n",
      "Epoch 8/10 | Train Loss: 0.6928, Train Acc: 0.5141 | Val Loss: 0.6927, Val Acc: 0.5147\n",
      "Epoch 9/10 | Train Loss: 0.6928, Train Acc: 0.5141 | Val Loss: 0.6927, Val Acc: 0.5147\n",
      "Epoch 10/10 | Train Loss: 0.6928, Train Acc: 0.5141 | Val Loss: 0.6927, Val Acc: 0.5147\n",
      "Saved model for experiment 15.\n",
      "Experiment 15: Final Validation Accuracy: 0.5147\n",
      "Experiment 15: Test Accuracy: 0.5043\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta as ta\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from math import sqrt\n",
    "\n",
    "####################################\n",
    "# 1.     ( )\n",
    "####################################\n",
    "def calculate_indicators(data):\n",
    "    data['William_R'] = ta.willr(data['high'], data['low'], data['close'])\n",
    "    data['ATR'] = ta.atr(data['high'], data['low'], data['close'])\n",
    "    data['OBV'] = ta.obv(data['close'], data['volume'])\n",
    "    data['Z_Score'] = (data['close'] - data['close'].rolling(window=20).mean()) / data['close'].rolling(window=20).std()\n",
    "    data['Entropy'] = ta.entropy(data['close'], length=14)\n",
    "    data['SMA_5'] = data['close'].rolling(window=5).mean()\n",
    "    data['SMA_10'] = data['close'].rolling(window=10).mean()\n",
    "    data['SMA_20'] = data['close'].rolling(window=20).mean()\n",
    "    data['SMA_60'] = data['close'].rolling(window=60).mean()\n",
    "    data['SMA_120'] = data['close'].rolling(window=120).mean()\n",
    "    data['SMA_250'] = data['close'].rolling(window=250).mean()\n",
    "    data['RSI'] = ta.rsi(data['close'])\n",
    "    bb = ta.bbands(data['close'])\n",
    "    data['BB_Upper'], data['BB_Middle'], data['BB_Lower'] = bb.iloc[:, 0], bb.iloc[:, 1], bb.iloc[:, 2]\n",
    "    macd = ta.macd(data['close'])\n",
    "    data['MACD'] = macd.iloc[:, 0]\n",
    "    data['Stochastic'] = ta.stoch(data['high'], data['low'], data['close']).iloc[:, 0]\n",
    "    return data.dropna()\n",
    "\n",
    "####################################\n",
    "# 2. Datetime   Positional (Cyclical) \n",
    "####################################\n",
    "def encode_datetime_features_positional(data):\n",
    "    if 'datetime' not in data.columns:\n",
    "        data['datetime'] = pd.to_datetime(data.index)\n",
    "    data['hour_sin'] = np.sin(2 * np.pi * data['datetime'].dt.hour / 24)\n",
    "    data['hour_cos'] = np.cos(2 * np.pi * data['datetime'].dt.hour / 24)\n",
    "    data['day_sin'] = np.sin(2 * np.pi * data['datetime'].dt.dayofweek / 7)\n",
    "    data['day_cos'] = np.cos(2 * np.pi * data['datetime'].dt.dayofweek / 7)\n",
    "    data['week_of_month'] = (data['datetime'].dt.day - 1) // 7 + 1\n",
    "    data['week_sin'] = np.sin(2 * np.pi * data['week_of_month'] / 5)\n",
    "    data['week_cos'] = np.cos(2 * np.pi * data['week_of_month'] / 5)\n",
    "    data['month_sin'] = np.sin(2 * np.pi * data['datetime'].dt.month / 12)\n",
    "    data['month_cos'] = np.cos(2 * np.pi * data['datetime'].dt.month / 12)\n",
    "    return data\n",
    "\n",
    "####################################\n",
    "# 3.  50  rolling minmax scaling ( 0 )\n",
    "####################################\n",
    "def rolling_minmax_scale(series, window=12):\n",
    "    roll_min = series.rolling(window=window, min_periods=window).min()\n",
    "    roll_max = series.rolling(window=window, min_periods=window).max()\n",
    "    scaled = (series - roll_min) / ((roll_max - roll_min) + 1e-8)\n",
    "    return scaled\n",
    "\n",
    "####################################\n",
    "# 4. Binning  One-Hot  ( )\n",
    "####################################\n",
    "def bin_and_encode(data, features, bins=10, drop_original=True):\n",
    "    for feature in features:\n",
    "        data[f'{feature}_Bin'] = pd.cut(data[feature], bins=bins, labels=False)\n",
    "        one_hot = pd.get_dummies(data[f'{feature}_Bin'], prefix=f'{feature}_Bin')\n",
    "        expected_columns = [f'{feature}_Bin_{i}' for i in range(bins)]\n",
    "        one_hot = one_hot.reindex(columns=expected_columns, fill_value=0)\n",
    "        data = pd.concat([data, one_hot], axis=1)\n",
    "        if drop_original:\n",
    "            data.drop(columns=[f'{feature}_Bin'], inplace=True)\n",
    "    data = data.astype(np.float32)\n",
    "    return data\n",
    "\n",
    "####################################\n",
    "# 5.    \n",
    "####################################\n",
    "data = pd.read_csv(\"BTC_upbit_KRW_min5.csv\", index_col=0)\n",
    "data.columns = ['open', 'high', 'low', 'close', 'volume', 'value']\n",
    "data.index = pd.to_datetime(data.index)\n",
    "data = calculate_indicators(data)\n",
    "data = encode_datetime_features_positional(data)\n",
    "\n",
    "features_to_bin = ['open', 'high', 'low', 'volume', 'value', 'William_R',\n",
    "                   'ATR', 'OBV', 'Z_Score', 'Entropy', 'SMA_5', 'SMA_10', \n",
    "                   'SMA_20', 'SMA_60', 'SMA_120', 'SMA_250', 'RSI', 'BB_Upper', 'BB_Middle', \n",
    "                   'BB_Lower', 'MACD', 'Stochastic']\n",
    "datetime_features = ['hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'week_sin', 'week_cos', 'month_sin', 'month_cos']\n",
    "\n",
    "# close_target  close \n",
    "data['close_target'] = data['close']\n",
    "\n",
    "selected_features = features_to_bin + ['close_target'] + datetime_features\n",
    "data = data[selected_features].dropna()\n",
    "\n",
    "# rolling scaling  (window=512:   )\n",
    "for feature in features_to_bin:\n",
    "    data[feature] = rolling_minmax_scale(data[feature], window=32)\n",
    "data = data.dropna()\n",
    "\n",
    "data = bin_and_encode(data, features_to_bin, bins=10, drop_original=True)\n",
    "data['close_for_binning'] = data['close_target']\n",
    "data = bin_and_encode(data, ['close_for_binning'], bins=10, drop_original=False)\n",
    "data.drop(columns=['close_for_binning'], inplace=True)\n",
    "\n",
    "final_columns = []\n",
    "for feature in features_to_bin:\n",
    "    final_columns.extend([f'{feature}_Bin_{i}' for i in range(10)])\n",
    "final_columns.append('close_target')\n",
    "final_columns.extend([f'close_for_binning_Bin_{i}' for i in range(10)])\n",
    "final_columns.extend(datetime_features)\n",
    "data = data[final_columns]\n",
    "\n",
    "####################################\n",
    "# 6. Dataset \n",
    "####################################\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, lookback=32, target_col='close_target'):\n",
    "        self.data = data.values\n",
    "        self.lookback = lookback\n",
    "        self.target_idx = list(data.columns).index(target_col)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.lookback\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.lookback, :]\n",
    "        y = self.data[idx + self.lookback, self.target_idx]\n",
    "        y_prev = self.data[idx + self.lookback - 1, self.target_idx]\n",
    "        y_target = 1 if y > y_prev else 0\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y_target, dtype=torch.long)\n",
    "\n",
    "####################################\n",
    "# 7. Transformer Encoder  \n",
    "####################################\n",
    "# 7-1. Multi-Head Self-Attention\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == embed_dim, \"embed_dim num_heads  .\"\n",
    "        \n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key   = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out   = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        \n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / sqrt(self.head_dim)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, V)\n",
    "        \n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "\n",
    "# 7-2. Feed-Forward Network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ffn_dim):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, ffn_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(ffn_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "# 7-3. Transformer Encoder Layer (Self-Attention + FFN + Residual + LayerNorm)\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = FeedForward(embed_dim, ffn_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attn_out = self.self_attn(x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_out)\n",
    "        return x\n",
    "\n",
    "# 7-4. Encoder-Only Transformer  \n",
    "class EncoderOnlyTransformerCustom(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=512, num_heads=8, num_layers=6, ffn_dim=1024, num_classes=2, max_seq_len=32):\n",
    "        super(EncoderOnlyTransformerCustom, self).__init__()\n",
    "        self.token_embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embedding_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(embedding_dim, num_heads, ffn_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x = self.token_embedding(x)\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        x = x + self.position_embedding(positions)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x[:, -1, :]\n",
    "        return self.fc(x)\n",
    "\n",
    "####################################\n",
    "# 8.     (Fine-tuning  Validation Accuracy )\n",
    "####################################\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return total_loss / len(data_loader), correct / total\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, lr, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_state = None\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, device)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = model.state_dict()\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(data, num_experiments=16, lookback=32, num_epochs=10):\n",
    "    input_dim = data.shape[1]\n",
    "    step_size = 30000  #  \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    for exp in range(num_experiments):\n",
    "        train_start = exp * step_size\n",
    "        train_end = train_start + step_size * 8   #  \n",
    "        val_end = train_end + step_size            #  \n",
    "        test_end = val_end + step_size             #  \n",
    "        if test_end > len(data):\n",
    "            break\n",
    "        \n",
    "        train_data = data.iloc[train_start:train_end]\n",
    "        val_data = data.iloc[train_end:val_end]\n",
    "        test_data = data.iloc[val_end:test_end]\n",
    "        \n",
    "        train_dataset = TimeSeriesDataset(train_data, lookback=lookback, target_col='close_target')\n",
    "        val_dataset = TimeSeriesDataset(val_data, lookback=lookback, target_col='close_target')\n",
    "        test_dataset = TimeSeriesDataset(test_data, lookback=lookback, target_col='close_target')\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        # Fine-tuning:    lr,    lr \n",
    "        if exp == 0:\n",
    "            lr = 1e-4\n",
    "        else:\n",
    "            lr = 1e-5\n",
    "        model = EncoderOnlyTransformerCustom(input_dim=input_dim, embedding_dim=512, num_heads=8, \n",
    "                                               num_layers=6, ffn_dim=1024, num_classes=2, max_seq_len=32).to(device)\n",
    "        model_path = f\"model_experiment_{exp}.pth\"\n",
    "        if exp > 0:\n",
    "            try:\n",
    "                model.load_state_dict(torch.load(f\"model_experiment_{exp - 1}.pth\"))\n",
    "                print(f\"Loaded model from experiment {exp - 1}.\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Model file for experiment {exp - 1} not found. Starting fresh training.\")\n",
    "        \n",
    "        print(f\"Experiment {exp}: Training with lr={lr}\")\n",
    "        model = train_model(model, train_loader, val_loader, num_epochs, lr, device)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Saved model for experiment {exp}.\")\n",
    "        \n",
    "        #    \n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, device)\n",
    "        print(f\"Experiment {exp}: Final Validation Accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "        #   \n",
    "        test_loss, test_acc = evaluate_model(model, test_loader, device)\n",
    "        print(f\"Experiment {exp}: Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "train_and_evaluate(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 0: Training with lr=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.6920, Train Acc: 0.5437 | Val Loss: 0.6971, Val Acc: 0.5321\n",
      "Epoch 2/10 | Train Loss: 0.6886, Train Acc: 0.5507 | Val Loss: 0.6933, Val Acc: 0.5321\n",
      "Epoch 3/10 | Train Loss: 0.6882, Train Acc: 0.5507 | Val Loss: 0.6911, Val Acc: 0.5321\n",
      "Epoch 4/10 | Train Loss: 0.6883, Train Acc: 0.5507 | Val Loss: 0.6918, Val Acc: 0.5321\n",
      "Epoch 5/10 | Train Loss: 0.6882, Train Acc: 0.5507 | Val Loss: 0.6911, Val Acc: 0.5321\n",
      "Epoch 6/10 | Train Loss: 0.6883, Train Acc: 0.5507 | Val Loss: 0.6924, Val Acc: 0.5321\n",
      "Epoch 7/10 | Train Loss: 0.6881, Train Acc: 0.5507 | Val Loss: 0.6915, Val Acc: 0.5321\n",
      "Epoch 8/10 | Train Loss: 0.6881, Train Acc: 0.5507 | Val Loss: 0.6929, Val Acc: 0.5321\n",
      "Epoch 9/10 | Train Loss: 0.6881, Train Acc: 0.5507 | Val Loss: 0.6914, Val Acc: 0.5321\n",
      "Epoch 10/10 | Train Loss: 0.6881, Train Acc: 0.5507 | Val Loss: 0.6919, Val Acc: 0.5321\n",
      "Saved model for experiment 0.\n",
      "Experiment 0: Final Validation Accuracy: 0.5321\n",
      "Experiment 0: Test Accuracy: 0.5439\n",
      "Loaded model from experiment 0.\n",
      "Experiment 1: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6884, Train Acc: 0.5489 | Val Loss: 0.6894, Val Acc: 0.5439\n",
      "Epoch 2/10 | Train Loss: 0.6884, Train Acc: 0.5489 | Val Loss: 0.6895, Val Acc: 0.5439\n",
      "Epoch 3/10 | Train Loss: 0.6884, Train Acc: 0.5489 | Val Loss: 0.6894, Val Acc: 0.5439\n",
      "Epoch 4/10 | Train Loss: 0.6884, Train Acc: 0.5489 | Val Loss: 0.6893, Val Acc: 0.5439\n",
      "Epoch 5/10 | Train Loss: 0.6884, Train Acc: 0.5489 | Val Loss: 0.6894, Val Acc: 0.5439\n",
      "Epoch 6/10 | Train Loss: 0.6884, Train Acc: 0.5489 | Val Loss: 0.6894, Val Acc: 0.5439\n",
      "Epoch 7/10 | Train Loss: 0.6884, Train Acc: 0.5489 | Val Loss: 0.6893, Val Acc: 0.5439\n",
      "Epoch 8/10 | Train Loss: 0.6884, Train Acc: 0.5489 | Val Loss: 0.6894, Val Acc: 0.5439\n",
      "Epoch 9/10 | Train Loss: 0.6884, Train Acc: 0.5489 | Val Loss: 0.6894, Val Acc: 0.5439\n",
      "Epoch 10/10 | Train Loss: 0.6884, Train Acc: 0.5489 | Val Loss: 0.6894, Val Acc: 0.5439\n",
      "Saved model for experiment 1.\n",
      "Experiment 1: Final Validation Accuracy: 0.5439\n",
      "Experiment 1: Test Accuracy: 0.5307\n",
      "Loaded model from experiment 1.\n",
      "Experiment 2: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6879, Train Acc: 0.5512 | Val Loss: 0.6924, Val Acc: 0.5307\n",
      "Epoch 2/10 | Train Loss: 0.6879, Train Acc: 0.5512 | Val Loss: 0.6923, Val Acc: 0.5307\n",
      "Epoch 3/10 | Train Loss: 0.6879, Train Acc: 0.5512 | Val Loss: 0.6920, Val Acc: 0.5307\n",
      "Epoch 4/10 | Train Loss: 0.6879, Train Acc: 0.5512 | Val Loss: 0.6920, Val Acc: 0.5307\n",
      "Epoch 5/10 | Train Loss: 0.6879, Train Acc: 0.5512 | Val Loss: 0.6922, Val Acc: 0.5307\n",
      "Epoch 6/10 | Train Loss: 0.6879, Train Acc: 0.5512 | Val Loss: 0.6921, Val Acc: 0.5307\n",
      "Epoch 7/10 | Train Loss: 0.6879, Train Acc: 0.5512 | Val Loss: 0.6921, Val Acc: 0.5307\n",
      "Epoch 8/10 | Train Loss: 0.6879, Train Acc: 0.5512 | Val Loss: 0.6924, Val Acc: 0.5307\n",
      "Epoch 9/10 | Train Loss: 0.6879, Train Acc: 0.5512 | Val Loss: 0.6924, Val Acc: 0.5307\n",
      "Epoch 10/10 | Train Loss: 0.6879, Train Acc: 0.5512 | Val Loss: 0.6922, Val Acc: 0.5307\n",
      "Saved model for experiment 2.\n",
      "Experiment 2: Final Validation Accuracy: 0.5307\n",
      "Experiment 2: Test Accuracy: 0.5208\n",
      "Loaded model from experiment 2.\n",
      "Experiment 3: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6883, Train Acc: 0.5492 | Val Loss: 0.6940, Val Acc: 0.5208\n",
      "Epoch 2/10 | Train Loss: 0.6883, Train Acc: 0.5492 | Val Loss: 0.6943, Val Acc: 0.5208\n",
      "Epoch 3/10 | Train Loss: 0.6883, Train Acc: 0.5492 | Val Loss: 0.6944, Val Acc: 0.5208\n",
      "Epoch 4/10 | Train Loss: 0.6883, Train Acc: 0.5492 | Val Loss: 0.6939, Val Acc: 0.5208\n",
      "Epoch 5/10 | Train Loss: 0.6883, Train Acc: 0.5492 | Val Loss: 0.6939, Val Acc: 0.5208\n",
      "Epoch 6/10 | Train Loss: 0.6883, Train Acc: 0.5492 | Val Loss: 0.6950, Val Acc: 0.5208\n",
      "Epoch 7/10 | Train Loss: 0.6883, Train Acc: 0.5492 | Val Loss: 0.6933, Val Acc: 0.5208\n",
      "Epoch 8/10 | Train Loss: 0.6883, Train Acc: 0.5492 | Val Loss: 0.6938, Val Acc: 0.5208\n",
      "Epoch 9/10 | Train Loss: 0.6883, Train Acc: 0.5492 | Val Loss: 0.6938, Val Acc: 0.5208\n",
      "Epoch 10/10 | Train Loss: 0.6883, Train Acc: 0.5492 | Val Loss: 0.6939, Val Acc: 0.5208\n",
      "Saved model for experiment 3.\n",
      "Experiment 3: Final Validation Accuracy: 0.5208\n",
      "Experiment 3: Test Accuracy: 0.5330\n",
      "Loaded model from experiment 3.\n",
      "Experiment 4: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6892, Train Acc: 0.5447 | Val Loss: 0.6913, Val Acc: 0.5330\n",
      "Epoch 2/10 | Train Loss: 0.6892, Train Acc: 0.5447 | Val Loss: 0.6912, Val Acc: 0.5330\n",
      "Epoch 3/10 | Train Loss: 0.6892, Train Acc: 0.5447 | Val Loss: 0.6911, Val Acc: 0.5330\n",
      "Epoch 4/10 | Train Loss: 0.6892, Train Acc: 0.5447 | Val Loss: 0.6914, Val Acc: 0.5330\n",
      "Epoch 5/10 | Train Loss: 0.6892, Train Acc: 0.5447 | Val Loss: 0.6912, Val Acc: 0.5330\n",
      "Epoch 6/10 | Train Loss: 0.6892, Train Acc: 0.5447 | Val Loss: 0.6915, Val Acc: 0.5330\n",
      "Epoch 7/10 | Train Loss: 0.6892, Train Acc: 0.5447 | Val Loss: 0.6913, Val Acc: 0.5330\n",
      "Epoch 8/10 | Train Loss: 0.6891, Train Acc: 0.5447 | Val Loss: 0.6913, Val Acc: 0.5330\n",
      "Epoch 9/10 | Train Loss: 0.6892, Train Acc: 0.5447 | Val Loss: 0.6913, Val Acc: 0.5330\n",
      "Epoch 10/10 | Train Loss: 0.6891, Train Acc: 0.5447 | Val Loss: 0.6912, Val Acc: 0.5330\n",
      "Saved model for experiment 4.\n",
      "Experiment 4: Final Validation Accuracy: 0.5330\n",
      "Experiment 4: Test Accuracy: 0.5315\n",
      "Loaded model from experiment 4.\n",
      "Experiment 5: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6898, Train Acc: 0.5409 | Val Loss: 0.6912, Val Acc: 0.5315\n",
      "Epoch 2/10 | Train Loss: 0.6898, Train Acc: 0.5409 | Val Loss: 0.6914, Val Acc: 0.5315\n",
      "Epoch 3/10 | Train Loss: 0.6898, Train Acc: 0.5409 | Val Loss: 0.6915, Val Acc: 0.5315\n",
      "Epoch 4/10 | Train Loss: 0.6898, Train Acc: 0.5409 | Val Loss: 0.6914, Val Acc: 0.5315\n",
      "Epoch 5/10 | Train Loss: 0.6898, Train Acc: 0.5409 | Val Loss: 0.6914, Val Acc: 0.5315\n",
      "Epoch 6/10 | Train Loss: 0.6898, Train Acc: 0.5409 | Val Loss: 0.6914, Val Acc: 0.5315\n",
      "Epoch 7/10 | Train Loss: 0.6898, Train Acc: 0.5409 | Val Loss: 0.6913, Val Acc: 0.5315\n",
      "Epoch 8/10 | Train Loss: 0.6898, Train Acc: 0.5409 | Val Loss: 0.6913, Val Acc: 0.5315\n",
      "Epoch 9/10 | Train Loss: 0.6898, Train Acc: 0.5409 | Val Loss: 0.6914, Val Acc: 0.5315\n",
      "Epoch 10/10 | Train Loss: 0.6898, Train Acc: 0.5409 | Val Loss: 0.6914, Val Acc: 0.5315\n",
      "Saved model for experiment 5.\n",
      "Experiment 5: Final Validation Accuracy: 0.5315\n",
      "Experiment 5: Test Accuracy: 0.5355\n",
      "Loaded model from experiment 5.\n",
      "Experiment 6: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6901, Train Acc: 0.5390 | Val Loss: 0.6907, Val Acc: 0.5355\n",
      "Epoch 2/10 | Train Loss: 0.6901, Train Acc: 0.5390 | Val Loss: 0.6907, Val Acc: 0.5355\n",
      "Epoch 3/10 | Train Loss: 0.6901, Train Acc: 0.5390 | Val Loss: 0.6906, Val Acc: 0.5355\n",
      "Epoch 4/10 | Train Loss: 0.6901, Train Acc: 0.5390 | Val Loss: 0.6907, Val Acc: 0.5355\n",
      "Epoch 5/10 | Train Loss: 0.6901, Train Acc: 0.5390 | Val Loss: 0.6906, Val Acc: 0.5355\n",
      "Epoch 6/10 | Train Loss: 0.6901, Train Acc: 0.5390 | Val Loss: 0.6907, Val Acc: 0.5355\n",
      "Epoch 7/10 | Train Loss: 0.6901, Train Acc: 0.5390 | Val Loss: 0.6907, Val Acc: 0.5355\n",
      "Epoch 8/10 | Train Loss: 0.6901, Train Acc: 0.5390 | Val Loss: 0.6907, Val Acc: 0.5355\n",
      "Epoch 9/10 | Train Loss: 0.6901, Train Acc: 0.5390 | Val Loss: 0.6906, Val Acc: 0.5355\n",
      "Epoch 10/10 | Train Loss: 0.6901, Train Acc: 0.5390 | Val Loss: 0.6906, Val Acc: 0.5355\n",
      "Saved model for experiment 6.\n",
      "Experiment 6: Final Validation Accuracy: 0.5355\n",
      "Experiment 6: Test Accuracy: 0.5415\n",
      "Loaded model from experiment 6.\n",
      "Experiment 7: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6904, Train Acc: 0.5374 | Val Loss: 0.6897, Val Acc: 0.5415\n",
      "Epoch 2/10 | Train Loss: 0.6904, Train Acc: 0.5374 | Val Loss: 0.6899, Val Acc: 0.5415\n",
      "Epoch 3/10 | Train Loss: 0.6904, Train Acc: 0.5374 | Val Loss: 0.6897, Val Acc: 0.5415\n",
      "Epoch 4/10 | Train Loss: 0.6904, Train Acc: 0.5374 | Val Loss: 0.6898, Val Acc: 0.5415\n",
      "Epoch 5/10 | Train Loss: 0.6904, Train Acc: 0.5374 | Val Loss: 0.6897, Val Acc: 0.5415\n",
      "Epoch 6/10 | Train Loss: 0.6904, Train Acc: 0.5374 | Val Loss: 0.6898, Val Acc: 0.5415\n",
      "Epoch 7/10 | Train Loss: 0.6904, Train Acc: 0.5374 | Val Loss: 0.6898, Val Acc: 0.5415\n",
      "Epoch 8/10 | Train Loss: 0.6904, Train Acc: 0.5374 | Val Loss: 0.6897, Val Acc: 0.5415\n",
      "Epoch 9/10 | Train Loss: 0.6904, Train Acc: 0.5374 | Val Loss: 0.6897, Val Acc: 0.5415\n",
      "Epoch 10/10 | Train Loss: 0.6904, Train Acc: 0.5374 | Val Loss: 0.6898, Val Acc: 0.5415\n",
      "Saved model for experiment 7.\n",
      "Experiment 7: Final Validation Accuracy: 0.5415\n",
      "Experiment 7: Test Accuracy: 0.5469\n",
      "Loaded model from experiment 7.\n",
      "Experiment 8: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6909, Train Acc: 0.5336 | Val Loss: 0.6892, Val Acc: 0.5469\n",
      "Epoch 2/10 | Train Loss: 0.6909, Train Acc: 0.5336 | Val Loss: 0.6889, Val Acc: 0.5469\n",
      "Epoch 3/10 | Train Loss: 0.6909, Train Acc: 0.5336 | Val Loss: 0.6890, Val Acc: 0.5469\n",
      "Epoch 4/10 | Train Loss: 0.6909, Train Acc: 0.5336 | Val Loss: 0.6891, Val Acc: 0.5469\n",
      "Epoch 5/10 | Train Loss: 0.6909, Train Acc: 0.5336 | Val Loss: 0.6890, Val Acc: 0.5469\n",
      "Epoch 6/10 | Train Loss: 0.6909, Train Acc: 0.5336 | Val Loss: 0.6890, Val Acc: 0.5469\n",
      "Epoch 7/10 | Train Loss: 0.6909, Train Acc: 0.5336 | Val Loss: 0.6891, Val Acc: 0.5469\n",
      "Epoch 8/10 | Train Loss: 0.6909, Train Acc: 0.5336 | Val Loss: 0.6892, Val Acc: 0.5469\n",
      "Epoch 9/10 | Train Loss: 0.6909, Train Acc: 0.5336 | Val Loss: 0.6891, Val Acc: 0.5469\n",
      "Epoch 10/10 | Train Loss: 0.6909, Train Acc: 0.5336 | Val Loss: 0.6890, Val Acc: 0.5469\n",
      "Saved model for experiment 8.\n",
      "Experiment 8: Final Validation Accuracy: 0.5469\n",
      "Experiment 8: Test Accuracy: 0.5585\n",
      "Loaded model from experiment 8.\n",
      "Experiment 9: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6907, Train Acc: 0.5355 | Val Loss: 0.6872, Val Acc: 0.5585\n",
      "Epoch 2/10 | Train Loss: 0.6906, Train Acc: 0.5355 | Val Loss: 0.6875, Val Acc: 0.5585\n",
      "Epoch 3/10 | Train Loss: 0.6907, Train Acc: 0.5355 | Val Loss: 0.6873, Val Acc: 0.5585\n",
      "Epoch 4/10 | Train Loss: 0.6906, Train Acc: 0.5355 | Val Loss: 0.6867, Val Acc: 0.5585\n",
      "Epoch 5/10 | Train Loss: 0.6906, Train Acc: 0.5355 | Val Loss: 0.6875, Val Acc: 0.5585\n",
      "Epoch 6/10 | Train Loss: 0.6907, Train Acc: 0.5355 | Val Loss: 0.6875, Val Acc: 0.5585\n",
      "Epoch 7/10 | Train Loss: 0.6906, Train Acc: 0.5355 | Val Loss: 0.6874, Val Acc: 0.5585\n",
      "Epoch 8/10 | Train Loss: 0.6906, Train Acc: 0.5355 | Val Loss: 0.6873, Val Acc: 0.5585\n",
      "Epoch 9/10 | Train Loss: 0.6906, Train Acc: 0.5355 | Val Loss: 0.6873, Val Acc: 0.5585\n",
      "Epoch 10/10 | Train Loss: 0.6906, Train Acc: 0.5355 | Val Loss: 0.6873, Val Acc: 0.5585\n",
      "Saved model for experiment 9.\n",
      "Experiment 9: Final Validation Accuracy: 0.5585\n",
      "Experiment 9: Test Accuracy: 0.5850\n",
      "Loaded model from experiment 9.\n",
      "Experiment 10: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6904, Train Acc: 0.5373 | Val Loss: 0.6826, Val Acc: 0.5850\n",
      "Epoch 2/10 | Train Loss: 0.6904, Train Acc: 0.5373 | Val Loss: 0.6829, Val Acc: 0.5850\n",
      "Epoch 3/10 | Train Loss: 0.6904, Train Acc: 0.5373 | Val Loss: 0.6825, Val Acc: 0.5850\n",
      "Epoch 4/10 | Train Loss: 0.6904, Train Acc: 0.5373 | Val Loss: 0.6829, Val Acc: 0.5850\n",
      "Epoch 5/10 | Train Loss: 0.6904, Train Acc: 0.5373 | Val Loss: 0.6830, Val Acc: 0.5850\n",
      "Epoch 6/10 | Train Loss: 0.6904, Train Acc: 0.5373 | Val Loss: 0.6835, Val Acc: 0.5850\n",
      "Epoch 7/10 | Train Loss: 0.6904, Train Acc: 0.5373 | Val Loss: 0.6835, Val Acc: 0.5850\n",
      "Epoch 8/10 | Train Loss: 0.6904, Train Acc: 0.5373 | Val Loss: 0.6827, Val Acc: 0.5850\n",
      "Epoch 9/10 | Train Loss: 0.6904, Train Acc: 0.5373 | Val Loss: 0.6829, Val Acc: 0.5850\n",
      "Epoch 10/10 | Train Loss: 0.6904, Train Acc: 0.5373 | Val Loss: 0.6832, Val Acc: 0.5850\n",
      "Saved model for experiment 10.\n",
      "Experiment 10: Final Validation Accuracy: 0.5850\n",
      "Experiment 10: Test Accuracy: 0.5994\n",
      "Loaded model from experiment 10.\n",
      "Experiment 11: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6893, Train Acc: 0.5440 | Val Loss: 0.6797, Val Acc: 0.5994\n",
      "Epoch 2/10 | Train Loss: 0.6893, Train Acc: 0.5440 | Val Loss: 0.6784, Val Acc: 0.5994\n",
      "Epoch 3/10 | Train Loss: 0.6893, Train Acc: 0.5440 | Val Loss: 0.6798, Val Acc: 0.5994\n",
      "Epoch 4/10 | Train Loss: 0.6893, Train Acc: 0.5440 | Val Loss: 0.6781, Val Acc: 0.5994\n",
      "Epoch 5/10 | Train Loss: 0.6893, Train Acc: 0.5440 | Val Loss: 0.6792, Val Acc: 0.5994\n",
      "Epoch 6/10 | Train Loss: 0.6893, Train Acc: 0.5440 | Val Loss: 0.6797, Val Acc: 0.5994\n",
      "Epoch 7/10 | Train Loss: 0.6893, Train Acc: 0.5440 | Val Loss: 0.6804, Val Acc: 0.5994\n",
      "Epoch 8/10 | Train Loss: 0.6893, Train Acc: 0.5440 | Val Loss: 0.6788, Val Acc: 0.5994\n",
      "Epoch 9/10 | Train Loss: 0.6893, Train Acc: 0.5440 | Val Loss: 0.6794, Val Acc: 0.5994\n",
      "Epoch 10/10 | Train Loss: 0.6893, Train Acc: 0.5440 | Val Loss: 0.6799, Val Acc: 0.5994\n",
      "Saved model for experiment 11.\n",
      "Experiment 11: Final Validation Accuracy: 0.5994\n",
      "Experiment 11: Test Accuracy: 0.6288\n",
      "Loaded model from experiment 11.\n",
      "Experiment 12: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6874, Train Acc: 0.5539 | Val Loss: 0.6700, Val Acc: 0.6288\n",
      "Epoch 2/10 | Train Loss: 0.6874, Train Acc: 0.5539 | Val Loss: 0.6713, Val Acc: 0.6288\n",
      "Epoch 3/10 | Train Loss: 0.6874, Train Acc: 0.5539 | Val Loss: 0.6708, Val Acc: 0.6288\n",
      "Epoch 4/10 | Train Loss: 0.6873, Train Acc: 0.5539 | Val Loss: 0.6709, Val Acc: 0.6288\n",
      "Epoch 5/10 | Train Loss: 0.6873, Train Acc: 0.5539 | Val Loss: 0.6719, Val Acc: 0.6288\n",
      "Epoch 6/10 | Train Loss: 0.6873, Train Acc: 0.5539 | Val Loss: 0.6713, Val Acc: 0.6288\n",
      "Epoch 7/10 | Train Loss: 0.6873, Train Acc: 0.5539 | Val Loss: 0.6716, Val Acc: 0.6288\n",
      "Epoch 8/10 | Train Loss: 0.6873, Train Acc: 0.5539 | Val Loss: 0.6710, Val Acc: 0.6288\n",
      "Epoch 9/10 | Train Loss: 0.6873, Train Acc: 0.5539 | Val Loss: 0.6704, Val Acc: 0.6288\n",
      "Epoch 10/10 | Train Loss: 0.6873, Train Acc: 0.5539 | Val Loss: 0.6707, Val Acc: 0.6288\n",
      "Saved model for experiment 12.\n",
      "Experiment 12: Final Validation Accuracy: 0.6288\n",
      "Experiment 12: Test Accuracy: 0.5711\n",
      "Loaded model from experiment 12.\n",
      "Experiment 13: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6845, Train Acc: 0.5659 | Val Loss: 0.6831, Val Acc: 0.5711\n",
      "Epoch 2/10 | Train Loss: 0.6845, Train Acc: 0.5659 | Val Loss: 0.6830, Val Acc: 0.5711\n",
      "Epoch 3/10 | Train Loss: 0.6845, Train Acc: 0.5659 | Val Loss: 0.6830, Val Acc: 0.5711\n",
      "Epoch 4/10 | Train Loss: 0.6845, Train Acc: 0.5659 | Val Loss: 0.6830, Val Acc: 0.5711\n",
      "Epoch 5/10 | Train Loss: 0.6845, Train Acc: 0.5659 | Val Loss: 0.6830, Val Acc: 0.5711\n",
      "Epoch 6/10 | Train Loss: 0.6845, Train Acc: 0.5659 | Val Loss: 0.6830, Val Acc: 0.5711\n",
      "Epoch 7/10 | Train Loss: 0.6845, Train Acc: 0.5659 | Val Loss: 0.6831, Val Acc: 0.5711\n",
      "Epoch 8/10 | Train Loss: 0.6845, Train Acc: 0.5659 | Val Loss: 0.6830, Val Acc: 0.5711\n",
      "Epoch 9/10 | Train Loss: 0.6844, Train Acc: 0.5659 | Val Loss: 0.6830, Val Acc: 0.5711\n",
      "Epoch 10/10 | Train Loss: 0.6845, Train Acc: 0.5659 | Val Loss: 0.6830, Val Acc: 0.5711\n",
      "Saved model for experiment 13.\n",
      "Experiment 13: Final Validation Accuracy: 0.5711\n",
      "Experiment 13: Test Accuracy: 0.5395\n",
      "Loaded model from experiment 13.\n",
      "Experiment 14: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6831, Train Acc: 0.5708 | Val Loss: 0.6914, Val Acc: 0.5395\n",
      "Epoch 2/10 | Train Loss: 0.6831, Train Acc: 0.5708 | Val Loss: 0.6919, Val Acc: 0.5395\n",
      "Epoch 3/10 | Train Loss: 0.6831, Train Acc: 0.5708 | Val Loss: 0.6919, Val Acc: 0.5395\n",
      "Epoch 4/10 | Train Loss: 0.6831, Train Acc: 0.5708 | Val Loss: 0.6923, Val Acc: 0.5395\n",
      "Epoch 5/10 | Train Loss: 0.6831, Train Acc: 0.5708 | Val Loss: 0.6919, Val Acc: 0.5395\n",
      "Epoch 6/10 | Train Loss: 0.6831, Train Acc: 0.5708 | Val Loss: 0.6924, Val Acc: 0.5395\n",
      "Epoch 7/10 | Train Loss: 0.6831, Train Acc: 0.5708 | Val Loss: 0.6920, Val Acc: 0.5395\n",
      "Epoch 8/10 | Train Loss: 0.6831, Train Acc: 0.5708 | Val Loss: 0.6922, Val Acc: 0.5395\n",
      "Epoch 9/10 | Train Loss: 0.6831, Train Acc: 0.5708 | Val Loss: 0.6921, Val Acc: 0.5395\n",
      "Epoch 10/10 | Train Loss: 0.6831, Train Acc: 0.5708 | Val Loss: 0.6921, Val Acc: 0.5395\n",
      "Saved model for experiment 14.\n",
      "Experiment 14: Final Validation Accuracy: 0.5395\n",
      "Experiment 14: Test Accuracy: 0.5442\n",
      "Loaded model from experiment 14.\n",
      "Experiment 15: Training with lr=1e-05\n",
      "Epoch 1/10 | Train Loss: 0.6830, Train Acc: 0.5714 | Val Loss: 0.6910, Val Acc: 0.5442\n",
      "Epoch 2/10 | Train Loss: 0.6830, Train Acc: 0.5714 | Val Loss: 0.6906, Val Acc: 0.5442\n",
      "Epoch 3/10 | Train Loss: 0.6830, Train Acc: 0.5714 | Val Loss: 0.6907, Val Acc: 0.5442\n",
      "Epoch 4/10 | Train Loss: 0.6830, Train Acc: 0.5714 | Val Loss: 0.6909, Val Acc: 0.5442\n",
      "Epoch 5/10 | Train Loss: 0.6829, Train Acc: 0.5714 | Val Loss: 0.6907, Val Acc: 0.5442\n",
      "Epoch 6/10 | Train Loss: 0.6829, Train Acc: 0.5714 | Val Loss: 0.6904, Val Acc: 0.5442\n",
      "Epoch 7/10 | Train Loss: 0.6829, Train Acc: 0.5714 | Val Loss: 0.6912, Val Acc: 0.5442\n",
      "Epoch 8/10 | Train Loss: 0.6829, Train Acc: 0.5714 | Val Loss: 0.6906, Val Acc: 0.5442\n",
      "Epoch 9/10 | Train Loss: 0.6829, Train Acc: 0.5714 | Val Loss: 0.6908, Val Acc: 0.5442\n",
      "Epoch 10/10 | Train Loss: 0.6829, Train Acc: 0.5714 | Val Loss: 0.6906, Val Acc: 0.5442\n",
      "Saved model for experiment 15.\n",
      "Experiment 15: Final Validation Accuracy: 0.5442\n",
      "Experiment 15: Test Accuracy: 0.5485\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta as ta\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from math import sqrt\n",
    "\n",
    "####################################\n",
    "# 1.     ( )\n",
    "####################################\n",
    "def calculate_indicators(data):\n",
    "    data['William_R'] = ta.willr(data['high'], data['low'], data['close'])\n",
    "    data['ATR'] = ta.atr(data['high'], data['low'], data['close'])\n",
    "    data['OBV'] = ta.obv(data['close'], data['volume'])\n",
    "    data['Z_Score'] = (data['close'] - data['close'].rolling(window=20).mean()) / data['close'].rolling(window=20).std()\n",
    "    data['Entropy'] = ta.entropy(data['close'], length=14)\n",
    "    data['SMA_5'] = data['close'].rolling(window=5).mean()\n",
    "    data['SMA_10'] = data['close'].rolling(window=10).mean()\n",
    "    data['SMA_20'] = data['close'].rolling(window=20).mean()\n",
    "    data['SMA_60'] = data['close'].rolling(window=60).mean()\n",
    "    data['SMA_120'] = data['close'].rolling(window=120).mean()\n",
    "    data['SMA_250'] = data['close'].rolling(window=250).mean()\n",
    "    data['RSI'] = ta.rsi(data['close'])\n",
    "    bb = ta.bbands(data['close'])\n",
    "    data['BB_Upper'], data['BB_Middle'], data['BB_Lower'] = bb.iloc[:, 0], bb.iloc[:, 1], bb.iloc[:, 2]\n",
    "    macd = ta.macd(data['close'])\n",
    "    data['MACD'] = macd.iloc[:, 0]\n",
    "    data['Stochastic'] = ta.stoch(data['high'], data['low'], data['close']).iloc[:, 0]\n",
    "    return data.dropna()\n",
    "\n",
    "####################################\n",
    "# 2. Datetime   Positional (Cyclical) \n",
    "####################################\n",
    "def encode_datetime_features_positional(data):\n",
    "    if 'datetime' not in data.columns:\n",
    "        data['datetime'] = pd.to_datetime(data.index)\n",
    "    data['hour_sin'] = np.sin(2 * np.pi * data['datetime'].dt.hour / 24)\n",
    "    data['hour_cos'] = np.cos(2 * np.pi * data['datetime'].dt.hour / 24)\n",
    "    data['day_sin'] = np.sin(2 * np.pi * data['datetime'].dt.dayofweek / 7)\n",
    "    data['day_cos'] = np.cos(2 * np.pi * data['datetime'].dt.dayofweek / 7)\n",
    "    data['week_of_month'] = (data['datetime'].dt.day - 1) // 7 + 1\n",
    "    data['week_sin'] = np.sin(2 * np.pi * data['week_of_month'] / 5)\n",
    "    data['week_cos'] = np.cos(2 * np.pi * data['week_of_month'] / 5)\n",
    "    data['month_sin'] = np.sin(2 * np.pi * data['datetime'].dt.month / 12)\n",
    "    data['month_cos'] = np.cos(2 * np.pi * data['datetime'].dt.month / 12)\n",
    "    return data\n",
    "\n",
    "####################################\n",
    "# 3.  50  rolling minmax scaling ( 0 )\n",
    "####################################\n",
    "def rolling_minmax_scale(series, window=12):\n",
    "    roll_min = series.rolling(window=window, min_periods=window).min()\n",
    "    roll_max = series.rolling(window=window, min_periods=window).max()\n",
    "    scaled = (series - roll_min) / ((roll_max - roll_min) + 1e-8)\n",
    "    return scaled\n",
    "\n",
    "####################################\n",
    "# 4. Binning  One-Hot  ( )\n",
    "####################################\n",
    "def bin_and_encode(data, features, bins=10, drop_original=True):\n",
    "    for feature in features:\n",
    "        data[f'{feature}_Bin'] = pd.cut(data[feature], bins=bins, labels=False)\n",
    "        one_hot = pd.get_dummies(data[f'{feature}_Bin'], prefix=f'{feature}_Bin')\n",
    "        expected_columns = [f'{feature}_Bin_{i}' for i in range(bins)]\n",
    "        one_hot = one_hot.reindex(columns=expected_columns, fill_value=0)\n",
    "        data = pd.concat([data, one_hot], axis=1)\n",
    "        if drop_original:\n",
    "            data.drop(columns=[f'{feature}_Bin'], inplace=True)\n",
    "    data = data.astype(np.float32)\n",
    "    return data\n",
    "\n",
    "####################################\n",
    "# 5.    \n",
    "####################################\n",
    "data = pd.read_csv(\"ETH_upbit_KRW_min5.csv\", index_col=0)\n",
    "data.columns = ['open', 'high', 'low', 'close', 'volume', 'value']\n",
    "data.index = pd.to_datetime(data.index)\n",
    "data = calculate_indicators(data)\n",
    "data = encode_datetime_features_positional(data)\n",
    "\n",
    "features_to_bin = ['open', 'high', 'low', 'volume', 'value', 'William_R',\n",
    "                   'ATR', 'OBV', 'Z_Score', 'Entropy', 'SMA_5', 'SMA_10', \n",
    "                   'SMA_20', 'SMA_60', 'SMA_120', 'SMA_250', 'RSI', 'BB_Upper', 'BB_Middle', \n",
    "                   'BB_Lower', 'MACD', 'Stochastic']\n",
    "datetime_features = ['hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'week_sin', 'week_cos', 'month_sin', 'month_cos']\n",
    "\n",
    "# close_target  close \n",
    "data['close_target'] = data['close']\n",
    "\n",
    "selected_features = features_to_bin + ['close_target'] + datetime_features\n",
    "data = data[selected_features].dropna()\n",
    "\n",
    "# rolling scaling  (window=512:   )\n",
    "for feature in features_to_bin:\n",
    "    data[feature] = rolling_minmax_scale(data[feature], window=32)\n",
    "data = data.dropna()\n",
    "\n",
    "data = bin_and_encode(data, features_to_bin, bins=10, drop_original=True)\n",
    "data['close_for_binning'] = data['close_target']\n",
    "data = bin_and_encode(data, ['close_for_binning'], bins=10, drop_original=False)\n",
    "data.drop(columns=['close_for_binning'], inplace=True)\n",
    "\n",
    "final_columns = []\n",
    "for feature in features_to_bin:\n",
    "    final_columns.extend([f'{feature}_Bin_{i}' for i in range(10)])\n",
    "final_columns.append('close_target')\n",
    "final_columns.extend([f'close_for_binning_Bin_{i}' for i in range(10)])\n",
    "final_columns.extend(datetime_features)\n",
    "data = data[final_columns]\n",
    "\n",
    "####################################\n",
    "# 6. Dataset \n",
    "####################################\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, lookback=32, target_col='close_target'):\n",
    "        self.data = data.values\n",
    "        self.lookback = lookback\n",
    "        self.target_idx = list(data.columns).index(target_col)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.lookback\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.lookback, :]\n",
    "        y = self.data[idx + self.lookback, self.target_idx]\n",
    "        y_prev = self.data[idx + self.lookback - 1, self.target_idx]\n",
    "        y_target = 1 if y > y_prev else 0\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y_target, dtype=torch.long)\n",
    "\n",
    "####################################\n",
    "# 7. Transformer Encoder  \n",
    "####################################\n",
    "# 7-1. Multi-Head Self-Attention\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == embed_dim, \"embed_dim num_heads  .\"\n",
    "        \n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key   = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out   = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        \n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / sqrt(self.head_dim)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, V)\n",
    "        \n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "\n",
    "# 7-2. Feed-Forward Network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ffn_dim):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, ffn_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(ffn_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "# 7-3. Transformer Encoder Layer (Self-Attention + FFN + Residual + LayerNorm)\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = FeedForward(embed_dim, ffn_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attn_out = self.self_attn(x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_out)\n",
    "        return x\n",
    "\n",
    "# 7-4. Encoder-Only Transformer  \n",
    "class EncoderOnlyTransformerCustom(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=512, num_heads=8, num_layers=6, ffn_dim=1024, num_classes=2, max_seq_len=32):\n",
    "        super(EncoderOnlyTransformerCustom, self).__init__()\n",
    "        self.token_embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embedding_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(embedding_dim, num_heads, ffn_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x = self.token_embedding(x)\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        x = x + self.position_embedding(positions)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x[:, -1, :]\n",
    "        return self.fc(x)\n",
    "\n",
    "####################################\n",
    "# 8.     (Fine-tuning  Validation Accuracy )\n",
    "####################################\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return total_loss / len(data_loader), correct / total\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, lr, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_state = None\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, device)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = model.state_dict()\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(data, num_experiments=16, lookback=32, num_epochs=10):\n",
    "    input_dim = data.shape[1]\n",
    "    step_size = 30000  #  \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    for exp in range(num_experiments):\n",
    "        train_start = exp * step_size\n",
    "        train_end = train_start + step_size * 8   #  \n",
    "        val_end = train_end + step_size            #  \n",
    "        test_end = val_end + step_size             #  \n",
    "        if test_end > len(data):\n",
    "            break\n",
    "        \n",
    "        train_data = data.iloc[train_start:train_end]\n",
    "        val_data = data.iloc[train_end:val_end]\n",
    "        test_data = data.iloc[val_end:test_end]\n",
    "        \n",
    "        train_dataset = TimeSeriesDataset(train_data, lookback=lookback, target_col='close_target')\n",
    "        val_dataset = TimeSeriesDataset(val_data, lookback=lookback, target_col='close_target')\n",
    "        test_dataset = TimeSeriesDataset(test_data, lookback=lookback, target_col='close_target')\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        # Fine-tuning:    lr,    lr \n",
    "        if exp == 0:\n",
    "            lr = 1e-4\n",
    "        else:\n",
    "            lr = 1e-5\n",
    "        model = EncoderOnlyTransformerCustom(input_dim=input_dim, embedding_dim=512, num_heads=8, \n",
    "                                               num_layers=6, ffn_dim=1024, num_classes=2, max_seq_len=32).to(device)\n",
    "        model_path = f\"model_experiment_{exp}.pth\"\n",
    "        if exp > 0:\n",
    "            try:\n",
    "                model.load_state_dict(torch.load(f\"model_experiment_{exp - 1}.pth\"))\n",
    "                print(f\"Loaded model from experiment {exp - 1}.\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Model file for experiment {exp - 1} not found. Starting fresh training.\")\n",
    "        \n",
    "        print(f\"Experiment {exp}: Training with lr={lr}\")\n",
    "        model = train_model(model, train_loader, val_loader, num_epochs, lr, device)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Saved model for experiment {exp}.\")\n",
    "        \n",
    "        #    \n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, device)\n",
    "        print(f\"Experiment {exp}: Final Validation Accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "        #   \n",
    "        test_loss, test_acc = evaluate_model(model, test_loader, device)\n",
    "        print(f\"Experiment {exp}: Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "train_and_evaluate(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39coin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
