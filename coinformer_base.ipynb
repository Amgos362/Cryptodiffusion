{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         open      high       low    volume     value  \\\n",
      "2017-10-07 02:00:00  0.679612  0.662338  0.338346  0.003346  0.003359   \n",
      "2017-10-07 03:00:00  0.020202  0.298701  0.285714  0.002740  0.002707   \n",
      "2017-10-07 04:00:00  0.070707  0.298701  0.240602  0.003067  0.003032   \n",
      "2017-10-07 05:00:00  0.464646  0.519481  0.368421  0.003391  0.003412   \n",
      "2017-10-07 06:00:00  0.505050  0.636364  0.375940  0.002723  0.002738   \n",
      "...                       ...       ...       ...       ...       ...   \n",
      "2024-12-18 12:00:00  0.000000  0.000000  0.000000  0.442018  0.435797   \n",
      "2024-12-18 13:00:00  0.000000  0.000000  0.000000  1.000000  1.000000   \n",
      "2024-12-18 14:00:00  0.000000  0.000000  0.199575  0.298076  0.297948   \n",
      "2024-12-18 15:00:00  0.000000  0.000000  0.226983  0.109934  0.109102   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.000000  0.000000   \n",
      "\n",
      "                     William_R       ATR       OBV   Z_Score   Entropy  ...  \\\n",
      "2017-10-07 02:00:00   0.000000  1.000000  0.000000  0.000000  0.000000  ...   \n",
      "2017-10-07 03:00:00   0.437500  0.967643  0.005921  0.377739  0.001146  ...   \n",
      "2017-10-07 04:00:00   0.395833  0.974865  0.000000  0.319537  0.000000  ...   \n",
      "2017-10-07 05:00:00   0.437500  0.981571  0.006543  0.356728  0.000000  ...   \n",
      "2017-10-07 06:00:00   0.270833  1.000000  0.000640  0.192992  0.000000  ...   \n",
      "...                        ...       ...       ...       ...       ...  ...   \n",
      "2024-12-18 12:00:00   0.000000  0.883939  0.000000  0.000000  0.000000  ...   \n",
      "2024-12-18 13:00:00   0.184551  1.000000  0.000000  0.000000  0.000000  ...   \n",
      "2024-12-18 14:00:00   0.207054  1.000000  0.000000  0.083813  0.000000  ...   \n",
      "2024-12-18 15:00:00   0.245174  0.954989  0.047716  0.178420  0.000000  ...   \n",
      "2024-12-18 16:00:00   0.323835  0.870034  0.063031  0.264760  0.000000  ...   \n",
      "\n",
      "                     close_for_binning_Bin_118  close_for_binning_Bin_119  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_120  close_for_binning_Bin_121  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_122  close_for_binning_Bin_123  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        1.0   \n",
      "2024-12-18 14:00:00                        0.0                        1.0   \n",
      "2024-12-18 15:00:00                        0.0                        1.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_124  close_for_binning_Bin_125  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        1.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        1.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_126  close_for_binning_Bin_127  \n",
      "2017-10-07 02:00:00                        0.0                        0.0  \n",
      "2017-10-07 03:00:00                        0.0                        0.0  \n",
      "2017-10-07 04:00:00                        0.0                        0.0  \n",
      "2017-10-07 05:00:00                        0.0                        0.0  \n",
      "2017-10-07 06:00:00                        0.0                        0.0  \n",
      "...                                        ...                        ...  \n",
      "2024-12-18 12:00:00                        0.0                        0.0  \n",
      "2024-12-18 13:00:00                        0.0                        0.0  \n",
      "2024-12-18 14:00:00                        0.0                        0.0  \n",
      "2024-12-18 15:00:00                        0.0                        0.0  \n",
      "2024-12-18 16:00:00                        0.0                        0.0  \n",
      "\n",
      "[63011 rows x 4254 columns]\n",
      "Experiment 0: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.7224, Train Acc: 0.5010 | Val Loss: 0.6944, Val Acc: 0.4903\n",
      "Epoch 2/10 | Train Loss: 0.6966, Train Acc: 0.5125 | Val Loss: 0.6933, Val Acc: 0.5036\n",
      "Epoch 3/10 | Train Loss: 0.6833, Train Acc: 0.5683 | Val Loss: 0.6928, Val Acc: 0.5404\n",
      "Epoch 4/10 | Train Loss: 0.6631, Train Acc: 0.6071 | Val Loss: 0.7076, Val Acc: 0.5412\n",
      "Epoch 5/10 | Train Loss: 0.6474, Train Acc: 0.6342 | Val Loss: 0.7254, Val Acc: 0.5347\n",
      "Epoch 6/10 | Train Loss: 0.6216, Train Acc: 0.6696 | Val Loss: 0.7528, Val Acc: 0.5295\n",
      "Epoch 7/10 | Train Loss: 0.5756, Train Acc: 0.7173 | Val Loss: 0.7682, Val Acc: 0.5307\n",
      "Epoch 8/10 | Train Loss: 0.5556, Train Acc: 0.7397 | Val Loss: 0.7567, Val Acc: 0.5234\n",
      "Epoch 9/10 | Train Loss: 0.5400, Train Acc: 0.7550 | Val Loss: 0.8329, Val Acc: 0.5234\n",
      "Epoch 10/10 | Train Loss: 0.5090, Train Acc: 0.7835 | Val Loss: 0.8741, Val Acc: 0.5254\n",
      "Saved model for experiment 0.\n",
      "Experiment 0: Final Validation Accuracy: 0.5254\n",
      "Experiment 0: Test Accuracy: 0.5222\n",
      "                         open      high       low    volume     value  \\\n",
      "2017-10-07 02:00:00  0.679612  0.662338  0.338346  0.003346  0.003359   \n",
      "2017-10-07 03:00:00  0.020202  0.298701  0.285714  0.002740  0.002707   \n",
      "2017-10-07 04:00:00  0.070707  0.298701  0.240602  0.003067  0.003032   \n",
      "2017-10-07 05:00:00  0.464646  0.519481  0.368421  0.003391  0.003412   \n",
      "2017-10-07 06:00:00  0.505050  0.636364  0.375940  0.002723  0.002738   \n",
      "...                       ...       ...       ...       ...       ...   \n",
      "2024-12-18 12:00:00  0.000000  0.000000  0.000000  0.442018  0.435797   \n",
      "2024-12-18 13:00:00  0.000000  0.000000  0.000000  1.000000  1.000000   \n",
      "2024-12-18 14:00:00  0.000000  0.000000  0.199575  0.298076  0.297948   \n",
      "2024-12-18 15:00:00  0.000000  0.000000  0.226983  0.109934  0.109102   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.000000  0.000000   \n",
      "\n",
      "                     William_R       ATR       OBV   Z_Score   Entropy  ...  \\\n",
      "2017-10-07 02:00:00   0.000000  1.000000  0.000000  0.000000  0.000000  ...   \n",
      "2017-10-07 03:00:00   0.437500  0.967643  0.005921  0.377739  0.001146  ...   \n",
      "2017-10-07 04:00:00   0.395833  0.974865  0.000000  0.319537  0.000000  ...   \n",
      "2017-10-07 05:00:00   0.437500  0.981571  0.006543  0.356728  0.000000  ...   \n",
      "2017-10-07 06:00:00   0.270833  1.000000  0.000640  0.192992  0.000000  ...   \n",
      "...                        ...       ...       ...       ...       ...  ...   \n",
      "2024-12-18 12:00:00   0.000000  0.883939  0.000000  0.000000  0.000000  ...   \n",
      "2024-12-18 13:00:00   0.184551  1.000000  0.000000  0.000000  0.000000  ...   \n",
      "2024-12-18 14:00:00   0.207054  1.000000  0.000000  0.083813  0.000000  ...   \n",
      "2024-12-18 15:00:00   0.245174  0.954989  0.047716  0.178420  0.000000  ...   \n",
      "2024-12-18 16:00:00   0.323835  0.870034  0.063031  0.264760  0.000000  ...   \n",
      "\n",
      "                     close_for_binning_Bin_118  close_for_binning_Bin_119  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_120  close_for_binning_Bin_121  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_122  close_for_binning_Bin_123  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        1.0   \n",
      "2024-12-18 14:00:00                        0.0                        1.0   \n",
      "2024-12-18 15:00:00                        0.0                        1.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_124  close_for_binning_Bin_125  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        1.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        1.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_126  close_for_binning_Bin_127  \n",
      "2017-10-07 02:00:00                        0.0                        0.0  \n",
      "2017-10-07 03:00:00                        0.0                        0.0  \n",
      "2017-10-07 04:00:00                        0.0                        0.0  \n",
      "2017-10-07 05:00:00                        0.0                        0.0  \n",
      "2017-10-07 06:00:00                        0.0                        0.0  \n",
      "...                                        ...                        ...  \n",
      "2024-12-18 12:00:00                        0.0                        0.0  \n",
      "2024-12-18 13:00:00                        0.0                        0.0  \n",
      "2024-12-18 14:00:00                        0.0                        0.0  \n",
      "2024-12-18 15:00:00                        0.0                        0.0  \n",
      "2024-12-18 16:00:00                        0.0                        0.0  \n",
      "\n",
      "[63011 rows x 4254 columns]\n",
      "Loaded model from experiment 0 for fine-tuning.\n",
      "Experiment 1: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.5754, Train Acc: 0.7303 | Val Loss: 0.7941, Val Acc: 0.5263\n",
      "Epoch 2/10 | Train Loss: 0.5566, Train Acc: 0.7459 | Val Loss: 0.7947, Val Acc: 0.5287\n",
      "Epoch 3/10 | Train Loss: 0.5342, Train Acc: 0.7652 | Val Loss: 0.8581, Val Acc: 0.5194\n",
      "Epoch 4/10 | Train Loss: 0.5203, Train Acc: 0.7756 | Val Loss: 0.8982, Val Acc: 0.5214\n",
      "Epoch 5/10 | Train Loss: 0.4843, Train Acc: 0.8014 | Val Loss: 0.9614, Val Acc: 0.5190\n",
      "Epoch 6/10 | Train Loss: 0.4560, Train Acc: 0.8194 | Val Loss: 0.9568, Val Acc: 0.5263\n",
      "Epoch 7/10 | Train Loss: 0.4385, Train Acc: 0.8271 | Val Loss: 1.0476, Val Acc: 0.5174\n",
      "Epoch 8/10 | Train Loss: 0.4083, Train Acc: 0.8390 | Val Loss: 1.1346, Val Acc: 0.5198\n",
      "Epoch 9/10 | Train Loss: 0.3903, Train Acc: 0.8454 | Val Loss: 1.1559, Val Acc: 0.5162\n",
      "Epoch 10/10 | Train Loss: 0.3755, Train Acc: 0.8485 | Val Loss: 1.2063, Val Acc: 0.5153\n",
      "Saved model for experiment 1.\n",
      "Experiment 1: Final Validation Accuracy: 0.5153\n",
      "Experiment 1: Test Accuracy: 0.5044\n",
      "                         open      high       low    volume     value  \\\n",
      "2017-10-07 02:00:00  0.679612  0.662338  0.338346  0.003346  0.003359   \n",
      "2017-10-07 03:00:00  0.020202  0.298701  0.285714  0.002740  0.002707   \n",
      "2017-10-07 04:00:00  0.070707  0.298701  0.240602  0.003067  0.003032   \n",
      "2017-10-07 05:00:00  0.464646  0.519481  0.368421  0.003391  0.003412   \n",
      "2017-10-07 06:00:00  0.505050  0.636364  0.375940  0.002723  0.002738   \n",
      "...                       ...       ...       ...       ...       ...   \n",
      "2024-12-18 12:00:00  0.000000  0.000000  0.000000  0.442018  0.435797   \n",
      "2024-12-18 13:00:00  0.000000  0.000000  0.000000  1.000000  1.000000   \n",
      "2024-12-18 14:00:00  0.000000  0.000000  0.199575  0.298076  0.297948   \n",
      "2024-12-18 15:00:00  0.000000  0.000000  0.226983  0.109934  0.109102   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.000000  0.000000   \n",
      "\n",
      "                     William_R       ATR       OBV   Z_Score   Entropy  ...  \\\n",
      "2017-10-07 02:00:00   0.000000  1.000000  0.000000  0.000000  0.000000  ...   \n",
      "2017-10-07 03:00:00   0.437500  0.967643  0.005921  0.377739  0.001146  ...   \n",
      "2017-10-07 04:00:00   0.395833  0.974865  0.000000  0.319537  0.000000  ...   \n",
      "2017-10-07 05:00:00   0.437500  0.981571  0.006543  0.356728  0.000000  ...   \n",
      "2017-10-07 06:00:00   0.270833  1.000000  0.000640  0.192992  0.000000  ...   \n",
      "...                        ...       ...       ...       ...       ...  ...   \n",
      "2024-12-18 12:00:00   0.000000  0.883939  0.000000  0.000000  0.000000  ...   \n",
      "2024-12-18 13:00:00   0.184551  1.000000  0.000000  0.000000  0.000000  ...   \n",
      "2024-12-18 14:00:00   0.207054  1.000000  0.000000  0.083813  0.000000  ...   \n",
      "2024-12-18 15:00:00   0.245174  0.954989  0.047716  0.178420  0.000000  ...   \n",
      "2024-12-18 16:00:00   0.323835  0.870034  0.063031  0.264760  0.000000  ...   \n",
      "\n",
      "                     close_for_binning_Bin_118  close_for_binning_Bin_119  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_120  close_for_binning_Bin_121  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_122  close_for_binning_Bin_123  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        1.0   \n",
      "2024-12-18 14:00:00                        0.0                        1.0   \n",
      "2024-12-18 15:00:00                        0.0                        1.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_124  close_for_binning_Bin_125  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        1.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        1.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_126  close_for_binning_Bin_127  \n",
      "2017-10-07 02:00:00                        0.0                        0.0  \n",
      "2017-10-07 03:00:00                        0.0                        0.0  \n",
      "2017-10-07 04:00:00                        0.0                        0.0  \n",
      "2017-10-07 05:00:00                        0.0                        0.0  \n",
      "2017-10-07 06:00:00                        0.0                        0.0  \n",
      "...                                        ...                        ...  \n",
      "2024-12-18 12:00:00                        0.0                        0.0  \n",
      "2024-12-18 13:00:00                        0.0                        0.0  \n",
      "2024-12-18 14:00:00                        0.0                        0.0  \n",
      "2024-12-18 15:00:00                        0.0                        0.0  \n",
      "2024-12-18 16:00:00                        0.0                        0.0  \n",
      "\n",
      "[63011 rows x 4254 columns]\n",
      "Loaded model from experiment 1 for fine-tuning.\n",
      "Experiment 2: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.5278, Train Acc: 0.7666 | Val Loss: 0.9014, Val Acc: 0.4968\n",
      "Epoch 2/10 | Train Loss: 0.5236, Train Acc: 0.7686 | Val Loss: 0.8965, Val Acc: 0.5093\n",
      "Epoch 3/10 | Train Loss: 0.5031, Train Acc: 0.7800 | Val Loss: 0.9531, Val Acc: 0.5012\n",
      "Epoch 4/10 | Train Loss: 0.4798, Train Acc: 0.7895 | Val Loss: 0.9652, Val Acc: 0.5044\n",
      "Epoch 5/10 | Train Loss: 0.4601, Train Acc: 0.7957 | Val Loss: 0.9552, Val Acc: 0.5053\n",
      "Epoch 6/10 | Train Loss: 0.4055, Train Acc: 0.8173 | Val Loss: 1.0844, Val Acc: 0.5089\n",
      "Epoch 7/10 | Train Loss: 0.3812, Train Acc: 0.8210 | Val Loss: 1.2327, Val Acc: 0.4915\n",
      "Epoch 8/10 | Train Loss: 0.3518, Train Acc: 0.8276 | Val Loss: 1.4380, Val Acc: 0.5036\n",
      "Epoch 9/10 | Train Loss: 0.3143, Train Acc: 0.8517 | Val Loss: 1.4276, Val Acc: 0.5032\n",
      "Epoch 10/10 | Train Loss: 0.2953, Train Acc: 0.8714 | Val Loss: 1.5693, Val Acc: 0.4939\n",
      "Saved model for experiment 2.\n",
      "Experiment 2: Final Validation Accuracy: 0.4939\n",
      "Experiment 2: Test Accuracy: 0.5069\n",
      "                         open      high       low    volume     value  \\\n",
      "2017-10-07 02:00:00  0.679612  0.662338  0.338346  0.003346  0.003359   \n",
      "2017-10-07 03:00:00  0.020202  0.298701  0.285714  0.002740  0.002707   \n",
      "2017-10-07 04:00:00  0.070707  0.298701  0.240602  0.003067  0.003032   \n",
      "2017-10-07 05:00:00  0.464646  0.519481  0.368421  0.003391  0.003412   \n",
      "2017-10-07 06:00:00  0.505050  0.636364  0.375940  0.002723  0.002738   \n",
      "...                       ...       ...       ...       ...       ...   \n",
      "2024-12-18 12:00:00  0.000000  0.000000  0.000000  0.442018  0.435797   \n",
      "2024-12-18 13:00:00  0.000000  0.000000  0.000000  1.000000  1.000000   \n",
      "2024-12-18 14:00:00  0.000000  0.000000  0.199575  0.298076  0.297948   \n",
      "2024-12-18 15:00:00  0.000000  0.000000  0.226983  0.109934  0.109102   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.000000  0.000000   \n",
      "\n",
      "                     William_R       ATR       OBV   Z_Score   Entropy  ...  \\\n",
      "2017-10-07 02:00:00   0.000000  1.000000  0.000000  0.000000  0.000000  ...   \n",
      "2017-10-07 03:00:00   0.437500  0.967643  0.005921  0.377739  0.001146  ...   \n",
      "2017-10-07 04:00:00   0.395833  0.974865  0.000000  0.319537  0.000000  ...   \n",
      "2017-10-07 05:00:00   0.437500  0.981571  0.006543  0.356728  0.000000  ...   \n",
      "2017-10-07 06:00:00   0.270833  1.000000  0.000640  0.192992  0.000000  ...   \n",
      "...                        ...       ...       ...       ...       ...  ...   \n",
      "2024-12-18 12:00:00   0.000000  0.883939  0.000000  0.000000  0.000000  ...   \n",
      "2024-12-18 13:00:00   0.184551  1.000000  0.000000  0.000000  0.000000  ...   \n",
      "2024-12-18 14:00:00   0.207054  1.000000  0.000000  0.083813  0.000000  ...   \n",
      "2024-12-18 15:00:00   0.245174  0.954989  0.047716  0.178420  0.000000  ...   \n",
      "2024-12-18 16:00:00   0.323835  0.870034  0.063031  0.264760  0.000000  ...   \n",
      "\n",
      "                     close_for_binning_Bin_118  close_for_binning_Bin_119  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_120  close_for_binning_Bin_121  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_122  close_for_binning_Bin_123  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        1.0   \n",
      "2024-12-18 14:00:00                        0.0                        1.0   \n",
      "2024-12-18 15:00:00                        0.0                        1.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_124  close_for_binning_Bin_125  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        1.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        1.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_126  close_for_binning_Bin_127  \n",
      "2017-10-07 02:00:00                        0.0                        0.0  \n",
      "2017-10-07 03:00:00                        0.0                        0.0  \n",
      "2017-10-07 04:00:00                        0.0                        0.0  \n",
      "2017-10-07 05:00:00                        0.0                        0.0  \n",
      "2017-10-07 06:00:00                        0.0                        0.0  \n",
      "...                                        ...                        ...  \n",
      "2024-12-18 12:00:00                        0.0                        0.0  \n",
      "2024-12-18 13:00:00                        0.0                        0.0  \n",
      "2024-12-18 14:00:00                        0.0                        0.0  \n",
      "2024-12-18 15:00:00                        0.0                        0.0  \n",
      "2024-12-18 16:00:00                        0.0                        0.0  \n",
      "\n",
      "[63011 rows x 4254 columns]\n",
      "Loaded model from experiment 2 for fine-tuning.\n",
      "Experiment 3: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.4760, Train Acc: 0.7736 | Val Loss: 0.9590, Val Acc: 0.5097\n",
      "Epoch 2/10 | Train Loss: 0.4690, Train Acc: 0.7700 | Val Loss: 0.9982, Val Acc: 0.5065\n",
      "Epoch 3/10 | Train Loss: 0.4444, Train Acc: 0.7838 | Val Loss: 1.1429, Val Acc: 0.5206\n",
      "Epoch 4/10 | Train Loss: 0.4286, Train Acc: 0.7949 | Val Loss: 1.0646, Val Acc: 0.5295\n",
      "Epoch 5/10 | Train Loss: 0.3566, Train Acc: 0.8409 | Val Loss: 1.1771, Val Acc: 0.5105\n",
      "Epoch 6/10 | Train Loss: 0.3191, Train Acc: 0.8665 | Val Loss: 1.3187, Val Acc: 0.5295\n",
      "Epoch 7/10 | Train Loss: 0.2894, Train Acc: 0.8850 | Val Loss: 1.4266, Val Acc: 0.5271\n",
      "Epoch 8/10 | Train Loss: 0.2432, Train Acc: 0.9128 | Val Loss: 1.5240, Val Acc: 0.5182\n",
      "Epoch 9/10 | Train Loss: 0.2198, Train Acc: 0.9249 | Val Loss: 1.7316, Val Acc: 0.5109\n",
      "Epoch 10/10 | Train Loss: 0.2062, Train Acc: 0.9350 | Val Loss: 1.6919, Val Acc: 0.5053\n",
      "Saved model for experiment 3.\n",
      "Experiment 3: Final Validation Accuracy: 0.5053\n",
      "Experiment 3: Test Accuracy: 0.5230\n",
      "                         open      high       low    volume     value  \\\n",
      "2017-10-07 02:00:00  0.679612  0.662338  0.338346  0.003346  0.003359   \n",
      "2017-10-07 03:00:00  0.020202  0.298701  0.285714  0.002740  0.002707   \n",
      "2017-10-07 04:00:00  0.070707  0.298701  0.240602  0.003067  0.003032   \n",
      "2017-10-07 05:00:00  0.464646  0.519481  0.368421  0.003391  0.003412   \n",
      "2017-10-07 06:00:00  0.505050  0.636364  0.375940  0.002723  0.002738   \n",
      "...                       ...       ...       ...       ...       ...   \n",
      "2024-12-18 12:00:00  0.000000  0.000000  0.000000  0.442018  0.435797   \n",
      "2024-12-18 13:00:00  0.000000  0.000000  0.000000  1.000000  1.000000   \n",
      "2024-12-18 14:00:00  0.000000  0.000000  0.199575  0.298076  0.297948   \n",
      "2024-12-18 15:00:00  0.000000  0.000000  0.226983  0.109934  0.109102   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.000000  0.000000   \n",
      "\n",
      "                     William_R       ATR       OBV   Z_Score   Entropy  ...  \\\n",
      "2017-10-07 02:00:00   0.000000  1.000000  0.000000  0.000000  0.000000  ...   \n",
      "2017-10-07 03:00:00   0.437500  0.967643  0.005921  0.377739  0.001146  ...   \n",
      "2017-10-07 04:00:00   0.395833  0.974865  0.000000  0.319537  0.000000  ...   \n",
      "2017-10-07 05:00:00   0.437500  0.981571  0.006543  0.356728  0.000000  ...   \n",
      "2017-10-07 06:00:00   0.270833  1.000000  0.000640  0.192992  0.000000  ...   \n",
      "...                        ...       ...       ...       ...       ...  ...   \n",
      "2024-12-18 12:00:00   0.000000  0.883939  0.000000  0.000000  0.000000  ...   \n",
      "2024-12-18 13:00:00   0.184551  1.000000  0.000000  0.000000  0.000000  ...   \n",
      "2024-12-18 14:00:00   0.207054  1.000000  0.000000  0.083813  0.000000  ...   \n",
      "2024-12-18 15:00:00   0.245174  0.954989  0.047716  0.178420  0.000000  ...   \n",
      "2024-12-18 16:00:00   0.323835  0.870034  0.063031  0.264760  0.000000  ...   \n",
      "\n",
      "                     close_for_binning_Bin_118  close_for_binning_Bin_119  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_120  close_for_binning_Bin_121  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_122  close_for_binning_Bin_123  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        1.0   \n",
      "2024-12-18 14:00:00                        0.0                        1.0   \n",
      "2024-12-18 15:00:00                        0.0                        1.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_124  close_for_binning_Bin_125  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        1.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        1.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_126  close_for_binning_Bin_127  \n",
      "2017-10-07 02:00:00                        0.0                        0.0  \n",
      "2017-10-07 03:00:00                        0.0                        0.0  \n",
      "2017-10-07 04:00:00                        0.0                        0.0  \n",
      "2017-10-07 05:00:00                        0.0                        0.0  \n",
      "2017-10-07 06:00:00                        0.0                        0.0  \n",
      "...                                        ...                        ...  \n",
      "2024-12-18 12:00:00                        0.0                        0.0  \n",
      "2024-12-18 13:00:00                        0.0                        0.0  \n",
      "2024-12-18 14:00:00                        0.0                        0.0  \n",
      "2024-12-18 15:00:00                        0.0                        0.0  \n",
      "2024-12-18 16:00:00                        0.0                        0.0  \n",
      "\n",
      "[63011 rows x 4254 columns]\n",
      "Loaded model from experiment 3 for fine-tuning.\n",
      "Experiment 4: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.4363, Train Acc: 0.8186 | Val Loss: 1.0417, Val Acc: 0.5133\n",
      "Epoch 2/10 | Train Loss: 0.4308, Train Acc: 0.8141 | Val Loss: 1.1547, Val Acc: 0.5194\n",
      "Epoch 3/10 | Train Loss: 0.3978, Train Acc: 0.8332 | Val Loss: 1.0804, Val Acc: 0.5125\n",
      "Epoch 4/10 | Train Loss: 0.3737, Train Acc: 0.8458 | Val Loss: 1.1595, Val Acc: 0.5210\n",
      "Epoch 5/10 | Train Loss: 0.3050, Train Acc: 0.8868 | Val Loss: 1.2464, Val Acc: 0.5267\n",
      "Epoch 6/10 | Train Loss: 0.2667, Train Acc: 0.9080 | Val Loss: 1.4190, Val Acc: 0.5198\n",
      "Epoch 7/10 | Train Loss: 0.2436, Train Acc: 0.9186 | Val Loss: 1.5727, Val Acc: 0.5238\n",
      "Epoch 8/10 | Train Loss: 0.1930, Train Acc: 0.9410 | Val Loss: 1.7185, Val Acc: 0.5170\n",
      "Epoch 9/10 | Train Loss: 0.1694, Train Acc: 0.9512 | Val Loss: 1.8136, Val Acc: 0.5137\n",
      "Epoch 10/10 | Train Loss: 0.1644, Train Acc: 0.9526 | Val Loss: 1.7334, Val Acc: 0.5149\n",
      "Saved model for experiment 4.\n",
      "Experiment 4: Final Validation Accuracy: 0.5149\n",
      "Experiment 4: Test Accuracy: 0.4996\n",
      "                         open      high       low    volume     value  \\\n",
      "2017-10-07 02:00:00  0.679612  0.662338  0.338346  0.003346  0.003359   \n",
      "2017-10-07 03:00:00  0.020202  0.298701  0.285714  0.002740  0.002707   \n",
      "2017-10-07 04:00:00  0.070707  0.298701  0.240602  0.003067  0.003032   \n",
      "2017-10-07 05:00:00  0.464646  0.519481  0.368421  0.003391  0.003412   \n",
      "2017-10-07 06:00:00  0.505050  0.636364  0.375940  0.002723  0.002738   \n",
      "...                       ...       ...       ...       ...       ...   \n",
      "2024-12-18 12:00:00  0.000000  0.000000  0.000000  0.442018  0.435797   \n",
      "2024-12-18 13:00:00  0.000000  0.000000  0.000000  1.000000  1.000000   \n",
      "2024-12-18 14:00:00  0.000000  0.000000  0.199575  0.298076  0.297948   \n",
      "2024-12-18 15:00:00  0.000000  0.000000  0.226983  0.109934  0.109102   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.000000  0.000000   \n",
      "\n",
      "                     William_R       ATR       OBV   Z_Score   Entropy  ...  \\\n",
      "2017-10-07 02:00:00   0.000000  1.000000  0.000000  0.000000  0.000000  ...   \n",
      "2017-10-07 03:00:00   0.437500  0.967643  0.005921  0.377739  0.001146  ...   \n",
      "2017-10-07 04:00:00   0.395833  0.974865  0.000000  0.319537  0.000000  ...   \n",
      "2017-10-07 05:00:00   0.437500  0.981571  0.006543  0.356728  0.000000  ...   \n",
      "2017-10-07 06:00:00   0.270833  1.000000  0.000640  0.192992  0.000000  ...   \n",
      "...                        ...       ...       ...       ...       ...  ...   \n",
      "2024-12-18 12:00:00   0.000000  0.883939  0.000000  0.000000  0.000000  ...   \n",
      "2024-12-18 13:00:00   0.184551  1.000000  0.000000  0.000000  0.000000  ...   \n",
      "2024-12-18 14:00:00   0.207054  1.000000  0.000000  0.083813  0.000000  ...   \n",
      "2024-12-18 15:00:00   0.245174  0.954989  0.047716  0.178420  0.000000  ...   \n",
      "2024-12-18 16:00:00   0.323835  0.870034  0.063031  0.264760  0.000000  ...   \n",
      "\n",
      "                     close_for_binning_Bin_118  close_for_binning_Bin_119  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_120  close_for_binning_Bin_121  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_122  close_for_binning_Bin_123  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        1.0   \n",
      "2024-12-18 14:00:00                        0.0                        1.0   \n",
      "2024-12-18 15:00:00                        0.0                        1.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_124  close_for_binning_Bin_125  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        1.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        1.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_126  close_for_binning_Bin_127  \n",
      "2017-10-07 02:00:00                        0.0                        0.0  \n",
      "2017-10-07 03:00:00                        0.0                        0.0  \n",
      "2017-10-07 04:00:00                        0.0                        0.0  \n",
      "2017-10-07 05:00:00                        0.0                        0.0  \n",
      "2017-10-07 06:00:00                        0.0                        0.0  \n",
      "...                                        ...                        ...  \n",
      "2024-12-18 12:00:00                        0.0                        0.0  \n",
      "2024-12-18 13:00:00                        0.0                        0.0  \n",
      "2024-12-18 14:00:00                        0.0                        0.0  \n",
      "2024-12-18 15:00:00                        0.0                        0.0  \n",
      "2024-12-18 16:00:00                        0.0                        0.0  \n",
      "\n",
      "[63011 rows x 4254 columns]\n",
      "Loaded model from experiment 4 for fine-tuning.\n",
      "Experiment 5: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.4103, Train Acc: 0.8402 | Val Loss: 1.0017, Val Acc: 0.4891\n",
      "Epoch 2/10 | Train Loss: 0.3947, Train Acc: 0.8457 | Val Loss: 1.1460, Val Acc: 0.4915\n",
      "Epoch 3/10 | Train Loss: 0.3686, Train Acc: 0.8582 | Val Loss: 1.0916, Val Acc: 0.4867\n",
      "Epoch 4/10 | Train Loss: 0.3551, Train Acc: 0.8656 | Val Loss: 1.2580, Val Acc: 0.4939\n",
      "Epoch 5/10 | Train Loss: 0.2707, Train Acc: 0.9048 | Val Loss: 1.4486, Val Acc: 0.4891\n",
      "Epoch 6/10 | Train Loss: 0.2352, Train Acc: 0.9222 | Val Loss: 1.4487, Val Acc: 0.4927\n",
      "Epoch 7/10 | Train Loss: 0.2130, Train Acc: 0.9337 | Val Loss: 1.5890, Val Acc: 0.4988\n",
      "Epoch 8/10 | Train Loss: 0.1708, Train Acc: 0.9497 | Val Loss: 1.7244, Val Acc: 0.4972\n",
      "Epoch 9/10 | Train Loss: 0.1569, Train Acc: 0.9544 | Val Loss: 1.7989, Val Acc: 0.5020\n",
      "Epoch 10/10 | Train Loss: 0.1431, Train Acc: 0.9612 | Val Loss: 1.9112, Val Acc: 0.4980\n",
      "Saved model for experiment 5.\n",
      "Experiment 5: Final Validation Accuracy: 0.4980\n",
      "Experiment 5: Test Accuracy: 0.5202\n",
      "                         open      high       low    volume     value  \\\n",
      "2017-10-07 02:00:00  0.679612  0.662338  0.338346  0.003346  0.003359   \n",
      "2017-10-07 03:00:00  0.020202  0.298701  0.285714  0.002740  0.002707   \n",
      "2017-10-07 04:00:00  0.070707  0.298701  0.240602  0.003067  0.003032   \n",
      "2017-10-07 05:00:00  0.464646  0.519481  0.368421  0.003391  0.003412   \n",
      "2017-10-07 06:00:00  0.505050  0.636364  0.375940  0.002723  0.002738   \n",
      "...                       ...       ...       ...       ...       ...   \n",
      "2024-12-18 12:00:00  0.000000  0.000000  0.000000  0.442018  0.435797   \n",
      "2024-12-18 13:00:00  0.000000  0.000000  0.000000  1.000000  1.000000   \n",
      "2024-12-18 14:00:00  0.000000  0.000000  0.199575  0.298076  0.297948   \n",
      "2024-12-18 15:00:00  0.000000  0.000000  0.226983  0.109934  0.109102   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.000000  0.000000   \n",
      "\n",
      "                     William_R       ATR       OBV   Z_Score   Entropy  ...  \\\n",
      "2017-10-07 02:00:00   0.000000  1.000000  0.000000  0.000000  0.000000  ...   \n",
      "2017-10-07 03:00:00   0.437500  0.967643  0.005921  0.377739  0.001146  ...   \n",
      "2017-10-07 04:00:00   0.395833  0.974865  0.000000  0.319537  0.000000  ...   \n",
      "2017-10-07 05:00:00   0.437500  0.981571  0.006543  0.356728  0.000000  ...   \n",
      "2017-10-07 06:00:00   0.270833  1.000000  0.000640  0.192992  0.000000  ...   \n",
      "...                        ...       ...       ...       ...       ...  ...   \n",
      "2024-12-18 12:00:00   0.000000  0.883939  0.000000  0.000000  0.000000  ...   \n",
      "2024-12-18 13:00:00   0.184551  1.000000  0.000000  0.000000  0.000000  ...   \n",
      "2024-12-18 14:00:00   0.207054  1.000000  0.000000  0.083813  0.000000  ...   \n",
      "2024-12-18 15:00:00   0.245174  0.954989  0.047716  0.178420  0.000000  ...   \n",
      "2024-12-18 16:00:00   0.323835  0.870034  0.063031  0.264760  0.000000  ...   \n",
      "\n",
      "                     close_for_binning_Bin_118  close_for_binning_Bin_119  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_120  close_for_binning_Bin_121  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_122  close_for_binning_Bin_123  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        1.0   \n",
      "2024-12-18 14:00:00                        0.0                        1.0   \n",
      "2024-12-18 15:00:00                        0.0                        1.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_124  close_for_binning_Bin_125  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        1.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        1.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_126  close_for_binning_Bin_127  \n",
      "2017-10-07 02:00:00                        0.0                        0.0  \n",
      "2017-10-07 03:00:00                        0.0                        0.0  \n",
      "2017-10-07 04:00:00                        0.0                        0.0  \n",
      "2017-10-07 05:00:00                        0.0                        0.0  \n",
      "2017-10-07 06:00:00                        0.0                        0.0  \n",
      "...                                        ...                        ...  \n",
      "2024-12-18 12:00:00                        0.0                        0.0  \n",
      "2024-12-18 13:00:00                        0.0                        0.0  \n",
      "2024-12-18 14:00:00                        0.0                        0.0  \n",
      "2024-12-18 15:00:00                        0.0                        0.0  \n",
      "2024-12-18 16:00:00                        0.0                        0.0  \n",
      "\n",
      "[63011 rows x 4254 columns]\n",
      "Loaded model from experiment 5 for fine-tuning.\n",
      "Experiment 6: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.4044, Train Acc: 0.8463 | Val Loss: 0.9555, Val Acc: 0.5218\n",
      "Epoch 2/10 | Train Loss: 0.3850, Train Acc: 0.8520 | Val Loss: 1.0466, Val Acc: 0.5077\n",
      "Epoch 3/10 | Train Loss: 0.3680, Train Acc: 0.8582 | Val Loss: 1.0402, Val Acc: 0.5109\n",
      "Epoch 4/10 | Train Loss: 0.3451, Train Acc: 0.8723 | Val Loss: 1.2409, Val Acc: 0.5214\n",
      "Epoch 5/10 | Train Loss: 0.2625, Train Acc: 0.9113 | Val Loss: 1.2795, Val Acc: 0.5226\n",
      "Epoch 6/10 | Train Loss: 0.2305, Train Acc: 0.9273 | Val Loss: 1.6100, Val Acc: 0.5158\n",
      "Epoch 7/10 | Train Loss: 0.2070, Train Acc: 0.9400 | Val Loss: 1.6279, Val Acc: 0.5214\n",
      "Epoch 8/10 | Train Loss: 0.1757, Train Acc: 0.9516 | Val Loss: 1.6947, Val Acc: 0.5202\n",
      "Epoch 9/10 | Train Loss: 0.1609, Train Acc: 0.9560 | Val Loss: 1.8027, Val Acc: 0.5234\n",
      "Epoch 10/10 | Train Loss: 0.1511, Train Acc: 0.9600 | Val Loss: 1.8041, Val Acc: 0.5182\n",
      "Saved model for experiment 6.\n",
      "Experiment 6: Final Validation Accuracy: 0.5182\n",
      "Experiment 6: Test Accuracy: 0.5109\n",
      "                         open      high       low    volume     value  \\\n",
      "2017-10-07 02:00:00  0.679612  0.662338  0.338346  0.003346  0.003359   \n",
      "2017-10-07 03:00:00  0.020202  0.298701  0.285714  0.002740  0.002707   \n",
      "2017-10-07 04:00:00  0.070707  0.298701  0.240602  0.003067  0.003032   \n",
      "2017-10-07 05:00:00  0.464646  0.519481  0.368421  0.003391  0.003412   \n",
      "2017-10-07 06:00:00  0.505050  0.636364  0.375940  0.002723  0.002738   \n",
      "...                       ...       ...       ...       ...       ...   \n",
      "2024-12-18 12:00:00  0.000000  0.000000  0.000000  0.442018  0.435797   \n",
      "2024-12-18 13:00:00  0.000000  0.000000  0.000000  1.000000  1.000000   \n",
      "2024-12-18 14:00:00  0.000000  0.000000  0.199575  0.298076  0.297948   \n",
      "2024-12-18 15:00:00  0.000000  0.000000  0.226983  0.109934  0.109102   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.000000  0.000000   \n",
      "\n",
      "                     William_R       ATR       OBV   Z_Score   Entropy  ...  \\\n",
      "2017-10-07 02:00:00   0.000000  1.000000  0.000000  0.000000  0.000000  ...   \n",
      "2017-10-07 03:00:00   0.437500  0.967643  0.005921  0.377739  0.001146  ...   \n",
      "2017-10-07 04:00:00   0.395833  0.974865  0.000000  0.319537  0.000000  ...   \n",
      "2017-10-07 05:00:00   0.437500  0.981571  0.006543  0.356728  0.000000  ...   \n",
      "2017-10-07 06:00:00   0.270833  1.000000  0.000640  0.192992  0.000000  ...   \n",
      "...                        ...       ...       ...       ...       ...  ...   \n",
      "2024-12-18 12:00:00   0.000000  0.883939  0.000000  0.000000  0.000000  ...   \n",
      "2024-12-18 13:00:00   0.184551  1.000000  0.000000  0.000000  0.000000  ...   \n",
      "2024-12-18 14:00:00   0.207054  1.000000  0.000000  0.083813  0.000000  ...   \n",
      "2024-12-18 15:00:00   0.245174  0.954989  0.047716  0.178420  0.000000  ...   \n",
      "2024-12-18 16:00:00   0.323835  0.870034  0.063031  0.264760  0.000000  ...   \n",
      "\n",
      "                     close_for_binning_Bin_118  close_for_binning_Bin_119  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_120  close_for_binning_Bin_121  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_122  close_for_binning_Bin_123  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        1.0   \n",
      "2024-12-18 14:00:00                        0.0                        1.0   \n",
      "2024-12-18 15:00:00                        0.0                        1.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_124  close_for_binning_Bin_125  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        1.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        1.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_126  close_for_binning_Bin_127  \n",
      "2017-10-07 02:00:00                        0.0                        0.0  \n",
      "2017-10-07 03:00:00                        0.0                        0.0  \n",
      "2017-10-07 04:00:00                        0.0                        0.0  \n",
      "2017-10-07 05:00:00                        0.0                        0.0  \n",
      "2017-10-07 06:00:00                        0.0                        0.0  \n",
      "...                                        ...                        ...  \n",
      "2024-12-18 12:00:00                        0.0                        0.0  \n",
      "2024-12-18 13:00:00                        0.0                        0.0  \n",
      "2024-12-18 14:00:00                        0.0                        0.0  \n",
      "2024-12-18 15:00:00                        0.0                        0.0  \n",
      "2024-12-18 16:00:00                        0.0                        0.0  \n",
      "\n",
      "[63011 rows x 4254 columns]\n",
      "Loaded model from experiment 6 for fine-tuning.\n",
      "Experiment 7: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.3893, Train Acc: 0.8540 | Val Loss: 0.9618, Val Acc: 0.5105\n",
      "Epoch 2/10 | Train Loss: 0.3761, Train Acc: 0.8554 | Val Loss: 1.1703, Val Acc: 0.5057\n",
      "Epoch 3/10 | Train Loss: 0.3628, Train Acc: 0.8635 | Val Loss: 1.2393, Val Acc: 0.5109\n",
      "Epoch 4/10 | Train Loss: 0.3371, Train Acc: 0.8764 | Val Loss: 1.1837, Val Acc: 0.5113\n",
      "Epoch 5/10 | Train Loss: 0.2638, Train Acc: 0.9117 | Val Loss: 1.3133, Val Acc: 0.5186\n",
      "Epoch 6/10 | Train Loss: 0.2212, Train Acc: 0.9314 | Val Loss: 1.4649, Val Acc: 0.5129\n",
      "Epoch 7/10 | Train Loss: 0.2029, Train Acc: 0.9400 | Val Loss: 1.6694, Val Acc: 0.5162\n",
      "Epoch 8/10 | Train Loss: 0.1679, Train Acc: 0.9529 | Val Loss: 1.9017, Val Acc: 0.5170\n",
      "Epoch 9/10 | Train Loss: 0.1501, Train Acc: 0.9586 | Val Loss: 1.9689, Val Acc: 0.5117\n",
      "Epoch 10/10 | Train Loss: 0.1364, Train Acc: 0.9634 | Val Loss: 2.0087, Val Acc: 0.5153\n",
      "Saved model for experiment 7.\n",
      "Experiment 7: Final Validation Accuracy: 0.5153\n",
      "Experiment 7: Test Accuracy: 0.5190\n",
      "                         open      high       low    volume     value  \\\n",
      "2017-10-07 02:00:00  0.679612  0.662338  0.338346  0.003346  0.003359   \n",
      "2017-10-07 03:00:00  0.020202  0.298701  0.285714  0.002740  0.002707   \n",
      "2017-10-07 04:00:00  0.070707  0.298701  0.240602  0.003067  0.003032   \n",
      "2017-10-07 05:00:00  0.464646  0.519481  0.368421  0.003391  0.003412   \n",
      "2017-10-07 06:00:00  0.505050  0.636364  0.375940  0.002723  0.002738   \n",
      "...                       ...       ...       ...       ...       ...   \n",
      "2024-12-18 12:00:00  0.000000  0.000000  0.000000  0.442018  0.435797   \n",
      "2024-12-18 13:00:00  0.000000  0.000000  0.000000  1.000000  1.000000   \n",
      "2024-12-18 14:00:00  0.000000  0.000000  0.199575  0.298076  0.297948   \n",
      "2024-12-18 15:00:00  0.000000  0.000000  0.226983  0.109934  0.109102   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.000000  0.000000   \n",
      "\n",
      "                     William_R       ATR       OBV   Z_Score   Entropy  ...  \\\n",
      "2017-10-07 02:00:00   0.000000  1.000000  0.000000  0.000000  0.000000  ...   \n",
      "2017-10-07 03:00:00   0.437500  0.967643  0.005921  0.377739  0.001146  ...   \n",
      "2017-10-07 04:00:00   0.395833  0.974865  0.000000  0.319537  0.000000  ...   \n",
      "2017-10-07 05:00:00   0.437500  0.981571  0.006543  0.356728  0.000000  ...   \n",
      "2017-10-07 06:00:00   0.270833  1.000000  0.000640  0.192992  0.000000  ...   \n",
      "...                        ...       ...       ...       ...       ...  ...   \n",
      "2024-12-18 12:00:00   0.000000  0.883939  0.000000  0.000000  0.000000  ...   \n",
      "2024-12-18 13:00:00   0.184551  1.000000  0.000000  0.000000  0.000000  ...   \n",
      "2024-12-18 14:00:00   0.207054  1.000000  0.000000  0.083813  0.000000  ...   \n",
      "2024-12-18 15:00:00   0.245174  0.954989  0.047716  0.178420  0.000000  ...   \n",
      "2024-12-18 16:00:00   0.323835  0.870034  0.063031  0.264760  0.000000  ...   \n",
      "\n",
      "                     close_for_binning_Bin_118  close_for_binning_Bin_119  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_120  close_for_binning_Bin_121  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_122  close_for_binning_Bin_123  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        1.0   \n",
      "2024-12-18 14:00:00                        0.0                        1.0   \n",
      "2024-12-18 15:00:00                        0.0                        1.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_124  close_for_binning_Bin_125  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        1.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        1.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_126  close_for_binning_Bin_127  \n",
      "2017-10-07 02:00:00                        0.0                        0.0  \n",
      "2017-10-07 03:00:00                        0.0                        0.0  \n",
      "2017-10-07 04:00:00                        0.0                        0.0  \n",
      "2017-10-07 05:00:00                        0.0                        0.0  \n",
      "2017-10-07 06:00:00                        0.0                        0.0  \n",
      "...                                        ...                        ...  \n",
      "2024-12-18 12:00:00                        0.0                        0.0  \n",
      "2024-12-18 13:00:00                        0.0                        0.0  \n",
      "2024-12-18 14:00:00                        0.0                        0.0  \n",
      "2024-12-18 15:00:00                        0.0                        0.0  \n",
      "2024-12-18 16:00:00                        0.0                        0.0  \n",
      "\n",
      "[63011 rows x 4254 columns]\n",
      "Loaded model from experiment 7 for fine-tuning.\n",
      "Experiment 8: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.3826, Train Acc: 0.8589 | Val Loss: 1.1273, Val Acc: 0.5149\n",
      "Epoch 2/10 | Train Loss: 0.3548, Train Acc: 0.8667 | Val Loss: 1.1635, Val Acc: 0.5089\n",
      "Epoch 3/10 | Train Loss: 0.3274, Train Acc: 0.8803 | Val Loss: 1.2588, Val Acc: 0.5182\n",
      "Epoch 4/10 | Train Loss: 0.3107, Train Acc: 0.8883 | Val Loss: 1.3460, Val Acc: 0.5093\n",
      "Epoch 5/10 | Train Loss: 0.2372, Train Acc: 0.9227 | Val Loss: 1.4152, Val Acc: 0.5073\n",
      "Epoch 6/10 | Train Loss: 0.1991, Train Acc: 0.9367 | Val Loss: 1.7036, Val Acc: 0.5081\n",
      "Epoch 7/10 | Train Loss: 0.1803, Train Acc: 0.9433 | Val Loss: 1.7992, Val Acc: 0.5170\n",
      "Epoch 8/10 | Train Loss: 0.1363, Train Acc: 0.9608 | Val Loss: 1.9922, Val Acc: 0.5153\n",
      "Epoch 9/10 | Train Loss: 0.1188, Train Acc: 0.9679 | Val Loss: 2.2417, Val Acc: 0.5113\n",
      "Epoch 10/10 | Train Loss: 0.1004, Train Acc: 0.9756 | Val Loss: 2.1705, Val Acc: 0.5230\n",
      "Saved model for experiment 8.\n",
      "Experiment 8: Final Validation Accuracy: 0.5230\n",
      "Experiment 8: Test Accuracy: 0.5121\n",
      "                         open      high       low    volume     value  \\\n",
      "2017-10-07 02:00:00  0.679612  0.662338  0.338346  0.003346  0.003359   \n",
      "2017-10-07 03:00:00  0.020202  0.298701  0.285714  0.002740  0.002707   \n",
      "2017-10-07 04:00:00  0.070707  0.298701  0.240602  0.003067  0.003032   \n",
      "2017-10-07 05:00:00  0.464646  0.519481  0.368421  0.003391  0.003412   \n",
      "2017-10-07 06:00:00  0.505050  0.636364  0.375940  0.002723  0.002738   \n",
      "...                       ...       ...       ...       ...       ...   \n",
      "2024-12-18 12:00:00  0.000000  0.000000  0.000000  0.442018  0.435797   \n",
      "2024-12-18 13:00:00  0.000000  0.000000  0.000000  1.000000  1.000000   \n",
      "2024-12-18 14:00:00  0.000000  0.000000  0.199575  0.298076  0.297948   \n",
      "2024-12-18 15:00:00  0.000000  0.000000  0.226983  0.109934  0.109102   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.000000  0.000000   \n",
      "\n",
      "                     William_R       ATR       OBV   Z_Score   Entropy  ...  \\\n",
      "2017-10-07 02:00:00   0.000000  1.000000  0.000000  0.000000  0.000000  ...   \n",
      "2017-10-07 03:00:00   0.437500  0.967643  0.005921  0.377739  0.001146  ...   \n",
      "2017-10-07 04:00:00   0.395833  0.974865  0.000000  0.319537  0.000000  ...   \n",
      "2017-10-07 05:00:00   0.437500  0.981571  0.006543  0.356728  0.000000  ...   \n",
      "2017-10-07 06:00:00   0.270833  1.000000  0.000640  0.192992  0.000000  ...   \n",
      "...                        ...       ...       ...       ...       ...  ...   \n",
      "2024-12-18 12:00:00   0.000000  0.883939  0.000000  0.000000  0.000000  ...   \n",
      "2024-12-18 13:00:00   0.184551  1.000000  0.000000  0.000000  0.000000  ...   \n",
      "2024-12-18 14:00:00   0.207054  1.000000  0.000000  0.083813  0.000000  ...   \n",
      "2024-12-18 15:00:00   0.245174  0.954989  0.047716  0.178420  0.000000  ...   \n",
      "2024-12-18 16:00:00   0.323835  0.870034  0.063031  0.264760  0.000000  ...   \n",
      "\n",
      "                     close_for_binning_Bin_118  close_for_binning_Bin_119  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_120  close_for_binning_Bin_121  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_122  close_for_binning_Bin_123  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        1.0   \n",
      "2024-12-18 14:00:00                        0.0                        1.0   \n",
      "2024-12-18 15:00:00                        0.0                        1.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_124  close_for_binning_Bin_125  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        1.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        1.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_126  close_for_binning_Bin_127  \n",
      "2017-10-07 02:00:00                        0.0                        0.0  \n",
      "2017-10-07 03:00:00                        0.0                        0.0  \n",
      "2017-10-07 04:00:00                        0.0                        0.0  \n",
      "2017-10-07 05:00:00                        0.0                        0.0  \n",
      "2017-10-07 06:00:00                        0.0                        0.0  \n",
      "...                                        ...                        ...  \n",
      "2024-12-18 12:00:00                        0.0                        0.0  \n",
      "2024-12-18 13:00:00                        0.0                        0.0  \n",
      "2024-12-18 14:00:00                        0.0                        0.0  \n",
      "2024-12-18 15:00:00                        0.0                        0.0  \n",
      "2024-12-18 16:00:00                        0.0                        0.0  \n",
      "\n",
      "[63011 rows x 4254 columns]\n",
      "Loaded model from experiment 8 for fine-tuning.\n",
      "Experiment 9: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.3802, Train Acc: 0.8600 | Val Loss: 1.0891, Val Acc: 0.5117\n",
      "Epoch 2/10 | Train Loss: 0.3378, Train Acc: 0.8751 | Val Loss: 1.1574, Val Acc: 0.5137\n",
      "Epoch 3/10 | Train Loss: 0.3078, Train Acc: 0.8871 | Val Loss: 1.1232, Val Acc: 0.5109\n",
      "Epoch 4/10 | Train Loss: 0.2827, Train Acc: 0.8990 | Val Loss: 1.1012, Val Acc: 0.5121\n",
      "Epoch 5/10 | Train Loss: 0.2093, Train Acc: 0.9321 | Val Loss: 1.5246, Val Acc: 0.5109\n",
      "Epoch 6/10 | Train Loss: 0.1656, Train Acc: 0.9517 | Val Loss: 1.8756, Val Acc: 0.5040\n",
      "Epoch 7/10 | Train Loss: 0.1461, Train Acc: 0.9595 | Val Loss: 1.9350, Val Acc: 0.5166\n",
      "Epoch 8/10 | Train Loss: 0.1071, Train Acc: 0.9733 | Val Loss: 2.0833, Val Acc: 0.5097\n",
      "Epoch 9/10 | Train Loss: 0.0851, Train Acc: 0.9800 | Val Loss: 2.3906, Val Acc: 0.5077\n",
      "Epoch 10/10 | Train Loss: 0.0727, Train Acc: 0.9837 | Val Loss: 2.6977, Val Acc: 0.5073\n",
      "Saved model for experiment 9.\n",
      "Experiment 9: Final Validation Accuracy: 0.5073\n",
      "Experiment 9: Test Accuracy: 0.5190\n",
      "                         open      high       low    volume     value  \\\n",
      "2017-10-07 02:00:00  0.679612  0.662338  0.338346  0.003346  0.003359   \n",
      "2017-10-07 03:00:00  0.020202  0.298701  0.285714  0.002740  0.002707   \n",
      "2017-10-07 04:00:00  0.070707  0.298701  0.240602  0.003067  0.003032   \n",
      "2017-10-07 05:00:00  0.464646  0.519481  0.368421  0.003391  0.003412   \n",
      "2017-10-07 06:00:00  0.505050  0.636364  0.375940  0.002723  0.002738   \n",
      "...                       ...       ...       ...       ...       ...   \n",
      "2024-12-18 12:00:00  0.000000  0.000000  0.000000  0.442018  0.435797   \n",
      "2024-12-18 13:00:00  0.000000  0.000000  0.000000  1.000000  1.000000   \n",
      "2024-12-18 14:00:00  0.000000  0.000000  0.199575  0.298076  0.297948   \n",
      "2024-12-18 15:00:00  0.000000  0.000000  0.226983  0.109934  0.109102   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.000000  0.000000   \n",
      "\n",
      "                     William_R       ATR       OBV   Z_Score   Entropy  ...  \\\n",
      "2017-10-07 02:00:00   0.000000  1.000000  0.000000  0.000000  0.000000  ...   \n",
      "2017-10-07 03:00:00   0.437500  0.967643  0.005921  0.377739  0.001146  ...   \n",
      "2017-10-07 04:00:00   0.395833  0.974865  0.000000  0.319537  0.000000  ...   \n",
      "2017-10-07 05:00:00   0.437500  0.981571  0.006543  0.356728  0.000000  ...   \n",
      "2017-10-07 06:00:00   0.270833  1.000000  0.000640  0.192992  0.000000  ...   \n",
      "...                        ...       ...       ...       ...       ...  ...   \n",
      "2024-12-18 12:00:00   0.000000  0.883939  0.000000  0.000000  0.000000  ...   \n",
      "2024-12-18 13:00:00   0.184551  1.000000  0.000000  0.000000  0.000000  ...   \n",
      "2024-12-18 14:00:00   0.207054  1.000000  0.000000  0.083813  0.000000  ...   \n",
      "2024-12-18 15:00:00   0.245174  0.954989  0.047716  0.178420  0.000000  ...   \n",
      "2024-12-18 16:00:00   0.323835  0.870034  0.063031  0.264760  0.000000  ...   \n",
      "\n",
      "                     close_for_binning_Bin_118  close_for_binning_Bin_119  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_120  close_for_binning_Bin_121  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_122  close_for_binning_Bin_123  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        1.0   \n",
      "2024-12-18 14:00:00                        0.0                        1.0   \n",
      "2024-12-18 15:00:00                        0.0                        1.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_124  close_for_binning_Bin_125  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        1.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        1.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_126  close_for_binning_Bin_127  \n",
      "2017-10-07 02:00:00                        0.0                        0.0  \n",
      "2017-10-07 03:00:00                        0.0                        0.0  \n",
      "2017-10-07 04:00:00                        0.0                        0.0  \n",
      "2017-10-07 05:00:00                        0.0                        0.0  \n",
      "2017-10-07 06:00:00                        0.0                        0.0  \n",
      "...                                        ...                        ...  \n",
      "2024-12-18 12:00:00                        0.0                        0.0  \n",
      "2024-12-18 13:00:00                        0.0                        0.0  \n",
      "2024-12-18 14:00:00                        0.0                        0.0  \n",
      "2024-12-18 15:00:00                        0.0                        0.0  \n",
      "2024-12-18 16:00:00                        0.0                        0.0  \n",
      "\n",
      "[63011 rows x 4254 columns]\n",
      "Loaded model from experiment 9 for fine-tuning.\n",
      "Experiment 10: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.3614, Train Acc: 0.8715 | Val Loss: 1.1259, Val Acc: 0.5178\n",
      "Epoch 2/10 | Train Loss: 0.3210, Train Acc: 0.8801 | Val Loss: 1.2485, Val Acc: 0.5331\n",
      "Epoch 3/10 | Train Loss: 0.2745, Train Acc: 0.9000 | Val Loss: 1.3879, Val Acc: 0.5149\n",
      "Epoch 4/10 | Train Loss: 0.2398, Train Acc: 0.9137 | Val Loss: 1.4582, Val Acc: 0.5210\n",
      "Epoch 5/10 | Train Loss: 0.1564, Train Acc: 0.9509 | Val Loss: 1.9607, Val Acc: 0.5254\n",
      "Epoch 6/10 | Train Loss: 0.1271, Train Acc: 0.9640 | Val Loss: 2.1262, Val Acc: 0.5174\n",
      "Epoch 7/10 | Train Loss: 0.1053, Train Acc: 0.9706 | Val Loss: 2.1387, Val Acc: 0.5206\n",
      "Epoch 8/10 | Train Loss: 0.0642, Train Acc: 0.9857 | Val Loss: 2.2666, Val Acc: 0.5190\n",
      "Epoch 9/10 | Train Loss: 0.0484, Train Acc: 0.9894 | Val Loss: 2.5266, Val Acc: 0.5113\n",
      "Epoch 10/10 | Train Loss: 0.0378, Train Acc: 0.9923 | Val Loss: 2.8108, Val Acc: 0.5141\n",
      "Saved model for experiment 10.\n",
      "Experiment 10: Final Validation Accuracy: 0.5141\n",
      "Experiment 10: Test Accuracy: 0.4980\n",
      "                         open      high       low    volume     value  \\\n",
      "2017-10-07 02:00:00  0.679612  0.662338  0.338346  0.003346  0.003359   \n",
      "2017-10-07 03:00:00  0.020202  0.298701  0.285714  0.002740  0.002707   \n",
      "2017-10-07 04:00:00  0.070707  0.298701  0.240602  0.003067  0.003032   \n",
      "2017-10-07 05:00:00  0.464646  0.519481  0.368421  0.003391  0.003412   \n",
      "2017-10-07 06:00:00  0.505050  0.636364  0.375940  0.002723  0.002738   \n",
      "...                       ...       ...       ...       ...       ...   \n",
      "2024-12-18 12:00:00  0.000000  0.000000  0.000000  0.442018  0.435797   \n",
      "2024-12-18 13:00:00  0.000000  0.000000  0.000000  1.000000  1.000000   \n",
      "2024-12-18 14:00:00  0.000000  0.000000  0.199575  0.298076  0.297948   \n",
      "2024-12-18 15:00:00  0.000000  0.000000  0.226983  0.109934  0.109102   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.000000  0.000000   \n",
      "\n",
      "                     William_R       ATR       OBV   Z_Score   Entropy  ...  \\\n",
      "2017-10-07 02:00:00   0.000000  1.000000  0.000000  0.000000  0.000000  ...   \n",
      "2017-10-07 03:00:00   0.437500  0.967643  0.005921  0.377739  0.001146  ...   \n",
      "2017-10-07 04:00:00   0.395833  0.974865  0.000000  0.319537  0.000000  ...   \n",
      "2017-10-07 05:00:00   0.437500  0.981571  0.006543  0.356728  0.000000  ...   \n",
      "2017-10-07 06:00:00   0.270833  1.000000  0.000640  0.192992  0.000000  ...   \n",
      "...                        ...       ...       ...       ...       ...  ...   \n",
      "2024-12-18 12:00:00   0.000000  0.883939  0.000000  0.000000  0.000000  ...   \n",
      "2024-12-18 13:00:00   0.184551  1.000000  0.000000  0.000000  0.000000  ...   \n",
      "2024-12-18 14:00:00   0.207054  1.000000  0.000000  0.083813  0.000000  ...   \n",
      "2024-12-18 15:00:00   0.245174  0.954989  0.047716  0.178420  0.000000  ...   \n",
      "2024-12-18 16:00:00   0.323835  0.870034  0.063031  0.264760  0.000000  ...   \n",
      "\n",
      "                     close_for_binning_Bin_118  close_for_binning_Bin_119  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_120  close_for_binning_Bin_121  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_122  close_for_binning_Bin_123  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        1.0   \n",
      "2024-12-18 14:00:00                        0.0                        1.0   \n",
      "2024-12-18 15:00:00                        0.0                        1.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_124  close_for_binning_Bin_125  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        1.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        1.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_126  close_for_binning_Bin_127  \n",
      "2017-10-07 02:00:00                        0.0                        0.0  \n",
      "2017-10-07 03:00:00                        0.0                        0.0  \n",
      "2017-10-07 04:00:00                        0.0                        0.0  \n",
      "2017-10-07 05:00:00                        0.0                        0.0  \n",
      "2017-10-07 06:00:00                        0.0                        0.0  \n",
      "...                                        ...                        ...  \n",
      "2024-12-18 12:00:00                        0.0                        0.0  \n",
      "2024-12-18 13:00:00                        0.0                        0.0  \n",
      "2024-12-18 14:00:00                        0.0                        0.0  \n",
      "2024-12-18 15:00:00                        0.0                        0.0  \n",
      "2024-12-18 16:00:00                        0.0                        0.0  \n",
      "\n",
      "[63011 rows x 4254 columns]\n",
      "Loaded model from experiment 10 for fine-tuning.\n",
      "Experiment 11: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.3285, Train Acc: 0.8880 | Val Loss: 1.2327, Val Acc: 0.4952\n",
      "Epoch 2/10 | Train Loss: 0.2846, Train Acc: 0.8979 | Val Loss: 1.3009, Val Acc: 0.5057\n",
      "Epoch 3/10 | Train Loss: 0.2464, Train Acc: 0.9129 | Val Loss: 1.4502, Val Acc: 0.5234\n",
      "Epoch 4/10 | Train Loss: 0.2207, Train Acc: 0.9245 | Val Loss: 1.7092, Val Acc: 0.5186\n",
      "Epoch 5/10 | Train Loss: 0.1438, Train Acc: 0.9582 | Val Loss: 2.1437, Val Acc: 0.5036\n",
      "Epoch 6/10 | Train Loss: 0.1139, Train Acc: 0.9691 | Val Loss: 2.0846, Val Acc: 0.5008\n",
      "Epoch 7/10 | Train Loss: 0.0918, Train Acc: 0.9764 | Val Loss: 2.3092, Val Acc: 0.5081\n",
      "Epoch 8/10 | Train Loss: 0.0551, Train Acc: 0.9872 | Val Loss: 2.8065, Val Acc: 0.5028\n",
      "Epoch 9/10 | Train Loss: 0.0398, Train Acc: 0.9916 | Val Loss: 2.9964, Val Acc: 0.4992\n",
      "Epoch 10/10 | Train Loss: 0.0318, Train Acc: 0.9935 | Val Loss: 2.9744, Val Acc: 0.5020\n",
      "Saved model for experiment 11.\n",
      "Experiment 11: Final Validation Accuracy: 0.5020\n",
      "Experiment 11: Test Accuracy: 0.5153\n",
      "                         open      high       low    volume     value  \\\n",
      "2017-10-07 02:00:00  0.679612  0.662338  0.338346  0.003346  0.003359   \n",
      "2017-10-07 03:00:00  0.020202  0.298701  0.285714  0.002740  0.002707   \n",
      "2017-10-07 04:00:00  0.070707  0.298701  0.240602  0.003067  0.003032   \n",
      "2017-10-07 05:00:00  0.464646  0.519481  0.368421  0.003391  0.003412   \n",
      "2017-10-07 06:00:00  0.505050  0.636364  0.375940  0.002723  0.002738   \n",
      "...                       ...       ...       ...       ...       ...   \n",
      "2024-12-18 12:00:00  0.000000  0.000000  0.000000  0.442018  0.435797   \n",
      "2024-12-18 13:00:00  0.000000  0.000000  0.000000  1.000000  1.000000   \n",
      "2024-12-18 14:00:00  0.000000  0.000000  0.199575  0.298076  0.297948   \n",
      "2024-12-18 15:00:00  0.000000  0.000000  0.226983  0.109934  0.109102   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.000000  0.000000   \n",
      "\n",
      "                     William_R       ATR       OBV   Z_Score   Entropy  ...  \\\n",
      "2017-10-07 02:00:00   0.000000  1.000000  0.000000  0.000000  0.000000  ...   \n",
      "2017-10-07 03:00:00   0.437500  0.967643  0.005921  0.377739  0.001146  ...   \n",
      "2017-10-07 04:00:00   0.395833  0.974865  0.000000  0.319537  0.000000  ...   \n",
      "2017-10-07 05:00:00   0.437500  0.981571  0.006543  0.356728  0.000000  ...   \n",
      "2017-10-07 06:00:00   0.270833  1.000000  0.000640  0.192992  0.000000  ...   \n",
      "...                        ...       ...       ...       ...       ...  ...   \n",
      "2024-12-18 12:00:00   0.000000  0.883939  0.000000  0.000000  0.000000  ...   \n",
      "2024-12-18 13:00:00   0.184551  1.000000  0.000000  0.000000  0.000000  ...   \n",
      "2024-12-18 14:00:00   0.207054  1.000000  0.000000  0.083813  0.000000  ...   \n",
      "2024-12-18 15:00:00   0.245174  0.954989  0.047716  0.178420  0.000000  ...   \n",
      "2024-12-18 16:00:00   0.323835  0.870034  0.063031  0.264760  0.000000  ...   \n",
      "\n",
      "                     close_for_binning_Bin_118  close_for_binning_Bin_119  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_120  close_for_binning_Bin_121  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_122  close_for_binning_Bin_123  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        1.0   \n",
      "2024-12-18 14:00:00                        0.0                        1.0   \n",
      "2024-12-18 15:00:00                        0.0                        1.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_124  close_for_binning_Bin_125  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        1.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        1.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_126  close_for_binning_Bin_127  \n",
      "2017-10-07 02:00:00                        0.0                        0.0  \n",
      "2017-10-07 03:00:00                        0.0                        0.0  \n",
      "2017-10-07 04:00:00                        0.0                        0.0  \n",
      "2017-10-07 05:00:00                        0.0                        0.0  \n",
      "2017-10-07 06:00:00                        0.0                        0.0  \n",
      "...                                        ...                        ...  \n",
      "2024-12-18 12:00:00                        0.0                        0.0  \n",
      "2024-12-18 13:00:00                        0.0                        0.0  \n",
      "2024-12-18 14:00:00                        0.0                        0.0  \n",
      "2024-12-18 15:00:00                        0.0                        0.0  \n",
      "2024-12-18 16:00:00                        0.0                        0.0  \n",
      "\n",
      "[63011 rows x 4254 columns]\n",
      "Loaded model from experiment 11 for fine-tuning.\n",
      "Experiment 12: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.3221, Train Acc: 0.8940 | Val Loss: 1.1983, Val Acc: 0.5190\n",
      "Epoch 2/10 | Train Loss: 0.2960, Train Acc: 0.8969 | Val Loss: 1.2618, Val Acc: 0.5202\n",
      "Epoch 3/10 | Train Loss: 0.2522, Train Acc: 0.9151 | Val Loss: 1.3488, Val Acc: 0.5101\n",
      "Epoch 4/10 | Train Loss: 0.2231, Train Acc: 0.9253 | Val Loss: 1.4722, Val Acc: 0.5182\n",
      "Epoch 5/10 | Train Loss: 0.1528, Train Acc: 0.9561 | Val Loss: 1.8469, Val Acc: 0.5174\n",
      "Epoch 6/10 | Train Loss: 0.1144, Train Acc: 0.9714 | Val Loss: 2.0313, Val Acc: 0.5254\n",
      "Epoch 7/10 | Train Loss: 0.1023, Train Acc: 0.9756 | Val Loss: 2.0910, Val Acc: 0.5202\n",
      "Epoch 8/10 | Train Loss: 0.0682, Train Acc: 0.9852 | Val Loss: 2.3741, Val Acc: 0.5287\n",
      "Epoch 9/10 | Train Loss: 0.0587, Train Acc: 0.9874 | Val Loss: 2.5497, Val Acc: 0.5210\n",
      "Epoch 10/10 | Train Loss: 0.0478, Train Acc: 0.9902 | Val Loss: 2.4268, Val Acc: 0.5242\n",
      "Saved model for experiment 12.\n",
      "Experiment 12: Final Validation Accuracy: 0.5242\n",
      "Experiment 12: Test Accuracy: 0.5129\n",
      "                         open      high       low    volume     value  \\\n",
      "2017-10-07 02:00:00  0.679612  0.662338  0.338346  0.003346  0.003359   \n",
      "2017-10-07 03:00:00  0.020202  0.298701  0.285714  0.002740  0.002707   \n",
      "2017-10-07 04:00:00  0.070707  0.298701  0.240602  0.003067  0.003032   \n",
      "2017-10-07 05:00:00  0.464646  0.519481  0.368421  0.003391  0.003412   \n",
      "2017-10-07 06:00:00  0.505050  0.636364  0.375940  0.002723  0.002738   \n",
      "...                       ...       ...       ...       ...       ...   \n",
      "2024-12-18 12:00:00  0.000000  0.000000  0.000000  0.442018  0.435797   \n",
      "2024-12-18 13:00:00  0.000000  0.000000  0.000000  1.000000  1.000000   \n",
      "2024-12-18 14:00:00  0.000000  0.000000  0.199575  0.298076  0.297948   \n",
      "2024-12-18 15:00:00  0.000000  0.000000  0.226983  0.109934  0.109102   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.000000  0.000000   \n",
      "\n",
      "                     William_R       ATR       OBV   Z_Score   Entropy  ...  \\\n",
      "2017-10-07 02:00:00   0.000000  1.000000  0.000000  0.000000  0.000000  ...   \n",
      "2017-10-07 03:00:00   0.437500  0.967643  0.005921  0.377739  0.001146  ...   \n",
      "2017-10-07 04:00:00   0.395833  0.974865  0.000000  0.319537  0.000000  ...   \n",
      "2017-10-07 05:00:00   0.437500  0.981571  0.006543  0.356728  0.000000  ...   \n",
      "2017-10-07 06:00:00   0.270833  1.000000  0.000640  0.192992  0.000000  ...   \n",
      "...                        ...       ...       ...       ...       ...  ...   \n",
      "2024-12-18 12:00:00   0.000000  0.883939  0.000000  0.000000  0.000000  ...   \n",
      "2024-12-18 13:00:00   0.184551  1.000000  0.000000  0.000000  0.000000  ...   \n",
      "2024-12-18 14:00:00   0.207054  1.000000  0.000000  0.083813  0.000000  ...   \n",
      "2024-12-18 15:00:00   0.245174  0.954989  0.047716  0.178420  0.000000  ...   \n",
      "2024-12-18 16:00:00   0.323835  0.870034  0.063031  0.264760  0.000000  ...   \n",
      "\n",
      "                     close_for_binning_Bin_118  close_for_binning_Bin_119  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_120  close_for_binning_Bin_121  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_122  close_for_binning_Bin_123  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        1.0   \n",
      "2024-12-18 14:00:00                        0.0                        1.0   \n",
      "2024-12-18 15:00:00                        0.0                        1.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_124  close_for_binning_Bin_125  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        1.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        1.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_126  close_for_binning_Bin_127  \n",
      "2017-10-07 02:00:00                        0.0                        0.0  \n",
      "2017-10-07 03:00:00                        0.0                        0.0  \n",
      "2017-10-07 04:00:00                        0.0                        0.0  \n",
      "2017-10-07 05:00:00                        0.0                        0.0  \n",
      "2017-10-07 06:00:00                        0.0                        0.0  \n",
      "...                                        ...                        ...  \n",
      "2024-12-18 12:00:00                        0.0                        0.0  \n",
      "2024-12-18 13:00:00                        0.0                        0.0  \n",
      "2024-12-18 14:00:00                        0.0                        0.0  \n",
      "2024-12-18 15:00:00                        0.0                        0.0  \n",
      "2024-12-18 16:00:00                        0.0                        0.0  \n",
      "\n",
      "[63011 rows x 4254 columns]\n",
      "Loaded model from experiment 12 for fine-tuning.\n",
      "Experiment 13: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.3155, Train Acc: 0.8928 | Val Loss: 1.2431, Val Acc: 0.5113\n",
      "Epoch 2/10 | Train Loss: 0.2770, Train Acc: 0.9067 | Val Loss: 1.2524, Val Acc: 0.5186\n",
      "Epoch 3/10 | Train Loss: 0.2495, Train Acc: 0.9164 | Val Loss: 1.4211, Val Acc: 0.5194\n",
      "Epoch 4/10 | Train Loss: 0.2215, Train Acc: 0.9298 | Val Loss: 1.4440, Val Acc: 0.5210\n",
      "Epoch 5/10 | Train Loss: 0.1561, Train Acc: 0.9561 | Val Loss: 1.7271, Val Acc: 0.5153\n",
      "Epoch 6/10 | Train Loss: 0.1226, Train Acc: 0.9696 | Val Loss: 1.8357, Val Acc: 0.5077\n",
      "Epoch 7/10 | Train Loss: 0.1039, Train Acc: 0.9758 | Val Loss: 2.1976, Val Acc: 0.5081\n",
      "Epoch 8/10 | Train Loss: 0.0781, Train Acc: 0.9832 | Val Loss: 2.0871, Val Acc: 0.5048\n",
      "Epoch 9/10 | Train Loss: 0.0663, Train Acc: 0.9865 | Val Loss: 2.3884, Val Acc: 0.5101\n",
      "Epoch 10/10 | Train Loss: 0.0589, Train Acc: 0.9878 | Val Loss: 2.5393, Val Acc: 0.4992\n",
      "Saved model for experiment 13.\n",
      "Experiment 13: Final Validation Accuracy: 0.4992\n",
      "Experiment 13: Test Accuracy: 0.4988\n",
      "                         open      high       low    volume     value  \\\n",
      "2017-10-07 02:00:00  0.679612  0.662338  0.338346  0.003346  0.003359   \n",
      "2017-10-07 03:00:00  0.020202  0.298701  0.285714  0.002740  0.002707   \n",
      "2017-10-07 04:00:00  0.070707  0.298701  0.240602  0.003067  0.003032   \n",
      "2017-10-07 05:00:00  0.464646  0.519481  0.368421  0.003391  0.003412   \n",
      "2017-10-07 06:00:00  0.505050  0.636364  0.375940  0.002723  0.002738   \n",
      "...                       ...       ...       ...       ...       ...   \n",
      "2024-12-18 12:00:00  0.000000  0.000000  0.000000  0.442018  0.435797   \n",
      "2024-12-18 13:00:00  0.000000  0.000000  0.000000  1.000000  1.000000   \n",
      "2024-12-18 14:00:00  0.000000  0.000000  0.199575  0.298076  0.297948   \n",
      "2024-12-18 15:00:00  0.000000  0.000000  0.226983  0.109934  0.109102   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.000000  0.000000   \n",
      "\n",
      "                     William_R       ATR       OBV   Z_Score   Entropy  ...  \\\n",
      "2017-10-07 02:00:00   0.000000  1.000000  0.000000  0.000000  0.000000  ...   \n",
      "2017-10-07 03:00:00   0.437500  0.967643  0.005921  0.377739  0.001146  ...   \n",
      "2017-10-07 04:00:00   0.395833  0.974865  0.000000  0.319537  0.000000  ...   \n",
      "2017-10-07 05:00:00   0.437500  0.981571  0.006543  0.356728  0.000000  ...   \n",
      "2017-10-07 06:00:00   0.270833  1.000000  0.000640  0.192992  0.000000  ...   \n",
      "...                        ...       ...       ...       ...       ...  ...   \n",
      "2024-12-18 12:00:00   0.000000  0.883939  0.000000  0.000000  0.000000  ...   \n",
      "2024-12-18 13:00:00   0.184551  1.000000  0.000000  0.000000  0.000000  ...   \n",
      "2024-12-18 14:00:00   0.207054  1.000000  0.000000  0.083813  0.000000  ...   \n",
      "2024-12-18 15:00:00   0.245174  0.954989  0.047716  0.178420  0.000000  ...   \n",
      "2024-12-18 16:00:00   0.323835  0.870034  0.063031  0.264760  0.000000  ...   \n",
      "\n",
      "                     close_for_binning_Bin_118  close_for_binning_Bin_119  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_120  close_for_binning_Bin_121  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_122  close_for_binning_Bin_123  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        1.0   \n",
      "2024-12-18 14:00:00                        0.0                        1.0   \n",
      "2024-12-18 15:00:00                        0.0                        1.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_124  close_for_binning_Bin_125  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        1.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        1.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_126  close_for_binning_Bin_127  \n",
      "2017-10-07 02:00:00                        0.0                        0.0  \n",
      "2017-10-07 03:00:00                        0.0                        0.0  \n",
      "2017-10-07 04:00:00                        0.0                        0.0  \n",
      "2017-10-07 05:00:00                        0.0                        0.0  \n",
      "2017-10-07 06:00:00                        0.0                        0.0  \n",
      "...                                        ...                        ...  \n",
      "2024-12-18 12:00:00                        0.0                        0.0  \n",
      "2024-12-18 13:00:00                        0.0                        0.0  \n",
      "2024-12-18 14:00:00                        0.0                        0.0  \n",
      "2024-12-18 15:00:00                        0.0                        0.0  \n",
      "2024-12-18 16:00:00                        0.0                        0.0  \n",
      "\n",
      "[63011 rows x 4254 columns]\n",
      "Loaded model from experiment 13 for fine-tuning.\n",
      "Experiment 14: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.3363, Train Acc: 0.8871 | Val Loss: 1.2097, Val Acc: 0.4960\n",
      "Epoch 2/10 | Train Loss: 0.2674, Train Acc: 0.9109 | Val Loss: 1.2480, Val Acc: 0.5044\n",
      "Epoch 3/10 | Train Loss: 0.2505, Train Acc: 0.9167 | Val Loss: 1.3850, Val Acc: 0.5077\n",
      "Epoch 4/10 | Train Loss: 0.2287, Train Acc: 0.9255 | Val Loss: 1.5407, Val Acc: 0.5032\n",
      "Epoch 5/10 | Train Loss: 0.1594, Train Acc: 0.9553 | Val Loss: 1.7460, Val Acc: 0.4931\n",
      "Epoch 6/10 | Train Loss: 0.1257, Train Acc: 0.9676 | Val Loss: 1.9391, Val Acc: 0.4907\n",
      "Epoch 7/10 | Train Loss: 0.1126, Train Acc: 0.9710 | Val Loss: 2.1010, Val Acc: 0.4952\n",
      "Epoch 8/10 | Train Loss: 0.0796, Train Acc: 0.9822 | Val Loss: 2.2414, Val Acc: 0.4935\n",
      "Epoch 9/10 | Train Loss: 0.0651, Train Acc: 0.9864 | Val Loss: 2.4148, Val Acc: 0.4972\n",
      "Epoch 10/10 | Train Loss: 0.0567, Train Acc: 0.9885 | Val Loss: 2.5369, Val Acc: 0.4980\n",
      "Saved model for experiment 14.\n",
      "Experiment 14: Final Validation Accuracy: 0.4980\n",
      "Experiment 14: Test Accuracy: 0.5174\n",
      "                         open      high       low    volume     value  \\\n",
      "2017-10-07 02:00:00  0.679612  0.662338  0.338346  0.003346  0.003359   \n",
      "2017-10-07 03:00:00  0.020202  0.298701  0.285714  0.002740  0.002707   \n",
      "2017-10-07 04:00:00  0.070707  0.298701  0.240602  0.003067  0.003032   \n",
      "2017-10-07 05:00:00  0.464646  0.519481  0.368421  0.003391  0.003412   \n",
      "2017-10-07 06:00:00  0.505050  0.636364  0.375940  0.002723  0.002738   \n",
      "...                       ...       ...       ...       ...       ...   \n",
      "2024-12-18 12:00:00  0.000000  0.000000  0.000000  0.442018  0.435797   \n",
      "2024-12-18 13:00:00  0.000000  0.000000  0.000000  1.000000  1.000000   \n",
      "2024-12-18 14:00:00  0.000000  0.000000  0.199575  0.298076  0.297948   \n",
      "2024-12-18 15:00:00  0.000000  0.000000  0.226983  0.109934  0.109102   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.000000  0.000000   \n",
      "\n",
      "                     William_R       ATR       OBV   Z_Score   Entropy  ...  \\\n",
      "2017-10-07 02:00:00   0.000000  1.000000  0.000000  0.000000  0.000000  ...   \n",
      "2017-10-07 03:00:00   0.437500  0.967643  0.005921  0.377739  0.001146  ...   \n",
      "2017-10-07 04:00:00   0.395833  0.974865  0.000000  0.319537  0.000000  ...   \n",
      "2017-10-07 05:00:00   0.437500  0.981571  0.006543  0.356728  0.000000  ...   \n",
      "2017-10-07 06:00:00   0.270833  1.000000  0.000640  0.192992  0.000000  ...   \n",
      "...                        ...       ...       ...       ...       ...  ...   \n",
      "2024-12-18 12:00:00   0.000000  0.883939  0.000000  0.000000  0.000000  ...   \n",
      "2024-12-18 13:00:00   0.184551  1.000000  0.000000  0.000000  0.000000  ...   \n",
      "2024-12-18 14:00:00   0.207054  1.000000  0.000000  0.083813  0.000000  ...   \n",
      "2024-12-18 15:00:00   0.245174  0.954989  0.047716  0.178420  0.000000  ...   \n",
      "2024-12-18 16:00:00   0.323835  0.870034  0.063031  0.264760  0.000000  ...   \n",
      "\n",
      "                     close_for_binning_Bin_118  close_for_binning_Bin_119  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_120  close_for_binning_Bin_121  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_122  close_for_binning_Bin_123  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        0.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        1.0   \n",
      "2024-12-18 14:00:00                        0.0                        1.0   \n",
      "2024-12-18 15:00:00                        0.0                        1.0   \n",
      "2024-12-18 16:00:00                        0.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_124  close_for_binning_Bin_125  \\\n",
      "2017-10-07 02:00:00                        0.0                        0.0   \n",
      "2017-10-07 03:00:00                        0.0                        0.0   \n",
      "2017-10-07 04:00:00                        0.0                        0.0   \n",
      "2017-10-07 05:00:00                        0.0                        0.0   \n",
      "2017-10-07 06:00:00                        0.0                        0.0   \n",
      "...                                        ...                        ...   \n",
      "2024-12-18 12:00:00                        1.0                        0.0   \n",
      "2024-12-18 13:00:00                        0.0                        0.0   \n",
      "2024-12-18 14:00:00                        0.0                        0.0   \n",
      "2024-12-18 15:00:00                        0.0                        0.0   \n",
      "2024-12-18 16:00:00                        1.0                        0.0   \n",
      "\n",
      "                     close_for_binning_Bin_126  close_for_binning_Bin_127  \n",
      "2017-10-07 02:00:00                        0.0                        0.0  \n",
      "2017-10-07 03:00:00                        0.0                        0.0  \n",
      "2017-10-07 04:00:00                        0.0                        0.0  \n",
      "2017-10-07 05:00:00                        0.0                        0.0  \n",
      "2017-10-07 06:00:00                        0.0                        0.0  \n",
      "...                                        ...                        ...  \n",
      "2024-12-18 12:00:00                        0.0                        0.0  \n",
      "2024-12-18 13:00:00                        0.0                        0.0  \n",
      "2024-12-18 14:00:00                        0.0                        0.0  \n",
      "2024-12-18 15:00:00                        0.0                        0.0  \n",
      "2024-12-18 16:00:00                        0.0                        0.0  \n",
      "\n",
      "[63011 rows x 4254 columns]\n",
      "Loaded model from experiment 14 for fine-tuning.\n",
      "Experiment 15: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.3188, Train Acc: 0.8929 | Val Loss: 1.1550, Val Acc: 0.5283\n",
      "Epoch 2/10 | Train Loss: 0.2688, Train Acc: 0.9069 | Val Loss: 1.4073, Val Acc: 0.5214\n",
      "Epoch 3/10 | Train Loss: 0.2309, Train Acc: 0.9227 | Val Loss: 1.4230, Val Acc: 0.5182\n",
      "Epoch 4/10 | Train Loss: 0.2059, Train Acc: 0.9305 | Val Loss: 1.6834, Val Acc: 0.5194\n",
      "Epoch 5/10 | Train Loss: 0.1364, Train Acc: 0.9595 | Val Loss: 1.9137, Val Acc: 0.5295\n",
      "Epoch 6/10 | Train Loss: 0.1024, Train Acc: 0.9727 | Val Loss: 2.1473, Val Acc: 0.5230\n",
      "Epoch 7/10 | Train Loss: 0.0788, Train Acc: 0.9815 | Val Loss: 2.2744, Val Acc: 0.5170\n",
      "Epoch 8/10 | Train Loss: 0.0488, Train Acc: 0.9893 | Val Loss: 2.5319, Val Acc: 0.5198\n",
      "Epoch 9/10 | Train Loss: 0.0361, Train Acc: 0.9928 | Val Loss: 2.8635, Val Acc: 0.5214\n",
      "Epoch 10/10 | Train Loss: 0.0287, Train Acc: 0.9946 | Val Loss: 2.9897, Val Acc: 0.5174\n",
      "Saved model for experiment 15.\n",
      "Experiment 15: Final Validation Accuracy: 0.5174\n",
      "Experiment 15: Test Accuracy: 0.5109\n",
      "\n",
      "Final Average Validation Accuracy: 0.5107\n",
      "Final Average Test Accuracy: 0.5119\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta as ta\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from math import sqrt\n",
    "\n",
    "####################################\n",
    "# 1. 기술적 지표 계산 함수 (변경 없음)\n",
    "####################################\n",
    "def calculate_indicators(data):\n",
    "    data['William_R'] = ta.willr(data['high'], data['low'], data['close'])\n",
    "    data['ATR'] = ta.atr(data['high'], data['low'], data['close'])\n",
    "    data['OBV'] = ta.obv(data['close'], data['volume'])\n",
    "    data['Z_Score'] = (data['close'] - data['close'].rolling(window=20).mean()) / data['close'].rolling(window=20).std()\n",
    "    data['Entropy'] = ta.entropy(data['close'], length=14)\n",
    "    data['SMA_5'] = data['close'].rolling(window=5).mean()\n",
    "    data['SMA_10'] = data['close'].rolling(window=10).mean()\n",
    "    data['SMA_20'] = data['close'].rolling(window=20).mean()\n",
    "    data['SMA_60'] = data['close'].rolling(window=60).mean()\n",
    "    data['SMA_120'] = data['close'].rolling(window=120).mean()\n",
    "    data['SMA_250'] = data['close'].rolling(window=250).mean()\n",
    "    data['RSI'] = ta.rsi(data['close'])\n",
    "    bb = ta.bbands(data['close'])\n",
    "    data['BB_Upper'], data['BB_Middle'], data['BB_Lower'] = bb.iloc[:, 0], bb.iloc[:, 1], bb.iloc[:, 2]\n",
    "    macd = ta.macd(data['close'])\n",
    "    data['MACD'] = macd.iloc[:, 0]\n",
    "    data['Stochastic'] = ta.stoch(data['high'], data['low'], data['close']).iloc[:, 0]\n",
    "    return data.dropna()\n",
    "\n",
    "####################################\n",
    "# 1-2. 추가 feature 계산 (가격 차이)\n",
    "####################################\n",
    "def calculate_price_differences(data):\n",
    "    data['close_open'] = data['close'] - data['open']\n",
    "    data['high_low'] = data['high'] - data['low']\n",
    "    data['high_open'] = data['high'] - data['open']\n",
    "    data['high_close'] = data['high'] - data['close']\n",
    "    data['open_low'] = data['open'] - data['low']\n",
    "    data['close_low'] = data['close'] - data['low']\n",
    "    return data\n",
    "\n",
    "####################################\n",
    "# 2. Datetime Feature One-Hot Encoding (각 feature 128차원)\n",
    "####################################\n",
    "def encode_datetime_features_onehot(data, dim=128):\n",
    "    if 'datetime' not in data.columns:\n",
    "        data['datetime'] = pd.to_datetime(data.index)\n",
    "    \n",
    "    data['hour_of_day'] = data['datetime'].dt.hour\n",
    "    data['day_of_week'] = data['datetime'].dt.dayofweek\n",
    "    data['week_of_month'] = (data['datetime'].dt.day - 1) // 7 + 1\n",
    "    data['month'] = data['datetime'].dt.month\n",
    "\n",
    "    def onehot_with_fixed_dim(series, prefix, dim):\n",
    "        dummies = pd.get_dummies(series, prefix=prefix)\n",
    "        expected_cols = [f\"{prefix}_{i}\" for i in range(dim)]\n",
    "        dummies = dummies.reindex(columns=expected_cols, fill_value=0)\n",
    "        return dummies\n",
    "\n",
    "    hour_one_hot = onehot_with_fixed_dim(data['hour_of_day'], 'Hour', dim)\n",
    "    day_one_hot = onehot_with_fixed_dim(data['day_of_week'], 'Day', dim)\n",
    "    week_one_hot = onehot_with_fixed_dim(data['week_of_month'], 'Week', dim)\n",
    "    month_one_hot = onehot_with_fixed_dim(data['month'], 'Month', dim)\n",
    "    \n",
    "    data = pd.concat([data, hour_one_hot, day_one_hot, week_one_hot, month_one_hot], axis=1)\n",
    "    return data\n",
    "\n",
    "####################################\n",
    "# 3. Rolling MinMax Scaling (분모 0 방지)\n",
    "####################################\n",
    "def rolling_minmax_scale(series, window=24):\n",
    "    roll_min = series.rolling(window=window, min_periods=window).min()\n",
    "    roll_max = series.rolling(window=window, min_periods=window).max()\n",
    "    scaled = (series - roll_min) / ((roll_max - roll_min) + 1e-8)\n",
    "    return scaled\n",
    "\n",
    "####################################\n",
    "# 4. Binning 후 One-Hot 인코딩 (각 feature를 128차원으로)\n",
    "####################################\n",
    "def bin_and_encode(data, features, bins=128, drop_original=True):\n",
    "    for feature in features:\n",
    "        data[f'{feature}_Bin'] = pd.cut(data[feature], bins=bins, labels=False)\n",
    "        one_hot = pd.get_dummies(data[f'{feature}_Bin'], prefix=f'{feature}_Bin')\n",
    "        expected_columns = [f'{feature}_Bin_{i}' for i in range(bins)]\n",
    "        one_hot = one_hot.reindex(columns=expected_columns, fill_value=0)\n",
    "        data = pd.concat([data, one_hot], axis=1)\n",
    "        if drop_original:\n",
    "            data.drop(columns=[f'{feature}_Bin'], inplace=True)\n",
    "    data = data.astype(np.float32)\n",
    "    return data\n",
    "\n",
    "####################################\n",
    "# 5. 데이터 로드 및 전처리\n",
    "####################################\n",
    "data = pd.read_csv(\"BTC_upbit_KRW_min60.csv\", index_col=0)\n",
    "data.columns = ['open', 'high', 'low', 'close', 'volume', 'value']\n",
    "data.index = pd.to_datetime(data.index)\n",
    "data = calculate_indicators(data)\n",
    "data = calculate_price_differences(data)   # 추가 feature 계산\n",
    "data = encode_datetime_features_onehot(data, dim=128)\n",
    "\n",
    "# 기존 feature와 추가한 가격 차이 feature들을 사용\n",
    "features_to_bin = ['open', 'high', 'low', 'volume', 'value', 'William_R',\n",
    "                   'ATR', 'OBV', 'Z_Score', 'Entropy', 'SMA_5', 'SMA_10', \n",
    "                   'SMA_20', 'SMA_60', 'SMA_120', 'SMA_250', 'RSI', 'BB_Upper', 'BB_Middle', \n",
    "                   'BB_Lower', 'MACD', 'Stochastic',\n",
    "                   'close_open', 'high_low', 'high_open', 'high_close', 'open_low', 'close_low']\n",
    "\n",
    "# datetime one-hot 컬럼: 이미 128차원씩 각 4개 (총 512차원)\n",
    "datetime_onehot_features = [col for col in data.columns if col.startswith('Hour_') or \n",
    "                              col.startswith('Day_') or col.startswith('Week_') or \n",
    "                              col.startswith('Month_')]\n",
    "\n",
    "# 타깃으로 사용할 close 값을 보존 (continuous)\n",
    "data['close_target'] = data['close']\n",
    "\n",
    "# 최종 데이터 선택 (여기서 close_target은 타깃으로만 사용)\n",
    "data = data[features_to_bin + ['close_target'] + datetime_onehot_features].dropna()\n",
    "\n",
    "# 각 기술적 및 가격 차이 feature에 대해 rolling scaling 적용 (window=24)\n",
    "for feature in features_to_bin:\n",
    "    data[feature] = rolling_minmax_scale(data[feature], window=24)\n",
    "data = data.dropna()\n",
    "\n",
    "# 기술적 및 가격 차이 feature들을 128차원의 one-hot 벡터로 변환\n",
    "data = bin_and_encode(data, features_to_bin, bins=128, drop_original=True)\n",
    "# close_target에 대해서도 one-hot 인코딩 (128차원) 보조 feature로 사용 가능하지만,\n",
    "# 모델 입력에서는 close_target (continuous)은 제외할 예정\n",
    "data['close_for_binning'] = data['close_target']\n",
    "data = bin_and_encode(data, ['close_for_binning'], bins=128, drop_original=False)\n",
    "data.drop(columns=['close_for_binning'], inplace=True)\n",
    "\n",
    "# 최종 입력 데이터: 오직 one-hot 인코딩된 feature만 사용 (continuous close_target은 타깃으로만 사용)\n",
    "final_input_columns = []\n",
    "for feature in features_to_bin:\n",
    "    final_input_columns.extend([f'{feature}_Bin_{i}' for i in range(128)])\n",
    "final_input_columns.extend(datetime_onehot_features)\n",
    "\n",
    "# 타깃 데이터: continuous close_target\n",
    "final_target_column = ['close_target']\n",
    "\n",
    "data_input = data[final_input_columns]\n",
    "data_target = data[final_target_column]\n",
    "\n",
    "####################################\n",
    "# 6-2. Dataset 정의 (입력과 타깃을 별도로 사용)\n",
    "####################################\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, input_data, target_data, lookback=24):\n",
    "        self.input_data = input_data.values\n",
    "        self.target_data = target_data.values  # shape: (N, 1)\n",
    "        self.lookback = lookback\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_data) - self.lookback\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.input_data[idx: idx + self.lookback, :]\n",
    "        y = self.target_data[idx + self.lookback, 0]\n",
    "        y_prev = self.target_data[idx + self.lookback - 1, 0]\n",
    "        y_target = 1 if y > y_prev else 0\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y_target, dtype=torch.long)\n",
    "\n",
    "####################################\n",
    "# 7. Transformer Encoder 직접 구현\n",
    "####################################\n",
    "# 7-1. Multi-Head Self-Attention\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == embed_dim, \"embed_dim은 num_heads로 나누어떨어져야 합니다.\"\n",
    "        \n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key   = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out   = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        \n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / sqrt(self.head_dim)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, V)\n",
    "        \n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "\n",
    "# 7-2. Feed-Forward Network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ffn_dim):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, ffn_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(ffn_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "# 7-3. Transformer Encoder Layer (Self-Attention + FFN + Residual + LayerNorm)\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = FeedForward(embed_dim, ffn_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attn_out = self.self_attn(x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_out)\n",
    "        return x\n",
    "\n",
    "# 7-4. Encoder-Only Transformer 직접 구현 (lookback=24이므로, max_seq_len=24)\n",
    "class EncoderOnlyTransformerCustom(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=512, num_heads=8, num_layers=6, ffn_dim=2048, num_classes=2, max_seq_len=24):\n",
    "        super(EncoderOnlyTransformerCustom, self).__init__()\n",
    "        self.token_embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embedding_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(embedding_dim, num_heads, ffn_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x = self.token_embedding(x)\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        x = x + self.position_embedding(positions)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x[:, -1, :]\n",
    "        return self.fc(x)\n",
    "\n",
    "####################################\n",
    "# 8. 학습 및 평가 루프 (Fine-tuning 및 Validation Accuracy 출력)\n",
    "####################################\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return total_loss / len(data_loader), correct / total\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, lr, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_state = None\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, device)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = model.state_dict()\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(data, num_experiments=16, lookback=24, num_epochs=10):\n",
    "    # 모델 입력은 one-hot 인코딩된 feature들만, 타깃은 continuous close_target\n",
    "    input_cols = []\n",
    "    for feature in features_to_bin:\n",
    "        input_cols.extend([f'{feature}_Bin_{i}' for i in range(128)])\n",
    "    input_cols.extend(datetime_onehot_features)\n",
    "    target_cols = ['close_target']\n",
    "    \n",
    "    data_input = data[input_cols]\n",
    "    data_target = data[target_cols]\n",
    "    \n",
    "    step_size = 2500  # 이동 단위\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    val_acc_list = []\n",
    "    test_acc_list = []\n",
    "    \n",
    "    for exp in range(num_experiments):\n",
    "        train_start = exp * step_size\n",
    "        train_end = train_start + step_size * 8\n",
    "        val_end = train_end + step_size\n",
    "        test_end = val_end + step_size\n",
    "        if test_end > len(data_input):\n",
    "            break\n",
    "        print(data)\n",
    "        \n",
    "        train_input = data_input.iloc[train_start:train_end]\n",
    "        train_target = data_target.iloc[train_start:train_end]\n",
    "        val_input = data_input.iloc[train_end:val_end]\n",
    "        val_target = data_target.iloc[train_end:val_end]\n",
    "        test_input = data_input.iloc[val_end:test_end]\n",
    "        test_target = data_target.iloc[val_end:test_end]\n",
    "        \n",
    "        # Dataset 생성: 입력과 타깃을 결합하여 사용\n",
    "        train_dataset = TimeSeriesDataset(train_input, train_target, lookback=lookback)\n",
    "        val_dataset = TimeSeriesDataset(val_input, val_target, lookback=lookback)\n",
    "        test_dataset = TimeSeriesDataset(test_input, test_target, lookback=lookback)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        # Fine-tuning: 이전 구간의 모델 파라미터를 그대로 로드하여 미세 조정\n",
    "        lr = 1e-4\n",
    "        input_dim = data_input.shape[1]\n",
    "        model = EncoderOnlyTransformerCustom(input_dim=input_dim, embedding_dim=512, num_heads=8, \n",
    "                                               num_layers=6, ffn_dim=2048, num_classes=2, max_seq_len=lookback).to(device)\n",
    "        model_path = f\"model_experiment_{exp}.pth\"\n",
    "        if exp > 0:\n",
    "            try:\n",
    "                model.load_state_dict(torch.load(f\"model_experiment_{exp - 1}.pth\"))\n",
    "                print(f\"Loaded model from experiment {exp - 1} for fine-tuning.\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Model file for experiment {exp - 1} not found. Starting fresh training.\")\n",
    "        \n",
    "        print(f\"Experiment {exp}: Training with lr={lr} (Fine-Tuning)\")\n",
    "        model = train_model(model, train_loader, val_loader, num_epochs, lr, device)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Saved model for experiment {exp}.\")\n",
    "        \n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, device)\n",
    "        val_acc_list.append(val_acc)\n",
    "        print(f\"Experiment {exp}: Final Validation Accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "        test_loss, test_acc = evaluate_model(model, test_loader, device)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(f\"Experiment {exp}: Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    avg_val_acc = sum(val_acc_list) / len(val_acc_list)\n",
    "    avg_test_acc = sum(test_acc_list) / len(test_acc_list)\n",
    "    print(f\"\\nFinal Average Validation Accuracy: {avg_val_acc:.4f}\")\n",
    "    print(f\"Final Average Test Accuracy: {avg_test_acc:.4f}\")\n",
    "\n",
    "train_and_evaluate(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'close'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'close'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 386\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFinal Average Validation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_val_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Average Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_test_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 386\u001b[0m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 319\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(data, num_experiments, lookback, num_epochs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;66;03m# 타깃 컬럼 추가: 원본 open, close 값을 보존 (continuous)\u001b[39;00m\n\u001b[0;32m    318\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopen_target\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopen\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 319\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclose_target\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclose\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    321\u001b[0m data_input \u001b[38;5;241m=\u001b[39m data[input_cols]\n\u001b[0;32m    322\u001b[0m data_target \u001b[38;5;241m=\u001b[39m data[target_cols]\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'close'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta as ta\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from math import sqrt\n",
    "\n",
    "####################################\n",
    "# 1. 기술적 지표 계산 함수 (변경 없음)\n",
    "####################################\n",
    "def calculate_indicators(data):\n",
    "    data['William_R'] = ta.willr(data['high'], data['low'], data['close'])\n",
    "    data['ATR'] = ta.atr(data['high'], data['low'], data['close'])\n",
    "    data['OBV'] = ta.obv(data['close'], data['volume'])\n",
    "    data['Z_Score'] = (data['close'] - data['close'].rolling(window=20).mean()) / data['close'].rolling(window=20).std()\n",
    "    data['Entropy'] = ta.entropy(data['close'], length=14)\n",
    "    data['SMA_5'] = data['close'].rolling(window=5).mean()\n",
    "    data['SMA_10'] = data['close'].rolling(window=10).mean()\n",
    "    data['SMA_20'] = data['close'].rolling(window=20).mean()\n",
    "    data['SMA_60'] = data['close'].rolling(window=60).mean()\n",
    "    data['SMA_120'] = data['close'].rolling(window=120).mean()\n",
    "    data['SMA_250'] = data['close'].rolling(window=250).mean()\n",
    "    data['RSI'] = ta.rsi(data['close'])\n",
    "    bb = ta.bbands(data['close'])\n",
    "    data['BB_Upper'], data['BB_Middle'], data['BB_Lower'] = bb.iloc[:, 0], bb.iloc[:, 1], bb.iloc[:, 2]\n",
    "    macd = ta.macd(data['close'])\n",
    "    data['MACD'] = macd.iloc[:, 0]\n",
    "    data['Stochastic'] = ta.stoch(data['high'], data['low'], data['close']).iloc[:, 0]\n",
    "    return data.dropna()\n",
    "\n",
    "####################################\n",
    "# 1-2. 추가 feature 계산 (가격 차이)\n",
    "####################################\n",
    "def calculate_price_differences(data):\n",
    "    data['close_open'] = data['close'] - data['open']\n",
    "    data['high_low'] = data['high'] - data['low']\n",
    "    data['high_open'] = data['high'] - data['open']\n",
    "    data['high_close'] = data['high'] - data['close']\n",
    "    data['open_low'] = data['open'] - data['low']\n",
    "    data['close_low'] = data['close'] - data['low']\n",
    "    return data\n",
    "\n",
    "####################################\n",
    "# 2. Datetime Feature One-Hot Encoding (각 feature 128차원)\n",
    "####################################\n",
    "def encode_datetime_features_onehot(data, dim=128):\n",
    "    if 'datetime' not in data.columns:\n",
    "        data['datetime'] = pd.to_datetime(data.index)\n",
    "    \n",
    "    data['hour_of_day'] = data['datetime'].dt.hour\n",
    "    data['day_of_week'] = data['datetime'].dt.dayofweek\n",
    "    data['week_of_month'] = (data['datetime'].dt.day - 1) // 7 + 1\n",
    "    data['month'] = data['datetime'].dt.month\n",
    "\n",
    "    def onehot_with_fixed_dim(series, prefix, dim):\n",
    "        dummies = pd.get_dummies(series, prefix=prefix)\n",
    "        expected_cols = [f\"{prefix}_{i}\" for i in range(dim)]\n",
    "        dummies = dummies.reindex(columns=expected_cols, fill_value=0)\n",
    "        return dummies\n",
    "\n",
    "    hour_one_hot = onehot_with_fixed_dim(data['hour_of_day'], 'Hour', dim)\n",
    "    day_one_hot = onehot_with_fixed_dim(data['day_of_week'], 'Day', dim)\n",
    "    week_one_hot = onehot_with_fixed_dim(data['week_of_month'], 'Week', dim)\n",
    "    month_one_hot = onehot_with_fixed_dim(data['month'], 'Month', dim)\n",
    "    \n",
    "    data = pd.concat([data, hour_one_hot, day_one_hot, week_one_hot, month_one_hot], axis=1)\n",
    "    return data\n",
    "\n",
    "####################################\n",
    "# 3. Rolling MinMax Scaling (분모 0 방지)\n",
    "####################################\n",
    "def rolling_minmax_scale(series, window=24):\n",
    "    roll_min = series.rolling(window=window, min_periods=window).min()\n",
    "    roll_max = series.rolling(window=window, min_periods=window).max()\n",
    "    scaled = (series - roll_min) / ((roll_max - roll_min) + 1e-8)\n",
    "    return scaled\n",
    "\n",
    "####################################\n",
    "# 4. Binning 후 One-Hot 인코딩 (각 feature를 128차원으로)\n",
    "####################################\n",
    "def bin_and_encode(data, features, bins=128, drop_original=True):\n",
    "    for feature in features:\n",
    "        data[f'{feature}_Bin'] = pd.cut(data[feature], bins=bins, labels=False)\n",
    "        one_hot = pd.get_dummies(data[f'{feature}_Bin'], prefix=f'{feature}_Bin')\n",
    "        expected_columns = [f'{feature}_Bin_{i}' for i in range(bins)]\n",
    "        one_hot = one_hot.reindex(columns=expected_columns, fill_value=0)\n",
    "        data = pd.concat([data, one_hot], axis=1)\n",
    "        if drop_original:\n",
    "            data.drop(columns=[f'{feature}_Bin'], inplace=True)\n",
    "    data = data.astype(np.float32)\n",
    "    return data\n",
    "\n",
    "####################################\n",
    "# 5. 데이터 로드 및 전처리\n",
    "####################################\n",
    "data = pd.read_csv(\"BTC_upbit_KRW_min5.csv\", index_col=0)\n",
    "data.columns = ['open', 'high', 'low', 'close', 'volume', 'value']\n",
    "data.index = pd.to_datetime(data.index)\n",
    "\n",
    "data = calculate_indicators(data)\n",
    "data = calculate_price_differences(data)   # 추가 feature 계산\n",
    "data = encode_datetime_features_onehot(data, dim=128)\n",
    "\n",
    "# 원본 feature와 추가한 가격 차이 feature들을 사용\n",
    "features_to_bin = ['open', 'high', 'low', 'volume', 'value', 'William_R',\n",
    "                   'ATR', 'OBV', 'Z_Score', 'Entropy', 'SMA_5', 'SMA_10', \n",
    "                   'SMA_20', 'SMA_60', 'SMA_120', 'SMA_250', 'RSI', 'BB_Upper', 'BB_Middle', \n",
    "                   'BB_Lower', 'MACD', 'Stochastic',\n",
    "                   'close_open', 'high_low', 'high_open', 'high_close', 'open_low', 'close_low']\n",
    "\n",
    "# datetime one-hot 컬럼: 이미 각각 128차원씩 4개 (총 512차원)\n",
    "datetime_onehot_features = [col for col in data.columns if col.startswith('Hour_') or \n",
    "                              col.startswith('Day_') or col.startswith('Week_') or \n",
    "                              col.startswith('Month_')]\n",
    "\n",
    "# 타깃으로 사용할 값들은 모델 타깃으로만 사용하기 위해 보존\n",
    "data['close_target'] = data['close']\n",
    "data['open_target'] = data['open']\n",
    "\n",
    "# 최종 데이터 선택 (여기서 close_target, open_target은 타깃으로만 사용)\n",
    "data = data[features_to_bin + ['close_target', 'open_target'] + datetime_onehot_features].dropna()\n",
    "\n",
    "# 각 기술적 및 가격 차이 feature에 대해 rolling scaling 적용 (window=24)\n",
    "for feature in features_to_bin:\n",
    "    data[feature] = rolling_minmax_scale(data[feature], window=24)\n",
    "data = data.dropna()\n",
    "\n",
    "# 기술적 및 가격 차이 feature들을 128차원의 one-hot 벡터로 변환\n",
    "data = bin_and_encode(data, features_to_bin, bins=128, drop_original=True)\n",
    "\n",
    "# 보조로 사용할 close_target에 대해서도 one-hot 인코딩 (128차원)이 필요하면 진행할 수 있으나,\n",
    "# 여기서는 타깃은 continuous 값으로만 사용합니다.\n",
    "# 최종 입력 데이터에는 타깃 컬럼은 포함하지 않습니다.\n",
    "\n",
    "####################################\n",
    "# 최종 입력/타깃 데이터 구성\n",
    "####################################\n",
    "# 입력: 기술적/가격 차이 feature의 one-hot 인코딩 결과와 datetime one-hot 인코딩 결과만\n",
    "final_input_columns = []\n",
    "for feature in features_to_bin:\n",
    "    final_input_columns.extend([f'{feature}_Bin_{i}' for i in range(128)])\n",
    "final_input_columns.extend(datetime_onehot_features)\n",
    "\n",
    "# 타깃: open_target과 close_target (continuous)\n",
    "final_target_columns = ['open_target', 'close_target']\n",
    "\n",
    "data_input = data[final_input_columns]\n",
    "data_target = data[final_target_columns]\n",
    "\n",
    "####################################\n",
    "# 6-2. Dataset 정의 (입력과 타깃을 별도로 사용)\n",
    "####################################\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, input_data, target_data, lookback=24):\n",
    "        self.input_data = input_data.values\n",
    "        self.target_data = target_data.values  # shape: (N, 2)\n",
    "        self.lookback = lookback\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_data) - self.lookback\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.input_data[idx: idx + self.lookback, :]\n",
    "        # 타깃은 lookback 이후 시점의 open_target와 close_target 값을 가져와서,\n",
    "        # candle이 양봉이면 1, 음봉이면 0으로 설정 (양봉: close > open)\n",
    "        open_val = self.target_data[idx + self.lookback, 0]\n",
    "        close_val = self.target_data[idx + self.lookback, 1]\n",
    "        y_target = 1 if close_val > open_val else 0\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y_target, dtype=torch.long)\n",
    "\n",
    "####################################\n",
    "# 7. Transformer Encoder 직접 구현\n",
    "####################################\n",
    "# 7-1. Multi-Head Self-Attention\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == embed_dim, \"embed_dim은 num_heads로 나누어떨어져야 합니다.\"\n",
    "        \n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key   = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out   = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        \n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / sqrt(self.head_dim)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, V)\n",
    "        \n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "\n",
    "# 7-2. Feed-Forward Network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ffn_dim):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, ffn_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(ffn_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "# 7-3. Transformer Encoder Layer (Self-Attention + FFN + Residual + LayerNorm)\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = FeedForward(embed_dim, ffn_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attn_out = self.self_attn(x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_out)\n",
    "        return x\n",
    "\n",
    "# 7-4. Encoder-Only Transformer 직접 구현 (lookback=24이므로, max_seq_len=24)\n",
    "class EncoderOnlyTransformerCustom(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=512, num_heads=8, num_layers=6, ffn_dim=2048, num_classes=2, max_seq_len=24):\n",
    "        super(EncoderOnlyTransformerCustom, self).__init__()\n",
    "        self.token_embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embedding_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(embedding_dim, num_heads, ffn_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x = self.token_embedding(x)\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        x = x + self.position_embedding(positions)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x[:, -1, :]\n",
    "        return self.fc(x)\n",
    "\n",
    "####################################\n",
    "# 8. 학습 및 평가 루프 (Fine-Tuning 및 Validation Accuracy 출력)\n",
    "####################################\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return total_loss / len(data_loader), correct / total\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, lr, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_state = None\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, device)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = model.state_dict()\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(data, num_experiments=16, lookback=24, num_epochs=10):\n",
    "    # 모델 입력은 one-hot 인코딩된 feature들만, 타깃은 continuous open_target과 close_target\n",
    "    input_cols = []\n",
    "    for feature in features_to_bin:\n",
    "        input_cols.extend([f'{feature}_Bin_{i}' for i in range(128)])\n",
    "    input_cols.extend(datetime_onehot_features)\n",
    "    target_cols = ['open_target', 'close_target']\n",
    "    \n",
    "    # 타깃 컬럼 추가: 원본 open, close 값을 보존 (continuous)\n",
    "    data['open_target'] = data['open']\n",
    "    data['close_target'] = data['close']\n",
    "    \n",
    "    data_input = data[input_cols]\n",
    "    data_target = data[target_cols]\n",
    "    \n",
    "    step_size = 30000  # 이동 단위\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    val_acc_list = []\n",
    "    test_acc_list = []\n",
    "    \n",
    "    for exp in range(num_experiments):\n",
    "        train_start = exp * step_size\n",
    "        train_end = train_start + step_size * 8\n",
    "        val_end = train_end + step_size\n",
    "        test_end = val_end + step_size\n",
    "        if test_end > len(data_input):\n",
    "            break\n",
    "        print(data)\n",
    "        \n",
    "        train_input = data_input.iloc[train_start:train_end]\n",
    "        train_target = data_target.iloc[train_start:train_end]\n",
    "        val_input = data_input.iloc[train_end:val_end]\n",
    "        val_target = data_target.iloc[train_end:val_end]\n",
    "        test_input = data_input.iloc[val_end:test_end]\n",
    "        test_target = data_target.iloc[val_end:test_end]\n",
    "        \n",
    "        # Dataset 생성: 입력과 타깃을 따로 전달\n",
    "        train_dataset = TimeSeriesDataset(train_input, train_target, lookback=lookback)\n",
    "        val_dataset = TimeSeriesDataset(val_input, val_target, lookback=lookback)\n",
    "        test_dataset = TimeSeriesDataset(test_input, test_target, lookback=lookback)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        # Fine-tuning: exp==0일 때는 새로 학습, 이후 구간에서는 이전 모델 파라미터를 그대로 로드하여 미세 조정\n",
    "        lr = 1e-4\n",
    "        input_dim = data_input.shape[1]\n",
    "        model = EncoderOnlyTransformerCustom(input_dim=input_dim, embedding_dim=512, num_heads=8, \n",
    "                                               num_layers=6, ffn_dim=2048, num_classes=2, max_seq_len=lookback).to(device)\n",
    "        model_path = f\"model_experiment_{exp}.pth\"\n",
    "        if exp > 0:\n",
    "            try:\n",
    "                model.load_state_dict(torch.load(f\"model_experiment_{exp - 1}.pth\"))\n",
    "                print(f\"Loaded model from experiment {exp - 1} for fine-tuning.\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Model file for experiment {exp - 1} not found. Starting fresh training.\")\n",
    "        \n",
    "        print(f\"Experiment {exp}: Training with lr={lr} (Fine-Tuning)\")\n",
    "        model = train_model(model, train_loader, val_loader, num_epochs, lr, device)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Saved model for experiment {exp}.\")\n",
    "        \n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, device)\n",
    "        val_acc_list.append(val_acc)\n",
    "        print(f\"Experiment {exp}: Final Validation Accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "        test_loss, test_acc = evaluate_model(model, test_loader, device)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(f\"Experiment {exp}: Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    avg_val_acc = sum(val_acc_list) / len(val_acc_list)\n",
    "    avg_test_acc = sum(test_acc_list) / len(test_acc_list)\n",
    "    print(f\"\\nFinal Average Validation Accuracy: {avg_val_acc:.4f}\")\n",
    "    print(f\"Final Average Test Accuracy: {avg_test_acc:.4f}\")\n",
    "\n",
    "train_and_evaluate(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'data_input' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 426\u001b[0m\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFinal Average Validation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_val_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Average Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_test_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 426\u001b[0m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 360\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(data, num_experiments, lookback, num_epochs)\u001b[0m\n\u001b[0;32m    358\u001b[0m data_targets \u001b[38;5;241m=\u001b[39m data_targets[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[0;32m    359\u001b[0m \u001b[38;5;66;03m# data_input과 동일한 인덱스 사용: 교집합 사용\u001b[39;00m\n\u001b[1;32m--> 360\u001b[0m common_index \u001b[38;5;241m=\u001b[39m \u001b[43mdata_input\u001b[49m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mintersection(data_targets\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m    361\u001b[0m data_input \u001b[38;5;241m=\u001b[39m data_input\u001b[38;5;241m.\u001b[39mloc[common_index]\n\u001b[0;32m    362\u001b[0m data_target \u001b[38;5;241m=\u001b[39m data_targets\u001b[38;5;241m.\u001b[39mloc[common_index]\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopen\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopen_target\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclose_target\u001b[39m\u001b[38;5;124m'\u001b[39m})\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'data_input' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta as ta\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from math import sqrt\n",
    "\n",
    "####################################\n",
    "# 1. 기술적 지표 계산 함수 (변경 없음)\n",
    "####################################\n",
    "def calculate_indicators(data):\n",
    "    data['William_R'] = ta.willr(data['high'], data['low'], data['close'])\n",
    "    data['ATR'] = ta.atr(data['high'], data['low'], data['close'])\n",
    "    data['OBV'] = ta.obv(data['close'], data['volume'])\n",
    "    data['Z_Score'] = (data['close'] - data['close'].rolling(window=20).mean()) / data['close'].rolling(window=20).std()\n",
    "    data['Entropy'] = ta.entropy(data['close'], length=14)\n",
    "    data['SMA_5'] = data['close'].rolling(window=5).mean()\n",
    "    data['SMA_10'] = data['close'].rolling(window=10).mean()\n",
    "    data['SMA_20'] = data['close'].rolling(window=20).mean()\n",
    "    data['SMA_60'] = data['close'].rolling(window=60).mean()\n",
    "    data['SMA_120'] = data['close'].rolling(window=120).mean()\n",
    "    data['SMA_250'] = data['close'].rolling(window=250).mean()\n",
    "    data['RSI'] = ta.rsi(data['close'])\n",
    "    bb = ta.bbands(data['close'])\n",
    "    data['BB_Upper'], data['BB_Middle'], data['BB_Lower'] = bb.iloc[:, 0], bb.iloc[:, 1], bb.iloc[:, 2]\n",
    "    macd = ta.macd(data['close'])\n",
    "    data['MACD'] = macd.iloc[:, 0]\n",
    "    data['Stochastic'] = ta.stoch(data['high'], data['low'], data['close']).iloc[:, 0]\n",
    "    return data.dropna()\n",
    "\n",
    "####################################\n",
    "# 1-2. 추가 feature 계산 (가격 차이)\n",
    "####################################\n",
    "def calculate_price_differences(data):\n",
    "    data['close_open'] = data['close'] - data['open']\n",
    "    data['high_low'] = data['high'] - data['low']\n",
    "    data['high_open'] = data['high'] - data['open']\n",
    "    data['high_close'] = data['high'] - data['close']\n",
    "    data['open_low'] = data['open'] - data['low']\n",
    "    data['close_low'] = data['close'] - data['low']\n",
    "    return data\n",
    "\n",
    "####################################\n",
    "# 2. Datetime Feature One-Hot Encoding (각 feature 128차원)\n",
    "####################################\n",
    "def encode_datetime_features_onehot(data, dim=128):\n",
    "    if 'datetime' not in data.columns:\n",
    "        data['datetime'] = pd.to_datetime(data.index)\n",
    "    \n",
    "    data['hour_of_day'] = data['datetime'].dt.hour\n",
    "    data['day_of_week'] = data['datetime'].dt.dayofweek\n",
    "    data['week_of_month'] = (data['datetime'].dt.day - 1) // 7 + 1\n",
    "    data['month'] = data['datetime'].dt.month\n",
    "\n",
    "    def onehot_with_fixed_dim(series, prefix, dim):\n",
    "        dummies = pd.get_dummies(series, prefix=prefix)\n",
    "        expected_cols = [f\"{prefix}_{i}\" for i in range(dim)]\n",
    "        dummies = dummies.reindex(columns=expected_cols, fill_value=0)\n",
    "        return dummies\n",
    "\n",
    "    hour_one_hot = onehot_with_fixed_dim(data['hour_of_day'], 'Hour', dim)\n",
    "    day_one_hot = onehot_with_fixed_dim(data['day_of_week'], 'Day', dim)\n",
    "    week_one_hot = onehot_with_fixed_dim(data['week_of_month'], 'Week', dim)\n",
    "    month_one_hot = onehot_with_fixed_dim(data['month'], 'Month', dim)\n",
    "    \n",
    "    data = pd.concat([data, hour_one_hot, day_one_hot, week_one_hot, month_one_hot], axis=1)\n",
    "    return data\n",
    "\n",
    "####################################\n",
    "# 3. Rolling MinMax Scaling (분모 0 방지)\n",
    "####################################\n",
    "def rolling_minmax_scale(series, window=24):\n",
    "    roll_min = series.rolling(window=window, min_periods=window).min()\n",
    "    roll_max = series.rolling(window=window, min_periods=window).max()\n",
    "    scaled = (series - roll_min) / ((roll_max - roll_min) + 1e-8)\n",
    "    return scaled\n",
    "\n",
    "####################################\n",
    "# 4. Binning 후 One-Hot 인코딩 (각 feature를 128차원으로)\n",
    "####################################\n",
    "def bin_and_encode(data, features, bins=128, drop_original=True):\n",
    "    for feature in features:\n",
    "        data[f'{feature}_Bin'] = pd.cut(data[feature], bins=bins, labels=False)\n",
    "        one_hot = pd.get_dummies(data[f'{feature}_Bin'], prefix=f'{feature}_Bin')\n",
    "        expected_columns = [f'{feature}_Bin_{i}' for i in range(bins)]\n",
    "        one_hot = one_hot.reindex(columns=expected_columns, fill_value=0)\n",
    "        data = pd.concat([data, one_hot], axis=1)\n",
    "        if drop_original:\n",
    "            data.drop(columns=[f'{feature}_Bin'], inplace=True)\n",
    "    data = data.astype(np.float32)\n",
    "    return data\n",
    "\n",
    "####################################\n",
    "# 5. 데이터 로드 및 전처리\n",
    "####################################\n",
    "data = pd.read_csv(\"BTC_upbit_KRW_min60.csv\", index_col=0)\n",
    "data.columns = ['open', 'high', 'low', 'close', 'volume', 'value']\n",
    "data.index = pd.to_datetime(data.index)\n",
    "data = calculate_indicators(data)\n",
    "data = calculate_price_differences(data)\n",
    "data = encode_datetime_features_onehot(data, dim=128)\n",
    "\n",
    "# 원본 feature와 추가한 가격 차이 feature들을 사용\n",
    "features_to_bin = ['open', 'high', 'low', 'volume', 'value', 'William_R',\n",
    "                   'ATR', 'OBV', 'Z_Score', 'Entropy', 'SMA_5', 'SMA_10', \n",
    "                   'SMA_20', 'SMA_60', 'SMA_120', 'SMA_250', 'RSI', 'BB_Upper', 'BB_Middle', \n",
    "                   'BB_Lower', 'MACD', 'Stochastic',\n",
    "                   'close_open', 'high_low', 'high_open', 'high_close', 'open_low', 'close_low']\n",
    "\n",
    "# datetime one-hot 컬럼: 이미 각각 128차원씩 4개 (총 512차원)\n",
    "datetime_onehot_features = [col for col in data.columns if col.startswith('Hour_') or \n",
    "                              col.startswith('Day_') or col.startswith('Week_') or \n",
    "                              col.startswith('Month_')]\n",
    "\n",
    "# 타깃으로 사용할 continuous 값들은 원본에서 보존 (아직 제거하지 않음)\n",
    "# 여기서는 이후에 타깃 데이터로만 사용될 예정\n",
    "data_targets = data[['open', 'close']].copy()\n",
    "\n",
    "# 최종 입력 데이터: 타깃 컬럼은 제외하고, 오직 features_to_bin와 datetime one-hot 인코딩된 값만 사용\n",
    "data_input = data[features_to_bin + datetime_onehot_features].dropna()\n",
    "\n",
    "# 각 기술적 및 가격 차이 feature에 대해 rolling scaling 적용 (window=24)\n",
    "for feature in features_to_bin:\n",
    "    data_input[feature] = rolling_minmax_scale(data_input[feature], window=24)\n",
    "data_input = data_input.dropna()\n",
    "\n",
    "# 기술적 및 가격 차이 feature들을 128차원의 one-hot 벡터로 변환\n",
    "data_input = bin_and_encode(data_input, features_to_bin, bins=128, drop_original=True)\n",
    "final_input_columns = []\n",
    "for feature in features_to_bin:\n",
    "    final_input_columns.extend([f'{feature}_Bin_{i}' for i in range(128)])\n",
    "final_input_columns.extend(datetime_onehot_features)\n",
    "data_input = data_input[final_input_columns]\n",
    "\n",
    "# 타깃 데이터: continuous 값, open과 close를 각각 사용\n",
    "data_target = data_targets.rename(columns={'open': 'open_target', 'close': 'close_target'})\n",
    "# data_target의 인덱스를 data_input과 맞춥니다.\n",
    "common_index = data_input.index.intersection(data_target.index)\n",
    "data_input = data_input.loc[common_index]\n",
    "data_target = data_target.loc[common_index]\n",
    "\n",
    "####################################\n",
    "# 6-2. Dataset 정의 (입력과 타깃을 별도로 사용)\n",
    "####################################\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, input_data, target_data, lookback=24):\n",
    "        self.input_data = input_data.values\n",
    "        self.target_data = target_data.values  # shape: (N, 2)\n",
    "        self.lookback = lookback\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_data) - self.lookback\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.input_data[idx: idx + self.lookback, :]\n",
    "        # 타깃: lookback 이후 시점의 open_target과 close_target\n",
    "        open_val = self.target_data[idx + self.lookback, 0]\n",
    "        close_val = self.target_data[idx + self.lookback, 1]\n",
    "        # 캔들 유형: 양봉이면 1, 음봉이면 0 (양봉: close > open)\n",
    "        y_target = 1 if close_val > open_val else 0\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y_target, dtype=torch.long)\n",
    "\n",
    "####################################\n",
    "# 7. Transformer Encoder-Decoder 직접 구현\n",
    "####################################\n",
    "# 7-1. Multi-Head Self-Attention (동일)\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == embed_dim, \"embed_dim은 num_heads로 나누어떨어져야 합니다.\"\n",
    "        \n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key   = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out   = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        \n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, V)\n",
    "        \n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "\n",
    "# 7-2. Feed-Forward Network (동일)\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ffn_dim):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, ffn_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(ffn_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "# 7-3. Transformer Encoder Layer (동일)\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = FeedForward(embed_dim, ffn_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out = self.self_attn(x, mask)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_out)\n",
    "        return x\n",
    "\n",
    "# 7-4. Transformer Decoder Layer\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        # Masked self-attention for decoder\n",
    "        self.self_attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        # Cross attention: query from decoder, key/value from encoder output\n",
    "        self.cross_attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = FeedForward(embed_dim, ffn_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x, encoder_output, tgt_mask=None):\n",
    "        # x: decoder input, encoder_output: encoder output\n",
    "        self_attn_out = self.self_attn(x, mask=tgt_mask)\n",
    "        x = self.norm1(x + self_attn_out)\n",
    "        # For cross attention, we use x as query and encoder_output as key/value.\n",
    "        cross_attn_out = self.cross_attn(x)\n",
    "        x = self.norm2(x + cross_attn_out)\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm3(x + ffn_out)\n",
    "        return x\n",
    "\n",
    "# 7-5. Encoder-Decoder Transformer 직접 구현\n",
    "class EncoderDecoderTransformerCustom(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=512, num_heads=8, num_encoder_layers=6, \n",
    "                 num_decoder_layers=6, ffn_dim=2048, num_classes=2, max_seq_len=24):\n",
    "        super(EncoderDecoderTransformerCustom, self).__init__()\n",
    "        # Encoder\n",
    "        self.token_embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embedding_dim)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(embedding_dim, num_heads, ffn_dim)\n",
    "            for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "        # Decoder: we'll use a learnable start token as decoder input (length 1)\n",
    "        self.decoder_start_token = nn.Parameter(torch.randn(1, 1, embedding_dim))\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(embedding_dim, num_heads, ffn_dim)\n",
    "            for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "        \n",
    "    def forward(self, encoder_input):\n",
    "        # encoder_input: (batch_size, seq_len, input_dim)\n",
    "        batch_size, seq_len, _ = encoder_input.shape\n",
    "        # Encoder\n",
    "        x = self.token_embedding(encoder_input)\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        x = x + self.position_embedding(positions)\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        encoder_output = x  # (batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        # Decoder: use a fixed start token repeated for batch\n",
    "        dec_input = self.decoder_start_token.expand(batch_size, -1, -1)  # (batch_size, 1, embedding_dim)\n",
    "        # Since decoder input length=1, no mask needed.\n",
    "        for layer in self.decoder_layers:\n",
    "            dec_input = layer(dec_input, encoder_output)\n",
    "        # dec_input: (batch_size, 1, embedding_dim)\n",
    "        out = dec_input[:, -1, :]  # (batch_size, embedding_dim)\n",
    "        logits = self.fc(out)\n",
    "        return logits\n",
    "\n",
    "####################################\n",
    "# 8. 학습 및 평가 루프 (Fine-Tuning 및 Validation Accuracy 출력)\n",
    "####################################\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return total_loss / len(data_loader), correct / total\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, lr, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_state = None\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, device)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = model.state_dict()\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(data, num_experiments=16, lookback=24, num_epochs=10):\n",
    "    # 모델 입력은 one-hot 인코딩된 feature들만, 타깃은 continuous open_target과 close_target\n",
    "    input_cols = []\n",
    "    for feature in features_to_bin:\n",
    "        input_cols.extend([f'{feature}_Bin_{i}' for i in range(128)])\n",
    "    input_cols.extend(datetime_onehot_features)\n",
    "    target_cols = ['open_target', 'close_target']\n",
    "    \n",
    "    # 타깃 컬럼 추가: 원본 'open', 'close' 값을 보존 (데이터 읽기 전 원본을 다시 읽어 사용)\n",
    "    data_targets = pd.read_csv(\"BTC_upbit_KRW_min5.csv\", index_col=0)\n",
    "    data_targets.columns = ['open', 'high', 'low', 'close', 'volume', 'value']\n",
    "    data_targets.index = pd.to_datetime(data_targets.index)\n",
    "    data_targets = data_targets[['open', 'close']].dropna()\n",
    "    # data_input과 동일한 인덱스 사용: 교집합 사용\n",
    "    common_index = data_input.index.intersection(data_targets.index)\n",
    "    data_input = data_input.loc[common_index]\n",
    "    data_target = data_targets.loc[common_index].rename(columns={'open': 'open_target', 'close': 'close_target'})\n",
    "    \n",
    "    step_size = 30000  # 이동 단위\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    val_acc_list = []\n",
    "    test_acc_list = []\n",
    "    \n",
    "    for exp in range(num_experiments):\n",
    "        train_start = exp * step_size\n",
    "        train_end = train_start + step_size * 8\n",
    "        val_end = train_end + step_size\n",
    "        test_end = val_end + step_size\n",
    "        if test_end > len(data_input):\n",
    "            break\n",
    "        print(data)\n",
    "        \n",
    "        train_input = data_input.iloc[train_start:train_end]\n",
    "        train_target = data_target.iloc[train_start:train_end]\n",
    "        val_input = data_input.iloc[train_end:val_end]\n",
    "        val_target = data_target.iloc[train_end:val_end]\n",
    "        test_input = data_input.iloc[val_end:test_end]\n",
    "        test_target = data_target.iloc[val_end:test_end]\n",
    "        \n",
    "        train_dataset = TimeSeriesDataset(train_input, train_target, lookback=lookback)\n",
    "        val_dataset = TimeSeriesDataset(val_input, val_target, lookback=lookback)\n",
    "        test_dataset = TimeSeriesDataset(test_input, test_target, lookback=lookback)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        # Fine-Tuning: 이전 구간의 모델 파라미터를 그대로 로드하여 미세 조정\n",
    "        lr = 1e-4\n",
    "        input_dim = data_input.shape[1]\n",
    "        model = EncoderDecoderTransformerCustom(input_dim=input_dim, embedding_dim=512, num_heads=8, \n",
    "                                                 num_encoder_layers=6, num_decoder_layers=6, ffn_dim=2048, \n",
    "                                                 num_classes=2, max_seq_len=lookback).to(device)\n",
    "        model_path = f\"model_experiment_{exp}.pth\"\n",
    "        if exp > 0:\n",
    "            try:\n",
    "                model.load_state_dict(torch.load(f\"model_experiment_{exp - 1}.pth\"))\n",
    "                print(f\"Loaded model from experiment {exp - 1} for fine-tuning.\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Model file for experiment {exp - 1} not found. Starting fresh training.\")\n",
    "        \n",
    "        print(f\"Experiment {exp}: Training with lr={lr} (Fine-Tuning with Encoder-Decoder)\")\n",
    "        model = train_model(model, train_loader, val_loader, num_epochs, lr, device)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Saved model for experiment {exp}.\")\n",
    "        \n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, device)\n",
    "        val_acc_list.append(val_acc)\n",
    "        print(f\"Experiment {exp}: Final Validation Accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "        test_loss, test_acc = evaluate_model(model, test_loader, device)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(f\"Experiment {exp}: Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    avg_val_acc = sum(val_acc_list) / len(val_acc_list)\n",
    "    avg_test_acc = sum(test_acc_list) / len(test_acc_list)\n",
    "    print(f\"\\nFinal Average Validation Accuracy: {avg_val_acc:.4f}\")\n",
    "    print(f\"Final Average Test Accuracy: {avg_test_acc:.4f}\")\n",
    "\n",
    "train_and_evaluate(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39coin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
