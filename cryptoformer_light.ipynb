{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_12720\\3724248470.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['close_target'] = data['close']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 0: 데이터 구간 [0:25000]\n",
      "                         open      high       low     close  open_Bin_0  \\\n",
      "2017-09-25 12:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 13:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 17:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 19:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 20:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "...                       ...       ...       ...       ...         ...   \n",
      "2024-12-18 12:00:00  0.000000  0.000000  0.000000  0.000000         1.0   \n",
      "2024-12-18 13:00:00  0.000000  0.000000  0.000000  0.000000         1.0   \n",
      "2024-12-18 14:00:00  0.000000  0.000000  0.199575  0.000000         1.0   \n",
      "2024-12-18 15:00:00  0.000000  0.000000  0.226983  0.036865         1.0   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.109052         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 17:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 19:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 20:00:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 14:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 15:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:00:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_Bin_91  close_Bin_92  \\\n",
      "2017-09-25 12:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 13:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 17:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 19:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 20:00:00         0.0  ...           0.0           0.0   \n",
      "...                         ...  ...           ...           ...   \n",
      "2024-12-18 12:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 13:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 14:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 15:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:00:00         1.0  ...           0.0           0.0   \n",
      "\n",
      "                     close_Bin_93  close_Bin_94  close_Bin_95  close_Bin_96  \\\n",
      "2017-09-25 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 13:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 17:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 19:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 20:00:00           0.0           0.0           0.0           0.0   \n",
      "...                           ...           ...           ...           ...   \n",
      "2024-12-18 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 13:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 14:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 15:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:00:00           0.0           0.0           0.0           0.0   \n",
      "\n",
      "                     close_Bin_97  close_Bin_98  close_Bin_99  close_target  \n",
      "2017-09-25 12:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 13:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 17:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 19:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 20:00:00           0.0           0.0           1.0      1.000000  \n",
      "...                           ...           ...           ...           ...  \n",
      "2024-12-18 12:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 13:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 14:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 15:00:00           0.0           0.0           0.0      0.036865  \n",
      "2024-12-18 16:00:00           0.0           0.0           0.0      0.109052  \n",
      "\n",
      "[63283 rows x 405 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 0: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.7100, Train Acc: 0.5196 | Val Loss: 0.7000, Val Acc: 0.4564\n",
      "Epoch 2/10 | Train Loss: 0.6934, Train Acc: 0.5316 | Val Loss: 0.6879, Val Acc: 0.5436\n",
      "Epoch 3/10 | Train Loss: 0.6934, Train Acc: 0.5309 | Val Loss: 0.6978, Val Acc: 0.5436\n",
      "Epoch 4/10 | Train Loss: 0.6919, Train Acc: 0.5348 | Val Loss: 0.6910, Val Acc: 0.5436\n",
      "Epoch 5/10 | Train Loss: 0.6924, Train Acc: 0.5345 | Val Loss: 0.6926, Val Acc: 0.5436\n",
      "Epoch 6/10 | Train Loss: 0.6901, Train Acc: 0.5391 | Val Loss: 0.6947, Val Acc: 0.5436\n",
      "Epoch 7/10 | Train Loss: 0.6749, Train Acc: 0.5360 | Val Loss: 0.6546, Val Acc: 0.5436\n",
      "Epoch 8/10 | Train Loss: 0.6456, Train Acc: 0.5511 | Val Loss: 0.6354, Val Acc: 0.5622\n",
      "Epoch 9/10 | Train Loss: 0.6390, Train Acc: 0.5521 | Val Loss: 0.6436, Val Acc: 0.5626\n",
      "Epoch 10/10 | Train Loss: 0.6376, Train Acc: 0.5574 | Val Loss: 0.6325, Val Acc: 0.5436\n",
      "Saved model for experiment 0.\n",
      "Experiment 0: Final Validation Accuracy: 0.5436\n",
      "Experiment 0: Test Accuracy: 0.5485\n",
      "Experiment 1: 데이터 구간 [2500:27500]\n",
      "                         open      high       low     close  open_Bin_0  \\\n",
      "2017-09-25 12:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 13:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 17:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 19:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 20:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "...                       ...       ...       ...       ...         ...   \n",
      "2024-12-18 12:00:00  0.000000  0.000000  0.000000  0.000000         1.0   \n",
      "2024-12-18 13:00:00  0.000000  0.000000  0.000000  0.000000         1.0   \n",
      "2024-12-18 14:00:00  0.000000  0.000000  0.199575  0.000000         1.0   \n",
      "2024-12-18 15:00:00  0.000000  0.000000  0.226983  0.036865         1.0   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.109052         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 17:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 19:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 20:00:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 14:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 15:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:00:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_Bin_91  close_Bin_92  \\\n",
      "2017-09-25 12:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 13:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 17:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 19:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 20:00:00         0.0  ...           0.0           0.0   \n",
      "...                         ...  ...           ...           ...   \n",
      "2024-12-18 12:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 13:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 14:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 15:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:00:00         1.0  ...           0.0           0.0   \n",
      "\n",
      "                     close_Bin_93  close_Bin_94  close_Bin_95  close_Bin_96  \\\n",
      "2017-09-25 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 13:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 17:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 19:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 20:00:00           0.0           0.0           0.0           0.0   \n",
      "...                           ...           ...           ...           ...   \n",
      "2024-12-18 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 13:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 14:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 15:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:00:00           0.0           0.0           0.0           0.0   \n",
      "\n",
      "                     close_Bin_97  close_Bin_98  close_Bin_99  close_target  \n",
      "2017-09-25 12:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 13:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 17:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 19:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 20:00:00           0.0           0.0           1.0      1.000000  \n",
      "...                           ...           ...           ...           ...  \n",
      "2024-12-18 12:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 13:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 14:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 15:00:00           0.0           0.0           0.0      0.036865  \n",
      "2024-12-18 16:00:00           0.0           0.0           0.0      0.109052  \n",
      "\n",
      "[63283 rows x 405 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 0 for fine-tuning.\n",
      "Experiment 1: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6490, Train Acc: 0.5490 | Val Loss: 0.6647, Val Acc: 0.5485\n",
      "Epoch 2/10 | Train Loss: 0.6915, Train Acc: 0.5310 | Val Loss: 0.6924, Val Acc: 0.5485\n",
      "Epoch 3/10 | Train Loss: 0.6905, Train Acc: 0.5396 | Val Loss: 0.6911, Val Acc: 0.5485\n",
      "Epoch 4/10 | Train Loss: 0.6904, Train Acc: 0.5437 | Val Loss: 0.6897, Val Acc: 0.5485\n",
      "Epoch 5/10 | Train Loss: 0.6899, Train Acc: 0.5438 | Val Loss: 0.6926, Val Acc: 0.5485\n",
      "Epoch 6/10 | Train Loss: 0.6904, Train Acc: 0.5437 | Val Loss: 0.6889, Val Acc: 0.5485\n",
      "Epoch 7/10 | Train Loss: 0.6902, Train Acc: 0.5439 | Val Loss: 0.6893, Val Acc: 0.5485\n",
      "Epoch 8/10 | Train Loss: 0.6898, Train Acc: 0.5439 | Val Loss: 0.6888, Val Acc: 0.5485\n",
      "Epoch 9/10 | Train Loss: 0.6898, Train Acc: 0.5439 | Val Loss: 0.6890, Val Acc: 0.5485\n",
      "Epoch 10/10 | Train Loss: 0.6899, Train Acc: 0.5439 | Val Loss: 0.6890, Val Acc: 0.5485\n",
      "Saved model for experiment 1.\n",
      "Experiment 1: Final Validation Accuracy: 0.5485\n",
      "Experiment 1: Test Accuracy: 0.5440\n",
      "Experiment 2: 데이터 구간 [5000:30000]\n",
      "                         open      high       low     close  open_Bin_0  \\\n",
      "2017-09-25 12:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 13:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 17:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 19:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 20:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "...                       ...       ...       ...       ...         ...   \n",
      "2024-12-18 12:00:00  0.000000  0.000000  0.000000  0.000000         1.0   \n",
      "2024-12-18 13:00:00  0.000000  0.000000  0.000000  0.000000         1.0   \n",
      "2024-12-18 14:00:00  0.000000  0.000000  0.199575  0.000000         1.0   \n",
      "2024-12-18 15:00:00  0.000000  0.000000  0.226983  0.036865         1.0   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.109052         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 17:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 19:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 20:00:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 14:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 15:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:00:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_Bin_91  close_Bin_92  \\\n",
      "2017-09-25 12:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 13:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 17:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 19:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 20:00:00         0.0  ...           0.0           0.0   \n",
      "...                         ...  ...           ...           ...   \n",
      "2024-12-18 12:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 13:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 14:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 15:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:00:00         1.0  ...           0.0           0.0   \n",
      "\n",
      "                     close_Bin_93  close_Bin_94  close_Bin_95  close_Bin_96  \\\n",
      "2017-09-25 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 13:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 17:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 19:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 20:00:00           0.0           0.0           0.0           0.0   \n",
      "...                           ...           ...           ...           ...   \n",
      "2024-12-18 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 13:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 14:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 15:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:00:00           0.0           0.0           0.0           0.0   \n",
      "\n",
      "                     close_Bin_97  close_Bin_98  close_Bin_99  close_target  \n",
      "2017-09-25 12:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 13:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 17:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 19:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 20:00:00           0.0           0.0           1.0      1.000000  \n",
      "...                           ...           ...           ...           ...  \n",
      "2024-12-18 12:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 13:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 14:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 15:00:00           0.0           0.0           0.0      0.036865  \n",
      "2024-12-18 16:00:00           0.0           0.0           0.0      0.109052  \n",
      "\n",
      "[63283 rows x 405 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 1 for fine-tuning.\n",
      "Experiment 2: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6906, Train Acc: 0.5443 | Val Loss: 0.6892, Val Acc: 0.5440\n",
      "Epoch 2/10 | Train Loss: 0.6904, Train Acc: 0.5428 | Val Loss: 0.6949, Val Acc: 0.4560\n",
      "Epoch 3/10 | Train Loss: 0.6899, Train Acc: 0.5440 | Val Loss: 0.6900, Val Acc: 0.5440\n",
      "Epoch 4/10 | Train Loss: 0.6901, Train Acc: 0.5456 | Val Loss: 0.6920, Val Acc: 0.5440\n",
      "Epoch 5/10 | Train Loss: 0.6893, Train Acc: 0.5452 | Val Loss: 0.6890, Val Acc: 0.5440\n",
      "Epoch 6/10 | Train Loss: 0.6895, Train Acc: 0.5445 | Val Loss: 0.6890, Val Acc: 0.5440\n",
      "Epoch 7/10 | Train Loss: 0.6896, Train Acc: 0.5450 | Val Loss: 0.6890, Val Acc: 0.5440\n",
      "Epoch 8/10 | Train Loss: 0.6895, Train Acc: 0.5452 | Val Loss: 0.6890, Val Acc: 0.5440\n",
      "Epoch 9/10 | Train Loss: 0.6893, Train Acc: 0.5453 | Val Loss: 0.6892, Val Acc: 0.5440\n",
      "Epoch 10/10 | Train Loss: 0.6895, Train Acc: 0.5453 | Val Loss: 0.6890, Val Acc: 0.5440\n",
      "Saved model for experiment 2.\n",
      "Experiment 2: Final Validation Accuracy: 0.5440\n",
      "Experiment 2: Test Accuracy: 0.5553\n",
      "Experiment 3: 데이터 구간 [7500:32500]\n",
      "                         open      high       low     close  open_Bin_0  \\\n",
      "2017-09-25 12:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 13:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 17:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 19:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 20:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "...                       ...       ...       ...       ...         ...   \n",
      "2024-12-18 12:00:00  0.000000  0.000000  0.000000  0.000000         1.0   \n",
      "2024-12-18 13:00:00  0.000000  0.000000  0.000000  0.000000         1.0   \n",
      "2024-12-18 14:00:00  0.000000  0.000000  0.199575  0.000000         1.0   \n",
      "2024-12-18 15:00:00  0.000000  0.000000  0.226983  0.036865         1.0   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.109052         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 17:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 19:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 20:00:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 14:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 15:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:00:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_Bin_91  close_Bin_92  \\\n",
      "2017-09-25 12:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 13:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 17:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 19:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 20:00:00         0.0  ...           0.0           0.0   \n",
      "...                         ...  ...           ...           ...   \n",
      "2024-12-18 12:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 13:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 14:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 15:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:00:00         1.0  ...           0.0           0.0   \n",
      "\n",
      "                     close_Bin_93  close_Bin_94  close_Bin_95  close_Bin_96  \\\n",
      "2017-09-25 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 13:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 17:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 19:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 20:00:00           0.0           0.0           0.0           0.0   \n",
      "...                           ...           ...           ...           ...   \n",
      "2024-12-18 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 13:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 14:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 15:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:00:00           0.0           0.0           0.0           0.0   \n",
      "\n",
      "                     close_Bin_97  close_Bin_98  close_Bin_99  close_target  \n",
      "2017-09-25 12:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 13:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 17:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 19:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 20:00:00           0.0           0.0           1.0      1.000000  \n",
      "...                           ...           ...           ...           ...  \n",
      "2024-12-18 12:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 13:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 14:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 15:00:00           0.0           0.0           0.0      0.036865  \n",
      "2024-12-18 16:00:00           0.0           0.0           0.0      0.109052  \n",
      "\n",
      "[63283 rows x 405 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 2 for fine-tuning.\n",
      "Experiment 3: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6897, Train Acc: 0.5459 | Val Loss: 0.6905, Val Acc: 0.5553\n",
      "Epoch 2/10 | Train Loss: 0.6895, Train Acc: 0.5446 | Val Loss: 0.6889, Val Acc: 0.5553\n",
      "Epoch 3/10 | Train Loss: 0.6899, Train Acc: 0.5422 | Val Loss: 0.6878, Val Acc: 0.5553\n",
      "Epoch 4/10 | Train Loss: 0.6897, Train Acc: 0.5461 | Val Loss: 0.6945, Val Acc: 0.4447\n",
      "Epoch 5/10 | Train Loss: 0.6899, Train Acc: 0.5427 | Val Loss: 0.6876, Val Acc: 0.5553\n",
      "Epoch 6/10 | Train Loss: 0.6898, Train Acc: 0.5464 | Val Loss: 0.6870, Val Acc: 0.5553\n",
      "Epoch 7/10 | Train Loss: 0.6897, Train Acc: 0.5452 | Val Loss: 0.6876, Val Acc: 0.5553\n",
      "Epoch 8/10 | Train Loss: 0.6899, Train Acc: 0.5458 | Val Loss: 0.6870, Val Acc: 0.5553\n",
      "Epoch 9/10 | Train Loss: 0.6895, Train Acc: 0.5439 | Val Loss: 0.6883, Val Acc: 0.5553\n",
      "Epoch 10/10 | Train Loss: 0.6892, Train Acc: 0.5463 | Val Loss: 0.6876, Val Acc: 0.5553\n",
      "Saved model for experiment 3.\n",
      "Experiment 3: Final Validation Accuracy: 0.5553\n",
      "Experiment 3: Test Accuracy: 0.5343\n",
      "Experiment 4: 데이터 구간 [10000:35000]\n",
      "                         open      high       low     close  open_Bin_0  \\\n",
      "2017-09-25 12:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 13:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 17:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 19:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 20:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "...                       ...       ...       ...       ...         ...   \n",
      "2024-12-18 12:00:00  0.000000  0.000000  0.000000  0.000000         1.0   \n",
      "2024-12-18 13:00:00  0.000000  0.000000  0.000000  0.000000         1.0   \n",
      "2024-12-18 14:00:00  0.000000  0.000000  0.199575  0.000000         1.0   \n",
      "2024-12-18 15:00:00  0.000000  0.000000  0.226983  0.036865         1.0   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.109052         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 17:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 19:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 20:00:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 14:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 15:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:00:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_Bin_91  close_Bin_92  \\\n",
      "2017-09-25 12:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 13:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 17:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 19:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 20:00:00         0.0  ...           0.0           0.0   \n",
      "...                         ...  ...           ...           ...   \n",
      "2024-12-18 12:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 13:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 14:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 15:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:00:00         1.0  ...           0.0           0.0   \n",
      "\n",
      "                     close_Bin_93  close_Bin_94  close_Bin_95  close_Bin_96  \\\n",
      "2017-09-25 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 13:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 17:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 19:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 20:00:00           0.0           0.0           0.0           0.0   \n",
      "...                           ...           ...           ...           ...   \n",
      "2024-12-18 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 13:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 14:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 15:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:00:00           0.0           0.0           0.0           0.0   \n",
      "\n",
      "                     close_Bin_97  close_Bin_98  close_Bin_99  close_target  \n",
      "2017-09-25 12:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 13:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 17:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 19:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 20:00:00           0.0           0.0           1.0      1.000000  \n",
      "...                           ...           ...           ...           ...  \n",
      "2024-12-18 12:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 13:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 14:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 15:00:00           0.0           0.0           0.0      0.036865  \n",
      "2024-12-18 16:00:00           0.0           0.0           0.0      0.109052  \n",
      "\n",
      "[63283 rows x 405 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 3 for fine-tuning.\n",
      "Experiment 4: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6894, Train Acc: 0.5468 | Val Loss: 0.6911, Val Acc: 0.5343\n",
      "Epoch 2/10 | Train Loss: 0.6896, Train Acc: 0.5466 | Val Loss: 0.6941, Val Acc: 0.5343\n",
      "Epoch 3/10 | Train Loss: 0.6894, Train Acc: 0.5477 | Val Loss: 0.6947, Val Acc: 0.5343\n",
      "Epoch 4/10 | Train Loss: 0.6899, Train Acc: 0.5463 | Val Loss: 0.6909, Val Acc: 0.5343\n",
      "Epoch 5/10 | Train Loss: 0.6895, Train Acc: 0.5467 | Val Loss: 0.6909, Val Acc: 0.5343\n",
      "Epoch 6/10 | Train Loss: 0.6889, Train Acc: 0.5481 | Val Loss: 0.6911, Val Acc: 0.5343\n",
      "Epoch 7/10 | Train Loss: 0.6895, Train Acc: 0.5472 | Val Loss: 0.6912, Val Acc: 0.5343\n",
      "Epoch 8/10 | Train Loss: 0.6894, Train Acc: 0.5479 | Val Loss: 0.6920, Val Acc: 0.5343\n",
      "Epoch 9/10 | Train Loss: 0.6892, Train Acc: 0.5477 | Val Loss: 0.6909, Val Acc: 0.5343\n",
      "Epoch 10/10 | Train Loss: 0.6889, Train Acc: 0.5478 | Val Loss: 0.6922, Val Acc: 0.5343\n",
      "Saved model for experiment 4.\n",
      "Experiment 4: Final Validation Accuracy: 0.5343\n",
      "Experiment 4: Test Accuracy: 0.5428\n",
      "Experiment 5: 데이터 구간 [12500:37500]\n",
      "                         open      high       low     close  open_Bin_0  \\\n",
      "2017-09-25 12:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 13:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 17:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 19:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 20:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "...                       ...       ...       ...       ...         ...   \n",
      "2024-12-18 12:00:00  0.000000  0.000000  0.000000  0.000000         1.0   \n",
      "2024-12-18 13:00:00  0.000000  0.000000  0.000000  0.000000         1.0   \n",
      "2024-12-18 14:00:00  0.000000  0.000000  0.199575  0.000000         1.0   \n",
      "2024-12-18 15:00:00  0.000000  0.000000  0.226983  0.036865         1.0   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.109052         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 17:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 19:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 20:00:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 14:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 15:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:00:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_Bin_91  close_Bin_92  \\\n",
      "2017-09-25 12:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 13:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 17:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 19:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 20:00:00         0.0  ...           0.0           0.0   \n",
      "...                         ...  ...           ...           ...   \n",
      "2024-12-18 12:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 13:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 14:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 15:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:00:00         1.0  ...           0.0           0.0   \n",
      "\n",
      "                     close_Bin_93  close_Bin_94  close_Bin_95  close_Bin_96  \\\n",
      "2017-09-25 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 13:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 17:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 19:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 20:00:00           0.0           0.0           0.0           0.0   \n",
      "...                           ...           ...           ...           ...   \n",
      "2024-12-18 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 13:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 14:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 15:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:00:00           0.0           0.0           0.0           0.0   \n",
      "\n",
      "                     close_Bin_97  close_Bin_98  close_Bin_99  close_target  \n",
      "2017-09-25 12:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 13:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 17:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 19:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 20:00:00           0.0           0.0           1.0      1.000000  \n",
      "...                           ...           ...           ...           ...  \n",
      "2024-12-18 12:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 13:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 14:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 15:00:00           0.0           0.0           0.0      0.036865  \n",
      "2024-12-18 16:00:00           0.0           0.0           0.0      0.109052  \n",
      "\n",
      "[63283 rows x 405 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 4 for fine-tuning.\n",
      "Experiment 5: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6897, Train Acc: 0.5450 | Val Loss: 0.6913, Val Acc: 0.5428\n",
      "Epoch 2/10 | Train Loss: 0.6897, Train Acc: 0.5433 | Val Loss: 0.6900, Val Acc: 0.5428\n",
      "Epoch 3/10 | Train Loss: 0.6900, Train Acc: 0.5447 | Val Loss: 0.6895, Val Acc: 0.5428\n",
      "Epoch 4/10 | Train Loss: 0.6899, Train Acc: 0.5446 | Val Loss: 0.6898, Val Acc: 0.5428\n",
      "Epoch 5/10 | Train Loss: 0.6903, Train Acc: 0.5448 | Val Loss: 0.6897, Val Acc: 0.5428\n",
      "Epoch 6/10 | Train Loss: 0.6902, Train Acc: 0.5426 | Val Loss: 0.6896, Val Acc: 0.5428\n",
      "Epoch 7/10 | Train Loss: 0.6895, Train Acc: 0.5453 | Val Loss: 0.6896, Val Acc: 0.5428\n",
      "Epoch 8/10 | Train Loss: 0.6893, Train Acc: 0.5452 | Val Loss: 0.6901, Val Acc: 0.5428\n",
      "Epoch 9/10 | Train Loss: 0.6896, Train Acc: 0.5455 | Val Loss: 0.6903, Val Acc: 0.5428\n",
      "Epoch 10/10 | Train Loss: 0.6893, Train Acc: 0.5454 | Val Loss: 0.6927, Val Acc: 0.5428\n",
      "Saved model for experiment 5.\n",
      "Experiment 5: Final Validation Accuracy: 0.5428\n",
      "Experiment 5: Test Accuracy: 0.5679\n",
      "Experiment 6: 데이터 구간 [15000:40000]\n",
      "                         open      high       low     close  open_Bin_0  \\\n",
      "2017-09-25 12:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 13:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 17:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 19:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 20:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "...                       ...       ...       ...       ...         ...   \n",
      "2024-12-18 12:00:00  0.000000  0.000000  0.000000  0.000000         1.0   \n",
      "2024-12-18 13:00:00  0.000000  0.000000  0.000000  0.000000         1.0   \n",
      "2024-12-18 14:00:00  0.000000  0.000000  0.199575  0.000000         1.0   \n",
      "2024-12-18 15:00:00  0.000000  0.000000  0.226983  0.036865         1.0   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.109052         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 17:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 19:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 20:00:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 14:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 15:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:00:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_Bin_91  close_Bin_92  \\\n",
      "2017-09-25 12:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 13:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 17:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 19:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 20:00:00         0.0  ...           0.0           0.0   \n",
      "...                         ...  ...           ...           ...   \n",
      "2024-12-18 12:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 13:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 14:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 15:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:00:00         1.0  ...           0.0           0.0   \n",
      "\n",
      "                     close_Bin_93  close_Bin_94  close_Bin_95  close_Bin_96  \\\n",
      "2017-09-25 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 13:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 17:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 19:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 20:00:00           0.0           0.0           0.0           0.0   \n",
      "...                           ...           ...           ...           ...   \n",
      "2024-12-18 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 13:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 14:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 15:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:00:00           0.0           0.0           0.0           0.0   \n",
      "\n",
      "                     close_Bin_97  close_Bin_98  close_Bin_99  close_target  \n",
      "2017-09-25 12:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 13:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 17:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 19:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 20:00:00           0.0           0.0           1.0      1.000000  \n",
      "...                           ...           ...           ...           ...  \n",
      "2024-12-18 12:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 13:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 14:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 15:00:00           0.0           0.0           0.0      0.036865  \n",
      "2024-12-18 16:00:00           0.0           0.0           0.0      0.109052  \n",
      "\n",
      "[63283 rows x 405 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 5 for fine-tuning.\n",
      "Experiment 6: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6901, Train Acc: 0.5453 | Val Loss: 0.6841, Val Acc: 0.5679\n",
      "Epoch 2/10 | Train Loss: 0.6898, Train Acc: 0.5444 | Val Loss: 0.6846, Val Acc: 0.5679\n",
      "Epoch 3/10 | Train Loss: 0.6899, Train Acc: 0.5452 | Val Loss: 0.6852, Val Acc: 0.5679\n",
      "Epoch 4/10 | Train Loss: 0.6899, Train Acc: 0.5442 | Val Loss: 0.6853, Val Acc: 0.5679\n",
      "Epoch 5/10 | Train Loss: 0.6896, Train Acc: 0.5455 | Val Loss: 0.6844, Val Acc: 0.5679\n",
      "Epoch 6/10 | Train Loss: 0.6895, Train Acc: 0.5446 | Val Loss: 0.6858, Val Acc: 0.5679\n",
      "Epoch 7/10 | Train Loss: 0.6897, Train Acc: 0.5454 | Val Loss: 0.6849, Val Acc: 0.5679\n",
      "Epoch 8/10 | Train Loss: 0.6892, Train Acc: 0.5454 | Val Loss: 0.6846, Val Acc: 0.5679\n",
      "Epoch 9/10 | Train Loss: 0.6894, Train Acc: 0.5454 | Val Loss: 0.6852, Val Acc: 0.5679\n",
      "Epoch 10/10 | Train Loss: 0.6892, Train Acc: 0.5454 | Val Loss: 0.6853, Val Acc: 0.5679\n",
      "Saved model for experiment 6.\n",
      "Experiment 6: Final Validation Accuracy: 0.5679\n",
      "Experiment 6: Test Accuracy: 0.5355\n",
      "Experiment 7: 데이터 구간 [17500:42500]\n",
      "                         open      high       low     close  open_Bin_0  \\\n",
      "2017-09-25 12:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 13:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 17:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 19:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 20:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "...                       ...       ...       ...       ...         ...   \n",
      "2024-12-18 12:00:00  0.000000  0.000000  0.000000  0.000000         1.0   \n",
      "2024-12-18 13:00:00  0.000000  0.000000  0.000000  0.000000         1.0   \n",
      "2024-12-18 14:00:00  0.000000  0.000000  0.199575  0.000000         1.0   \n",
      "2024-12-18 15:00:00  0.000000  0.000000  0.226983  0.036865         1.0   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.109052         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 17:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 19:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 20:00:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 14:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 15:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:00:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_Bin_91  close_Bin_92  \\\n",
      "2017-09-25 12:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 13:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 17:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 19:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 20:00:00         0.0  ...           0.0           0.0   \n",
      "...                         ...  ...           ...           ...   \n",
      "2024-12-18 12:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 13:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 14:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 15:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:00:00         1.0  ...           0.0           0.0   \n",
      "\n",
      "                     close_Bin_93  close_Bin_94  close_Bin_95  close_Bin_96  \\\n",
      "2017-09-25 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 13:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 17:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 19:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 20:00:00           0.0           0.0           0.0           0.0   \n",
      "...                           ...           ...           ...           ...   \n",
      "2024-12-18 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 13:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 14:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 15:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:00:00           0.0           0.0           0.0           0.0   \n",
      "\n",
      "                     close_Bin_97  close_Bin_98  close_Bin_99  close_target  \n",
      "2017-09-25 12:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 13:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 17:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 19:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 20:00:00           0.0           0.0           1.0      1.000000  \n",
      "...                           ...           ...           ...           ...  \n",
      "2024-12-18 12:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 13:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 14:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 15:00:00           0.0           0.0           0.0      0.036865  \n",
      "2024-12-18 16:00:00           0.0           0.0           0.0      0.109052  \n",
      "\n",
      "[63283 rows x 405 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 6 for fine-tuning.\n",
      "Experiment 7: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6896, Train Acc: 0.5469 | Val Loss: 0.6910, Val Acc: 0.5355\n",
      "Epoch 2/10 | Train Loss: 0.6898, Train Acc: 0.5448 | Val Loss: 0.6961, Val Acc: 0.5355\n",
      "Epoch 3/10 | Train Loss: 0.6898, Train Acc: 0.5456 | Val Loss: 0.6913, Val Acc: 0.5355\n",
      "Epoch 4/10 | Train Loss: 0.6900, Train Acc: 0.5454 | Val Loss: 0.6910, Val Acc: 0.5355\n",
      "Epoch 5/10 | Train Loss: 0.6892, Train Acc: 0.5465 | Val Loss: 0.6906, Val Acc: 0.5355\n",
      "Epoch 6/10 | Train Loss: 0.6892, Train Acc: 0.5466 | Val Loss: 0.6913, Val Acc: 0.5355\n",
      "Epoch 7/10 | Train Loss: 0.6894, Train Acc: 0.5463 | Val Loss: 0.6907, Val Acc: 0.5355\n",
      "Epoch 8/10 | Train Loss: 0.6894, Train Acc: 0.5464 | Val Loss: 0.6915, Val Acc: 0.5355\n",
      "Epoch 9/10 | Train Loss: 0.6892, Train Acc: 0.5464 | Val Loss: 0.6920, Val Acc: 0.5355\n",
      "Epoch 10/10 | Train Loss: 0.6892, Train Acc: 0.5464 | Val Loss: 0.6908, Val Acc: 0.5355\n",
      "Saved model for experiment 7.\n",
      "Experiment 7: Final Validation Accuracy: 0.5355\n",
      "Experiment 7: Test Accuracy: 0.5460\n",
      "Experiment 8: 데이터 구간 [20000:45000]\n",
      "                         open      high       low     close  open_Bin_0  \\\n",
      "2017-09-25 12:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 13:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 17:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 19:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 20:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "...                       ...       ...       ...       ...         ...   \n",
      "2024-12-18 12:00:00  0.000000  0.000000  0.000000  0.000000         1.0   \n",
      "2024-12-18 13:00:00  0.000000  0.000000  0.000000  0.000000         1.0   \n",
      "2024-12-18 14:00:00  0.000000  0.000000  0.199575  0.000000         1.0   \n",
      "2024-12-18 15:00:00  0.000000  0.000000  0.226983  0.036865         1.0   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.109052         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 17:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 19:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 20:00:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 14:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 15:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:00:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_Bin_91  close_Bin_92  \\\n",
      "2017-09-25 12:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 13:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 17:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 19:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 20:00:00         0.0  ...           0.0           0.0   \n",
      "...                         ...  ...           ...           ...   \n",
      "2024-12-18 12:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 13:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 14:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 15:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:00:00         1.0  ...           0.0           0.0   \n",
      "\n",
      "                     close_Bin_93  close_Bin_94  close_Bin_95  close_Bin_96  \\\n",
      "2017-09-25 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 13:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 17:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 19:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 20:00:00           0.0           0.0           0.0           0.0   \n",
      "...                           ...           ...           ...           ...   \n",
      "2024-12-18 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 13:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 14:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 15:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:00:00           0.0           0.0           0.0           0.0   \n",
      "\n",
      "                     close_Bin_97  close_Bin_98  close_Bin_99  close_target  \n",
      "2017-09-25 12:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 13:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 17:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 19:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 20:00:00           0.0           0.0           1.0      1.000000  \n",
      "...                           ...           ...           ...           ...  \n",
      "2024-12-18 12:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 13:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 14:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 15:00:00           0.0           0.0           0.0      0.036865  \n",
      "2024-12-18 16:00:00           0.0           0.0           0.0      0.109052  \n",
      "\n",
      "[63283 rows x 405 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 7 for fine-tuning.\n",
      "Experiment 8: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6896, Train Acc: 0.5461 | Val Loss: 0.6889, Val Acc: 0.5460\n",
      "Epoch 2/10 | Train Loss: 0.6896, Train Acc: 0.5450 | Val Loss: 0.6891, Val Acc: 0.5460\n",
      "Epoch 3/10 | Train Loss: 0.6897, Train Acc: 0.5462 | Val Loss: 0.6888, Val Acc: 0.5460\n",
      "Epoch 4/10 | Train Loss: 0.6899, Train Acc: 0.5458 | Val Loss: 0.6889, Val Acc: 0.5460\n",
      "Epoch 5/10 | Train Loss: 0.6892, Train Acc: 0.5462 | Val Loss: 0.6893, Val Acc: 0.5460\n",
      "Epoch 6/10 | Train Loss: 0.6895, Train Acc: 0.5461 | Val Loss: 0.6889, Val Acc: 0.5460\n",
      "Epoch 7/10 | Train Loss: 0.6894, Train Acc: 0.5461 | Val Loss: 0.6890, Val Acc: 0.5460\n",
      "Epoch 8/10 | Train Loss: 0.6890, Train Acc: 0.5459 | Val Loss: 0.6889, Val Acc: 0.5460\n",
      "Epoch 9/10 | Train Loss: 0.6891, Train Acc: 0.5461 | Val Loss: 0.6891, Val Acc: 0.5460\n",
      "Epoch 10/10 | Train Loss: 0.6889, Train Acc: 0.5461 | Val Loss: 0.6889, Val Acc: 0.5460\n",
      "Saved model for experiment 8.\n",
      "Experiment 8: Final Validation Accuracy: 0.5460\n",
      "Experiment 8: Test Accuracy: 0.5238\n",
      "Experiment 9: 데이터 구간 [22500:47500]\n",
      "                         open      high       low     close  open_Bin_0  \\\n",
      "2017-09-25 12:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 13:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 17:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 19:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 20:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "...                       ...       ...       ...       ...         ...   \n",
      "2024-12-18 12:00:00  0.000000  0.000000  0.000000  0.000000         1.0   \n",
      "2024-12-18 13:00:00  0.000000  0.000000  0.000000  0.000000         1.0   \n",
      "2024-12-18 14:00:00  0.000000  0.000000  0.199575  0.000000         1.0   \n",
      "2024-12-18 15:00:00  0.000000  0.000000  0.226983  0.036865         1.0   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.109052         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 17:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 19:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 20:00:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 14:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 15:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:00:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_Bin_91  close_Bin_92  \\\n",
      "2017-09-25 12:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 13:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 17:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 19:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 20:00:00         0.0  ...           0.0           0.0   \n",
      "...                         ...  ...           ...           ...   \n",
      "2024-12-18 12:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 13:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 14:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 15:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:00:00         1.0  ...           0.0           0.0   \n",
      "\n",
      "                     close_Bin_93  close_Bin_94  close_Bin_95  close_Bin_96  \\\n",
      "2017-09-25 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 13:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 17:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 19:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 20:00:00           0.0           0.0           0.0           0.0   \n",
      "...                           ...           ...           ...           ...   \n",
      "2024-12-18 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 13:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 14:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 15:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:00:00           0.0           0.0           0.0           0.0   \n",
      "\n",
      "                     close_Bin_97  close_Bin_98  close_Bin_99  close_target  \n",
      "2017-09-25 12:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 13:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 17:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 19:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 20:00:00           0.0           0.0           1.0      1.000000  \n",
      "...                           ...           ...           ...           ...  \n",
      "2024-12-18 12:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 13:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 14:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 15:00:00           0.0           0.0           0.0      0.036865  \n",
      "2024-12-18 16:00:00           0.0           0.0           0.0      0.109052  \n",
      "\n",
      "[63283 rows x 405 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 8 for fine-tuning.\n",
      "Experiment 9: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6899, Train Acc: 0.5465 | Val Loss: 0.6976, Val Acc: 0.5238\n",
      "Epoch 2/10 | Train Loss: 0.6899, Train Acc: 0.5466 | Val Loss: 0.6956, Val Acc: 0.5238\n",
      "Epoch 3/10 | Train Loss: 0.6901, Train Acc: 0.5460 | Val Loss: 0.6924, Val Acc: 0.5238\n",
      "Epoch 4/10 | Train Loss: 0.6896, Train Acc: 0.5461 | Val Loss: 0.6999, Val Acc: 0.5238\n",
      "Epoch 5/10 | Train Loss: 0.6895, Train Acc: 0.5467 | Val Loss: 0.6920, Val Acc: 0.5238\n",
      "Epoch 6/10 | Train Loss: 0.6896, Train Acc: 0.5446 | Val Loss: 0.6922, Val Acc: 0.5238\n",
      "Epoch 7/10 | Train Loss: 0.6895, Train Acc: 0.5456 | Val Loss: 0.6921, Val Acc: 0.5238\n",
      "Epoch 8/10 | Train Loss: 0.6897, Train Acc: 0.5467 | Val Loss: 0.6949, Val Acc: 0.5238\n",
      "Epoch 9/10 | Train Loss: 0.6892, Train Acc: 0.5467 | Val Loss: 0.6920, Val Acc: 0.5238\n",
      "Epoch 10/10 | Train Loss: 0.6894, Train Acc: 0.5466 | Val Loss: 0.6923, Val Acc: 0.5238\n",
      "Saved model for experiment 9.\n",
      "Experiment 9: Final Validation Accuracy: 0.5238\n",
      "Experiment 9: Test Accuracy: 0.5331\n",
      "Experiment 10: 데이터 구간 [25000:50000]\n",
      "                         open      high       low     close  open_Bin_0  \\\n",
      "2017-09-25 12:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 13:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 17:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 19:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 20:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "...                       ...       ...       ...       ...         ...   \n",
      "2024-12-18 12:00:00  0.000000  0.000000  0.000000  0.000000         1.0   \n",
      "2024-12-18 13:00:00  0.000000  0.000000  0.000000  0.000000         1.0   \n",
      "2024-12-18 14:00:00  0.000000  0.000000  0.199575  0.000000         1.0   \n",
      "2024-12-18 15:00:00  0.000000  0.000000  0.226983  0.036865         1.0   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.109052         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 17:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 19:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 20:00:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 14:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 15:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:00:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_Bin_91  close_Bin_92  \\\n",
      "2017-09-25 12:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 13:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 17:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 19:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 20:00:00         0.0  ...           0.0           0.0   \n",
      "...                         ...  ...           ...           ...   \n",
      "2024-12-18 12:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 13:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 14:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 15:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:00:00         1.0  ...           0.0           0.0   \n",
      "\n",
      "                     close_Bin_93  close_Bin_94  close_Bin_95  close_Bin_96  \\\n",
      "2017-09-25 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 13:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 17:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 19:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 20:00:00           0.0           0.0           0.0           0.0   \n",
      "...                           ...           ...           ...           ...   \n",
      "2024-12-18 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 13:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 14:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 15:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:00:00           0.0           0.0           0.0           0.0   \n",
      "\n",
      "                     close_Bin_97  close_Bin_98  close_Bin_99  close_target  \n",
      "2017-09-25 12:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 13:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 17:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 19:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 20:00:00           0.0           0.0           1.0      1.000000  \n",
      "...                           ...           ...           ...           ...  \n",
      "2024-12-18 12:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 13:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 14:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 15:00:00           0.0           0.0           0.0      0.036865  \n",
      "2024-12-18 16:00:00           0.0           0.0           0.0      0.109052  \n",
      "\n",
      "[63283 rows x 405 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 9 for fine-tuning.\n",
      "Experiment 10: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6901, Train Acc: 0.5434 | Val Loss: 0.6917, Val Acc: 0.5331\n",
      "Epoch 2/10 | Train Loss: 0.6901, Train Acc: 0.5432 | Val Loss: 0.6911, Val Acc: 0.5331\n",
      "Epoch 3/10 | Train Loss: 0.6900, Train Acc: 0.5414 | Val Loss: 0.6920, Val Acc: 0.5331\n",
      "Epoch 4/10 | Train Loss: 0.6901, Train Acc: 0.5434 | Val Loss: 0.6926, Val Acc: 0.5331\n",
      "Epoch 5/10 | Train Loss: 0.6901, Train Acc: 0.5432 | Val Loss: 0.6925, Val Acc: 0.5331\n",
      "Epoch 6/10 | Train Loss: 0.6900, Train Acc: 0.5431 | Val Loss: 0.6915, Val Acc: 0.5331\n",
      "Epoch 7/10 | Train Loss: 0.6898, Train Acc: 0.5435 | Val Loss: 0.6914, Val Acc: 0.5331\n",
      "Epoch 8/10 | Train Loss: 0.6896, Train Acc: 0.5436 | Val Loss: 0.6920, Val Acc: 0.5331\n",
      "Epoch 9/10 | Train Loss: 0.6896, Train Acc: 0.5435 | Val Loss: 0.6910, Val Acc: 0.5331\n",
      "Epoch 10/10 | Train Loss: 0.6895, Train Acc: 0.5434 | Val Loss: 0.6924, Val Acc: 0.5331\n",
      "Saved model for experiment 10.\n",
      "Experiment 10: Final Validation Accuracy: 0.5331\n",
      "Experiment 10: Test Accuracy: 0.5477\n",
      "Experiment 11: 데이터 구간 [27500:52500]\n",
      "                         open      high       low     close  open_Bin_0  \\\n",
      "2017-09-25 12:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 13:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 17:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 19:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 20:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "...                       ...       ...       ...       ...         ...   \n",
      "2024-12-18 12:00:00  0.000000  0.000000  0.000000  0.000000         1.0   \n",
      "2024-12-18 13:00:00  0.000000  0.000000  0.000000  0.000000         1.0   \n",
      "2024-12-18 14:00:00  0.000000  0.000000  0.199575  0.000000         1.0   \n",
      "2024-12-18 15:00:00  0.000000  0.000000  0.226983  0.036865         1.0   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.109052         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 17:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 19:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 20:00:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 14:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 15:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:00:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_Bin_91  close_Bin_92  \\\n",
      "2017-09-25 12:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 13:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 17:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 19:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 20:00:00         0.0  ...           0.0           0.0   \n",
      "...                         ...  ...           ...           ...   \n",
      "2024-12-18 12:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 13:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 14:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 15:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:00:00         1.0  ...           0.0           0.0   \n",
      "\n",
      "                     close_Bin_93  close_Bin_94  close_Bin_95  close_Bin_96  \\\n",
      "2017-09-25 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 13:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 17:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 19:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 20:00:00           0.0           0.0           0.0           0.0   \n",
      "...                           ...           ...           ...           ...   \n",
      "2024-12-18 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 13:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 14:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 15:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:00:00           0.0           0.0           0.0           0.0   \n",
      "\n",
      "                     close_Bin_97  close_Bin_98  close_Bin_99  close_target  \n",
      "2017-09-25 12:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 13:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 17:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 19:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 20:00:00           0.0           0.0           1.0      1.000000  \n",
      "...                           ...           ...           ...           ...  \n",
      "2024-12-18 12:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 13:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 14:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 15:00:00           0.0           0.0           0.0      0.036865  \n",
      "2024-12-18 16:00:00           0.0           0.0           0.0      0.109052  \n",
      "\n",
      "[63283 rows x 405 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 10 for fine-tuning.\n",
      "Experiment 11: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6902, Train Acc: 0.5422 | Val Loss: 0.6899, Val Acc: 0.5477\n",
      "Epoch 2/10 | Train Loss: 0.6904, Train Acc: 0.5413 | Val Loss: 0.6893, Val Acc: 0.5477\n",
      "Epoch 3/10 | Train Loss: 0.6903, Train Acc: 0.5419 | Val Loss: 0.6900, Val Acc: 0.5477\n",
      "Epoch 4/10 | Train Loss: 0.6904, Train Acc: 0.5401 | Val Loss: 0.6887, Val Acc: 0.5477\n",
      "Epoch 5/10 | Train Loss: 0.6903, Train Acc: 0.5427 | Val Loss: 0.6887, Val Acc: 0.5477\n",
      "Epoch 6/10 | Train Loss: 0.6904, Train Acc: 0.5419 | Val Loss: 0.6899, Val Acc: 0.5477\n",
      "Epoch 7/10 | Train Loss: 0.6899, Train Acc: 0.5410 | Val Loss: 0.6890, Val Acc: 0.5477\n",
      "Epoch 8/10 | Train Loss: 0.6901, Train Acc: 0.5422 | Val Loss: 0.6887, Val Acc: 0.5477\n",
      "Epoch 9/10 | Train Loss: 0.6901, Train Acc: 0.5422 | Val Loss: 0.6888, Val Acc: 0.5477\n",
      "Epoch 10/10 | Train Loss: 0.6901, Train Acc: 0.5423 | Val Loss: 0.6890, Val Acc: 0.5477\n",
      "Saved model for experiment 11.\n",
      "Experiment 11: Final Validation Accuracy: 0.5477\n",
      "Experiment 11: Test Accuracy: 0.5376\n",
      "Experiment 12: 데이터 구간 [30000:55000]\n",
      "                         open      high       low     close  open_Bin_0  \\\n",
      "2017-09-25 12:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 13:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 17:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 19:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 20:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "...                       ...       ...       ...       ...         ...   \n",
      "2024-12-18 12:00:00  0.000000  0.000000  0.000000  0.000000         1.0   \n",
      "2024-12-18 13:00:00  0.000000  0.000000  0.000000  0.000000         1.0   \n",
      "2024-12-18 14:00:00  0.000000  0.000000  0.199575  0.000000         1.0   \n",
      "2024-12-18 15:00:00  0.000000  0.000000  0.226983  0.036865         1.0   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.109052         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 17:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 19:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 20:00:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 14:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 15:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:00:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_Bin_91  close_Bin_92  \\\n",
      "2017-09-25 12:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 13:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 17:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 19:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 20:00:00         0.0  ...           0.0           0.0   \n",
      "...                         ...  ...           ...           ...   \n",
      "2024-12-18 12:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 13:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 14:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 15:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:00:00         1.0  ...           0.0           0.0   \n",
      "\n",
      "                     close_Bin_93  close_Bin_94  close_Bin_95  close_Bin_96  \\\n",
      "2017-09-25 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 13:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 17:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 19:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 20:00:00           0.0           0.0           0.0           0.0   \n",
      "...                           ...           ...           ...           ...   \n",
      "2024-12-18 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 13:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 14:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 15:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:00:00           0.0           0.0           0.0           0.0   \n",
      "\n",
      "                     close_Bin_97  close_Bin_98  close_Bin_99  close_target  \n",
      "2017-09-25 12:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 13:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 17:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 19:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 20:00:00           0.0           0.0           1.0      1.000000  \n",
      "...                           ...           ...           ...           ...  \n",
      "2024-12-18 12:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 13:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 14:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 15:00:00           0.0           0.0           0.0      0.036865  \n",
      "2024-12-18 16:00:00           0.0           0.0           0.0      0.109052  \n",
      "\n",
      "[63283 rows x 405 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 11 for fine-tuning.\n",
      "Experiment 12: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6907, Train Acc: 0.5409 | Val Loss: 0.6903, Val Acc: 0.5376\n",
      "Epoch 2/10 | Train Loss: 0.6902, Train Acc: 0.5403 | Val Loss: 0.6902, Val Acc: 0.5376\n",
      "Epoch 3/10 | Train Loss: 0.6906, Train Acc: 0.5407 | Val Loss: 0.6901, Val Acc: 0.5376\n",
      "Epoch 4/10 | Train Loss: 0.6903, Train Acc: 0.5410 | Val Loss: 0.6904, Val Acc: 0.5376\n",
      "Epoch 5/10 | Train Loss: 0.6901, Train Acc: 0.5414 | Val Loss: 0.6901, Val Acc: 0.5376\n",
      "Epoch 6/10 | Train Loss: 0.6904, Train Acc: 0.5406 | Val Loss: 0.6906, Val Acc: 0.5376\n",
      "Epoch 7/10 | Train Loss: 0.6904, Train Acc: 0.5408 | Val Loss: 0.6910, Val Acc: 0.5376\n",
      "Epoch 8/10 | Train Loss: 0.6905, Train Acc: 0.5410 | Val Loss: 0.6901, Val Acc: 0.5376\n",
      "Epoch 9/10 | Train Loss: 0.6899, Train Acc: 0.5416 | Val Loss: 0.6905, Val Acc: 0.5376\n",
      "Epoch 10/10 | Train Loss: 0.6901, Train Acc: 0.5411 | Val Loss: 0.6903, Val Acc: 0.5376\n",
      "Saved model for experiment 12.\n",
      "Experiment 12: Final Validation Accuracy: 0.5376\n",
      "Experiment 12: Test Accuracy: 0.5368\n",
      "Experiment 13: 데이터 구간 [32500:57500]\n",
      "                         open      high       low     close  open_Bin_0  \\\n",
      "2017-09-25 12:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 13:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 17:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 19:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 20:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "...                       ...       ...       ...       ...         ...   \n",
      "2024-12-18 12:00:00  0.000000  0.000000  0.000000  0.000000         1.0   \n",
      "2024-12-18 13:00:00  0.000000  0.000000  0.000000  0.000000         1.0   \n",
      "2024-12-18 14:00:00  0.000000  0.000000  0.199575  0.000000         1.0   \n",
      "2024-12-18 15:00:00  0.000000  0.000000  0.226983  0.036865         1.0   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.109052         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 17:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 19:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 20:00:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 14:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 15:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:00:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_Bin_91  close_Bin_92  \\\n",
      "2017-09-25 12:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 13:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 17:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 19:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 20:00:00         0.0  ...           0.0           0.0   \n",
      "...                         ...  ...           ...           ...   \n",
      "2024-12-18 12:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 13:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 14:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 15:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:00:00         1.0  ...           0.0           0.0   \n",
      "\n",
      "                     close_Bin_93  close_Bin_94  close_Bin_95  close_Bin_96  \\\n",
      "2017-09-25 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 13:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 17:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 19:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 20:00:00           0.0           0.0           0.0           0.0   \n",
      "...                           ...           ...           ...           ...   \n",
      "2024-12-18 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 13:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 14:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 15:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:00:00           0.0           0.0           0.0           0.0   \n",
      "\n",
      "                     close_Bin_97  close_Bin_98  close_Bin_99  close_target  \n",
      "2017-09-25 12:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 13:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 17:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 19:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 20:00:00           0.0           0.0           1.0      1.000000  \n",
      "...                           ...           ...           ...           ...  \n",
      "2024-12-18 12:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 13:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 14:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 15:00:00           0.0           0.0           0.0      0.036865  \n",
      "2024-12-18 16:00:00           0.0           0.0           0.0      0.109052  \n",
      "\n",
      "[63283 rows x 405 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 12 for fine-tuning.\n",
      "Experiment 13: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6902, Train Acc: 0.5415 | Val Loss: 0.6904, Val Acc: 0.5368\n",
      "Epoch 2/10 | Train Loss: 0.6903, Train Acc: 0.5414 | Val Loss: 0.6905, Val Acc: 0.5368\n",
      "Epoch 3/10 | Train Loss: 0.6904, Train Acc: 0.5412 | Val Loss: 0.6908, Val Acc: 0.5368\n",
      "Epoch 4/10 | Train Loss: 0.6903, Train Acc: 0.5395 | Val Loss: 0.6906, Val Acc: 0.5368\n",
      "Epoch 5/10 | Train Loss: 0.6901, Train Acc: 0.5416 | Val Loss: 0.6917, Val Acc: 0.5368\n",
      "Epoch 6/10 | Train Loss: 0.6903, Train Acc: 0.5420 | Val Loss: 0.6903, Val Acc: 0.5368\n",
      "Epoch 7/10 | Train Loss: 0.6899, Train Acc: 0.5417 | Val Loss: 0.6903, Val Acc: 0.5368\n",
      "Epoch 8/10 | Train Loss: 0.6897, Train Acc: 0.5416 | Val Loss: 0.6906, Val Acc: 0.5368\n",
      "Epoch 9/10 | Train Loss: 0.6897, Train Acc: 0.5416 | Val Loss: 0.6905, Val Acc: 0.5368\n",
      "Epoch 10/10 | Train Loss: 0.6900, Train Acc: 0.5418 | Val Loss: 0.6904, Val Acc: 0.5368\n",
      "Saved model for experiment 13.\n",
      "Experiment 13: Final Validation Accuracy: 0.5368\n",
      "Experiment 13: Test Accuracy: 0.5473\n",
      "Experiment 14: 데이터 구간 [35000:60000]\n",
      "                         open      high       low     close  open_Bin_0  \\\n",
      "2017-09-25 12:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 13:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 17:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 19:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 20:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "...                       ...       ...       ...       ...         ...   \n",
      "2024-12-18 12:00:00  0.000000  0.000000  0.000000  0.000000         1.0   \n",
      "2024-12-18 13:00:00  0.000000  0.000000  0.000000  0.000000         1.0   \n",
      "2024-12-18 14:00:00  0.000000  0.000000  0.199575  0.000000         1.0   \n",
      "2024-12-18 15:00:00  0.000000  0.000000  0.226983  0.036865         1.0   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.109052         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 17:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 19:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 20:00:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 14:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 15:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:00:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_Bin_91  close_Bin_92  \\\n",
      "2017-09-25 12:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 13:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 17:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 19:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 20:00:00         0.0  ...           0.0           0.0   \n",
      "...                         ...  ...           ...           ...   \n",
      "2024-12-18 12:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 13:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 14:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 15:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:00:00         1.0  ...           0.0           0.0   \n",
      "\n",
      "                     close_Bin_93  close_Bin_94  close_Bin_95  close_Bin_96  \\\n",
      "2017-09-25 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 13:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 17:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 19:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 20:00:00           0.0           0.0           0.0           0.0   \n",
      "...                           ...           ...           ...           ...   \n",
      "2024-12-18 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 13:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 14:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 15:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:00:00           0.0           0.0           0.0           0.0   \n",
      "\n",
      "                     close_Bin_97  close_Bin_98  close_Bin_99  close_target  \n",
      "2017-09-25 12:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 13:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 17:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 19:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 20:00:00           0.0           0.0           1.0      1.000000  \n",
      "...                           ...           ...           ...           ...  \n",
      "2024-12-18 12:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 13:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 14:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 15:00:00           0.0           0.0           0.0      0.036865  \n",
      "2024-12-18 16:00:00           0.0           0.0           0.0      0.109052  \n",
      "\n",
      "[63283 rows x 405 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 13 for fine-tuning.\n",
      "Experiment 14: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6906, Train Acc: 0.5399 | Val Loss: 0.6889, Val Acc: 0.5473\n",
      "Epoch 2/10 | Train Loss: 0.6903, Train Acc: 0.5409 | Val Loss: 0.6890, Val Acc: 0.5473\n",
      "Epoch 3/10 | Train Loss: 0.6903, Train Acc: 0.5408 | Val Loss: 0.6893, Val Acc: 0.5473\n",
      "Epoch 4/10 | Train Loss: 0.6909, Train Acc: 0.5404 | Val Loss: 0.6889, Val Acc: 0.5473\n",
      "Epoch 5/10 | Train Loss: 0.6899, Train Acc: 0.5411 | Val Loss: 0.6892, Val Acc: 0.5473\n",
      "Epoch 6/10 | Train Loss: 0.6904, Train Acc: 0.5401 | Val Loss: 0.6892, Val Acc: 0.5473\n",
      "Epoch 7/10 | Train Loss: 0.6902, Train Acc: 0.5409 | Val Loss: 0.6889, Val Acc: 0.5473\n",
      "Epoch 8/10 | Train Loss: 0.6899, Train Acc: 0.5409 | Val Loss: 0.6893, Val Acc: 0.5473\n",
      "Epoch 9/10 | Train Loss: 0.6904, Train Acc: 0.5408 | Val Loss: 0.6889, Val Acc: 0.5473\n",
      "Epoch 10/10 | Train Loss: 0.6898, Train Acc: 0.5410 | Val Loss: 0.6889, Val Acc: 0.5473\n",
      "Saved model for experiment 14.\n",
      "Experiment 14: Final Validation Accuracy: 0.5473\n",
      "Experiment 14: Test Accuracy: 0.5307\n",
      "Experiment 15: 데이터 구간 [37500:62500]\n",
      "                         open      high       low     close  open_Bin_0  \\\n",
      "2017-09-25 12:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 13:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 17:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 19:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 20:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "...                       ...       ...       ...       ...         ...   \n",
      "2024-12-18 12:00:00  0.000000  0.000000  0.000000  0.000000         1.0   \n",
      "2024-12-18 13:00:00  0.000000  0.000000  0.000000  0.000000         1.0   \n",
      "2024-12-18 14:00:00  0.000000  0.000000  0.199575  0.000000         1.0   \n",
      "2024-12-18 15:00:00  0.000000  0.000000  0.226983  0.036865         1.0   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.109052         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 17:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 19:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 20:00:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 14:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 15:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:00:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_Bin_91  close_Bin_92  \\\n",
      "2017-09-25 12:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 13:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 17:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 19:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 20:00:00         0.0  ...           0.0           0.0   \n",
      "...                         ...  ...           ...           ...   \n",
      "2024-12-18 12:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 13:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 14:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 15:00:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:00:00         1.0  ...           0.0           0.0   \n",
      "\n",
      "                     close_Bin_93  close_Bin_94  close_Bin_95  close_Bin_96  \\\n",
      "2017-09-25 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 13:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 17:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 19:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 20:00:00           0.0           0.0           0.0           0.0   \n",
      "...                           ...           ...           ...           ...   \n",
      "2024-12-18 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 13:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 14:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 15:00:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:00:00           0.0           0.0           0.0           0.0   \n",
      "\n",
      "                     close_Bin_97  close_Bin_98  close_Bin_99  close_target  \n",
      "2017-09-25 12:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 13:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 17:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 19:00:00           0.0           0.0           1.0      1.000000  \n",
      "2017-09-25 20:00:00           0.0           0.0           1.0      1.000000  \n",
      "...                           ...           ...           ...           ...  \n",
      "2024-12-18 12:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 13:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 14:00:00           0.0           0.0           0.0      0.000000  \n",
      "2024-12-18 15:00:00           0.0           0.0           0.0      0.036865  \n",
      "2024-12-18 16:00:00           0.0           0.0           0.0      0.109052  \n",
      "\n",
      "[63283 rows x 405 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 14 for fine-tuning.\n",
      "Experiment 15: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6912, Train Acc: 0.5374 | Val Loss: 0.6913, Val Acc: 0.5307\n",
      "Epoch 2/10 | Train Loss: 0.6906, Train Acc: 0.5358 | Val Loss: 0.6919, Val Acc: 0.5307\n",
      "Epoch 3/10 | Train Loss: 0.6911, Train Acc: 0.5368 | Val Loss: 0.6913, Val Acc: 0.5307\n",
      "Epoch 4/10 | Train Loss: 0.6909, Train Acc: 0.5374 | Val Loss: 0.6936, Val Acc: 0.5307\n",
      "Epoch 5/10 | Train Loss: 0.6906, Train Acc: 0.5381 | Val Loss: 0.6917, Val Acc: 0.5307\n",
      "Epoch 6/10 | Train Loss: 0.6907, Train Acc: 0.5380 | Val Loss: 0.6913, Val Acc: 0.5307\n",
      "Epoch 7/10 | Train Loss: 0.6906, Train Acc: 0.5382 | Val Loss: 0.6917, Val Acc: 0.5307\n",
      "Epoch 8/10 | Train Loss: 0.6903, Train Acc: 0.5384 | Val Loss: 0.6913, Val Acc: 0.5307\n",
      "Epoch 9/10 | Train Loss: 0.6906, Train Acc: 0.5383 | Val Loss: 0.6913, Val Acc: 0.5307\n",
      "Epoch 10/10 | Train Loss: 0.6906, Train Acc: 0.5383 | Val Loss: 0.6913, Val Acc: 0.5307\n",
      "Saved model for experiment 15.\n",
      "Experiment 15: Final Validation Accuracy: 0.5307\n",
      "Experiment 15: Test Accuracy: 0.5456\n",
      "\n",
      "Final Average Validation Accuracy: 0.5422\n",
      "Final Average Test Accuracy: 0.5423\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from math import sqrt\n",
    "\n",
    "# rolling minmax scaling 함수 (window=24)\n",
    "def rolling_minmax_scale(series, window=24):\n",
    "    roll_min = series.rolling(window=window, min_periods=window).min()\n",
    "    roll_max = series.rolling(window=window, min_periods=window).max()\n",
    "    scaled = (series - roll_min) / ((roll_max - roll_min) + 1e-8)\n",
    "    scaled = scaled.replace([np.inf, -np.inf], np.nan)\n",
    "    scaled = scaled.fillna(1.0)\n",
    "    return scaled.clip(upper=1.0)\n",
    "\n",
    "# binning 및 one-hot 인코딩 함수 (결과를 정수 0,1로)\n",
    "def bin_and_encode(data, features, bins=100, drop_original=True):\n",
    "    for feature in features:\n",
    "        # pd.cut에 bins로 지정된 구간을 사용 (여기서는 자동으로 최소~최대 값)\n",
    "        data[f'{feature}_Bin'] = pd.cut(data[feature], bins=bins, labels=False)\n",
    "        one_hot = pd.get_dummies(data[f'{feature}_Bin'], prefix=f'{feature}_Bin').astype(np.int32)\n",
    "        expected_columns = [f'{feature}_Bin_{i}' for i in range(bins)]\n",
    "        one_hot = one_hot.reindex(columns=expected_columns, fill_value=0)\n",
    "        data = pd.concat([data, one_hot], axis=1)\n",
    "        if drop_original:\n",
    "            data.drop(columns=[f'{feature}_Bin'], inplace=True)\n",
    "    # 개별 열 단위로 float32로 변환\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    for col in numeric_cols:\n",
    "        data[col] = data[col].astype(np.float32)\n",
    "    return data\n",
    "\n",
    "####################################\n",
    "# Dataset 및 모델 정의 (나머지 구성은 동일)\n",
    "####################################\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, input_data, target_data, lookback=24):\n",
    "        self.input_data = input_data.values\n",
    "        self.target_data = target_data.values\n",
    "        self.lookback = lookback\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_data) - self.lookback\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.input_data[idx: idx + self.lookback, :]\n",
    "        y = self.target_data[idx + self.lookback, 0]\n",
    "        y_prev = self.target_data[idx + self.lookback - 1, 0]\n",
    "        y_target = 1 if y > y_prev else 0\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y_target, dtype=torch.long)\n",
    "\n",
    "class EncoderOnlyTransformerCustom(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=512, num_layers=6, nhead=8, ffn_dim=2048, num_classes=2, max_seq_len=24):\n",
    "        super(EncoderOnlyTransformerCustom, self).__init__()\n",
    "        self.token_embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embedding_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=nhead, dim_feedforward=ffn_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x = self.token_embedding(x)\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        pos_emb = self.position_embedding(positions)\n",
    "        x = x + pos_emb\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.transformer_encoder(x)\n",
    "        return self.fc(x[-1, :, :])\n",
    "\n",
    "####################################\n",
    "# 데이터 로드 및 Variant 1 실행: OHLC 원본 데이터로 실험\n",
    "####################################\n",
    "# CSV 파일 로드 – OHLC 4개 데이터만 사용\n",
    "data = pd.read_csv(\"BTC_upbit_KRW_min60.csv\", index_col=0)\n",
    "data = data[['open', 'high', 'low', 'close']]\n",
    "data.index = pd.to_datetime(data.index)\n",
    "\n",
    "# 각 OHLC 열에 대해 rolling minmax scaling (window=24)\n",
    "ohlc_features = ['open', 'high', 'low', 'close']\n",
    "for feature in ohlc_features:\n",
    "    data[feature] = rolling_minmax_scale(data[feature], window=24)\n",
    "\n",
    "# One-hot 인코딩: 각 OHLC 열은 100구간으로 나눔\n",
    "data = bin_and_encode(data, ohlc_features, bins=100, drop_original=True)\n",
    "\n",
    "# 타깃은 원본 close 값 사용 (실험 목적)\n",
    "data['close_target'] = data['close']\n",
    "data = data.dropna()\n",
    "\n",
    "# 최종 입력은 _Bin_ 접미사 열들만 사용\n",
    "final_input_columns = [col for col in data.columns if '_Bin_' in col]\n",
    "final_target_column = ['close_target']\n",
    "\n",
    "data_input = data[final_input_columns]\n",
    "data_target = data[final_target_column]\n",
    "\n",
    "# # (아래는 모델 학습/평가 루프 코드 – 생략 가능)\n",
    "# print(\"Variant 1 - Raw OHLC with rolling minmax scaling and one-hot encoding\")\n",
    "# print(\"Final input shape:\", data_input.shape)\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return total_loss / len(data_loader), correct / total\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, lr, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_state = None\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, device)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = model.state_dict()\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(data, num_experiments=16, lookback=24, num_epochs=10):\n",
    "    # 최종 입력: 모든 _Bin_ 컬럼 + datetime one-hot + positional encoding + order 인코딩 컬럼\n",
    "    final_input_columns = [col for col in data.columns if '_Bin_' in col]\n",
    "    # datetime_onehot_features = [col for col in data.columns if col.startswith('Hour_') or \n",
    "    #                               col.startswith('Day_') or col.startswith('Week_') or \n",
    "    #                               col.startswith('Month_')]\n",
    "    # datetime_positional_features = ['hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'week_sin', 'week_cos', 'month_sin', 'month_cos']\n",
    "    # order_encoding_features = [col for col in data.columns if '_order_' in col]\n",
    "    # final_input_columns.extend(datetime_onehot_features)\n",
    "    # final_input_columns.extend(datetime_positional_features)\n",
    "    # final_input_columns.extend(order_encoding_features)\n",
    "    target_cols = ['close_target']\n",
    "    \n",
    "    data_input = data[final_input_columns]\n",
    "    data_target = data[target_cols]\n",
    "    \n",
    "    data_input = data_input.apply(pd.to_numeric).astype(np.float32)\n",
    "    data_target = data_target.apply(pd.to_numeric).astype(np.float32)\n",
    "    \n",
    "    step_size = 2500\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    val_acc_list = []\n",
    "    test_acc_list = []\n",
    "    \n",
    "    for exp in range(num_experiments):\n",
    "        train_start = exp * step_size\n",
    "        train_end = train_start + step_size * 8\n",
    "        val_end = train_end + step_size\n",
    "        test_end = val_end + step_size\n",
    "        if test_end > len(data_input):\n",
    "            break\n",
    "        print(f\"Experiment {exp}: 데이터 구간 [{train_start}:{test_end}]\")\n",
    "        print(data)\n",
    "        data.to_csv('BTC60_one_hot.csv')\n",
    "        \n",
    "        train_input = data_input.iloc[train_start:train_end]\n",
    "        train_target = data_target.iloc[train_start:train_end]\n",
    "        val_input = data_input.iloc[train_end:val_end]\n",
    "        val_target = data_target.iloc[train_end:val_end]\n",
    "        test_input = data_input.iloc[val_end:test_end]\n",
    "        test_target = data_target.iloc[val_end:test_end]\n",
    "        \n",
    "        train_dataset = TimeSeriesDataset(train_input, train_target, lookback=lookback)\n",
    "        val_dataset = TimeSeriesDataset(val_input, val_target, lookback=lookback)\n",
    "        test_dataset = TimeSeriesDataset(test_input, test_target, lookback=lookback)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        lr = 1e-4\n",
    "        input_dim = data_input.shape[1]\n",
    "        model = EncoderOnlyTransformerCustom(input_dim=input_dim, embedding_dim=512, num_layers=6, nhead=8,\n",
    "                                               ffn_dim=2048, num_classes=2, max_seq_len=lookback).to(device)\n",
    "        model_path = f\"model_experiment_{exp}.pth\"\n",
    "        if exp > 0:\n",
    "            try:\n",
    "                model.load_state_dict(torch.load(f\"model_experiment_{exp - 1}.pth\"))\n",
    "                print(f\"Loaded model from experiment {exp - 1} for fine-tuning.\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Model file for experiment {exp - 1} not found. Starting fresh training.\")\n",
    "        \n",
    "        print(f\"Experiment {exp}: Training with lr={lr} (Fine-Tuning)\")\n",
    "        model = train_model(model, train_loader, val_loader, num_epochs, lr, device)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Saved model for experiment {exp}.\")\n",
    "        \n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, device)\n",
    "        val_acc_list.append(val_acc)\n",
    "        print(f\"Experiment {exp}: Final Validation Accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "        test_loss, test_acc = evaluate_model(model, test_loader, device)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(f\"Experiment {exp}: Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    if val_acc_list:\n",
    "        avg_val_acc = sum(val_acc_list) / len(val_acc_list)\n",
    "        avg_test_acc = sum(test_acc_list) / len(test_acc_list)\n",
    "        print(f\"\\nFinal Average Validation Accuracy: {avg_val_acc:.4f}\")\n",
    "        print(f\"Final Average Test Accuracy: {avg_test_acc:.4f}\")\n",
    "    else:\n",
    "        print(\"실험이 한 번도 실행되지 않았습니다.\")\n",
    "\n",
    "# 참조용 리스트 (최종 구성에는 _Bin_ 컬럼들이 포함됨)\n",
    "# features_pct = [f + '_pct' for f in features_to_bin]\n",
    "# features_minmax = [f + '_minmax' for f in features_to_bin]\n",
    "\n",
    "train_and_evaluate(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_12720\\1345690871.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['close_target'] = data['close']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 0: 데이터 구간 [0:25000]\n",
      "                            open         high          low        close  \\\n",
      "2017-09-25 13:00:00    4222000.0    4235000.0    4208000.0    4235000.0   \n",
      "2017-09-25 17:00:00    4235000.0    4235000.0    4235000.0    4235000.0   \n",
      "2017-09-25 19:00:00    4213000.0    4213000.0    4187000.0    4204000.0   \n",
      "2017-09-25 20:00:00    4204000.0    4204000.0    4175000.0    4191000.0   \n",
      "2017-09-25 21:00:00    4191000.0    4218000.0    4183000.0    4211000.0   \n",
      "...                          ...          ...          ...          ...   \n",
      "2024-12-18 12:00:00  153867008.0  153870000.0  153000000.0  153452000.0   \n",
      "2024-12-18 13:00:00  153451008.0  153452000.0  150348000.0  152200000.0   \n",
      "2024-12-18 14:00:00  152200000.0  152800000.0  151382000.0  151994000.0   \n",
      "2024-12-18 15:00:00  151904992.0  152500000.0  151524000.0  152184992.0   \n",
      "2024-12-18 16:00:00  152186000.0  152696992.0  151999008.0  152559008.0   \n",
      "\n",
      "                     open_pct  high_pct   low_pct  close_pct  open_pct_Bin_0  \\\n",
      "2017-09-25 13:00:00  0.499881 -0.212064  0.405631   0.785340             0.0   \n",
      "2017-09-25 17:00:00  0.307911  0.000000  0.641635   0.000000             0.0   \n",
      "2017-09-25 19:00:00 -0.519481 -0.519481 -1.133412  -0.731995             0.0   \n",
      "2017-09-25 20:00:00 -0.213624 -0.213624 -0.286601  -0.309229             0.0   \n",
      "2017-09-25 21:00:00 -0.309229  0.333016  0.191617   0.477213             0.0   \n",
      "...                       ...       ...       ...        ...             ...   \n",
      "2024-12-18 12:00:00 -0.342625 -0.536522 -0.552486  -0.268417             0.0   \n",
      "2024-12-18 13:00:00 -0.270363 -0.271658 -1.733333  -0.815890             0.0   \n",
      "2024-12-18 14:00:00 -0.815244 -0.424889  0.687738  -0.135348             0.0   \n",
      "2024-12-18 15:00:00 -0.193824 -0.196335  0.093802   0.125663             0.0   \n",
      "2024-12-18 16:00:00  0.184984  0.129180  0.313482   0.245754             0.0   \n",
      "\n",
      "                     open_pct_Bin_1  ...  close_pct_Bin_91  close_pct_Bin_92  \\\n",
      "2017-09-25 13:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 17:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 19:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 20:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 21:00:00             0.0  ...               0.0               0.0   \n",
      "...                             ...  ...               ...               ...   \n",
      "2024-12-18 12:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 13:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 14:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 15:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 16:00:00             0.0  ...               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_93  close_pct_Bin_94  close_pct_Bin_95  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 12:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 13:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 14:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 15:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_96  close_pct_Bin_97  close_pct_Bin_98  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 12:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 13:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 14:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 15:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_99  close_target  \n",
      "2017-09-25 13:00:00               0.0     4235000.0  \n",
      "2017-09-25 17:00:00               0.0     4235000.0  \n",
      "2017-09-25 19:00:00               0.0     4204000.0  \n",
      "2017-09-25 20:00:00               0.0     4191000.0  \n",
      "2017-09-25 21:00:00               0.0     4211000.0  \n",
      "...                               ...           ...  \n",
      "2024-12-18 12:00:00               0.0   153452000.0  \n",
      "2024-12-18 13:00:00               0.0   152200000.0  \n",
      "2024-12-18 14:00:00               0.0   151994000.0  \n",
      "2024-12-18 15:00:00               0.0   152184992.0  \n",
      "2024-12-18 16:00:00               0.0   152559008.0  \n",
      "\n",
      "[63282 rows x 409 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 0: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.7163, Train Acc: 0.4974 | Val Loss: 0.6942, Val Acc: 0.5053\n",
      "Epoch 2/10 | Train Loss: 0.6978, Train Acc: 0.4957 | Val Loss: 0.6940, Val Acc: 0.4947\n",
      "Epoch 3/10 | Train Loss: 0.6973, Train Acc: 0.5009 | Val Loss: 0.6933, Val Acc: 0.5053\n",
      "Epoch 4/10 | Train Loss: 0.6983, Train Acc: 0.4948 | Val Loss: 0.6976, Val Acc: 0.4947\n",
      "Epoch 5/10 | Train Loss: 0.6959, Train Acc: 0.5022 | Val Loss: 0.6936, Val Acc: 0.4947\n",
      "Epoch 6/10 | Train Loss: 0.6956, Train Acc: 0.4996 | Val Loss: 0.6947, Val Acc: 0.4947\n",
      "Epoch 7/10 | Train Loss: 0.6944, Train Acc: 0.5051 | Val Loss: 0.6940, Val Acc: 0.4947\n",
      "Epoch 8/10 | Train Loss: 0.6949, Train Acc: 0.5021 | Val Loss: 0.6937, Val Acc: 0.4947\n",
      "Epoch 9/10 | Train Loss: 0.6943, Train Acc: 0.4964 | Val Loss: 0.6950, Val Acc: 0.4947\n",
      "Epoch 10/10 | Train Loss: 0.6938, Train Acc: 0.4996 | Val Loss: 0.6931, Val Acc: 0.5053\n",
      "Saved model for experiment 0.\n",
      "Experiment 0: Final Validation Accuracy: 0.5053\n",
      "Experiment 0: Test Accuracy: 0.4883\n",
      "Experiment 1: 데이터 구간 [2500:27500]\n",
      "                            open         high          low        close  \\\n",
      "2017-09-25 13:00:00    4222000.0    4235000.0    4208000.0    4235000.0   \n",
      "2017-09-25 17:00:00    4235000.0    4235000.0    4235000.0    4235000.0   \n",
      "2017-09-25 19:00:00    4213000.0    4213000.0    4187000.0    4204000.0   \n",
      "2017-09-25 20:00:00    4204000.0    4204000.0    4175000.0    4191000.0   \n",
      "2017-09-25 21:00:00    4191000.0    4218000.0    4183000.0    4211000.0   \n",
      "...                          ...          ...          ...          ...   \n",
      "2024-12-18 12:00:00  153867008.0  153870000.0  153000000.0  153452000.0   \n",
      "2024-12-18 13:00:00  153451008.0  153452000.0  150348000.0  152200000.0   \n",
      "2024-12-18 14:00:00  152200000.0  152800000.0  151382000.0  151994000.0   \n",
      "2024-12-18 15:00:00  151904992.0  152500000.0  151524000.0  152184992.0   \n",
      "2024-12-18 16:00:00  152186000.0  152696992.0  151999008.0  152559008.0   \n",
      "\n",
      "                     open_pct  high_pct   low_pct  close_pct  open_pct_Bin_0  \\\n",
      "2017-09-25 13:00:00  0.499881 -0.212064  0.405631   0.785340             0.0   \n",
      "2017-09-25 17:00:00  0.307911  0.000000  0.641635   0.000000             0.0   \n",
      "2017-09-25 19:00:00 -0.519481 -0.519481 -1.133412  -0.731995             0.0   \n",
      "2017-09-25 20:00:00 -0.213624 -0.213624 -0.286601  -0.309229             0.0   \n",
      "2017-09-25 21:00:00 -0.309229  0.333016  0.191617   0.477213             0.0   \n",
      "...                       ...       ...       ...        ...             ...   \n",
      "2024-12-18 12:00:00 -0.342625 -0.536522 -0.552486  -0.268417             0.0   \n",
      "2024-12-18 13:00:00 -0.270363 -0.271658 -1.733333  -0.815890             0.0   \n",
      "2024-12-18 14:00:00 -0.815244 -0.424889  0.687738  -0.135348             0.0   \n",
      "2024-12-18 15:00:00 -0.193824 -0.196335  0.093802   0.125663             0.0   \n",
      "2024-12-18 16:00:00  0.184984  0.129180  0.313482   0.245754             0.0   \n",
      "\n",
      "                     open_pct_Bin_1  ...  close_pct_Bin_91  close_pct_Bin_92  \\\n",
      "2017-09-25 13:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 17:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 19:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 20:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 21:00:00             0.0  ...               0.0               0.0   \n",
      "...                             ...  ...               ...               ...   \n",
      "2024-12-18 12:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 13:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 14:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 15:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 16:00:00             0.0  ...               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_93  close_pct_Bin_94  close_pct_Bin_95  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 12:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 13:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 14:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 15:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_96  close_pct_Bin_97  close_pct_Bin_98  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 12:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 13:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 14:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 15:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_99  close_target  \n",
      "2017-09-25 13:00:00               0.0     4235000.0  \n",
      "2017-09-25 17:00:00               0.0     4235000.0  \n",
      "2017-09-25 19:00:00               0.0     4204000.0  \n",
      "2017-09-25 20:00:00               0.0     4191000.0  \n",
      "2017-09-25 21:00:00               0.0     4211000.0  \n",
      "...                               ...           ...  \n",
      "2024-12-18 12:00:00               0.0   153452000.0  \n",
      "2024-12-18 13:00:00               0.0   152200000.0  \n",
      "2024-12-18 14:00:00               0.0   151994000.0  \n",
      "2024-12-18 15:00:00               0.0   152184992.0  \n",
      "2024-12-18 16:00:00               0.0   152559008.0  \n",
      "\n",
      "[63282 rows x 409 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 0 for fine-tuning.\n",
      "Experiment 1: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6958, Train Acc: 0.4954 | Val Loss: 0.6944, Val Acc: 0.4883\n",
      "Epoch 2/10 | Train Loss: 0.6950, Train Acc: 0.4998 | Val Loss: 0.6936, Val Acc: 0.5117\n",
      "Epoch 3/10 | Train Loss: 0.6949, Train Acc: 0.5004 | Val Loss: 0.6974, Val Acc: 0.4883\n",
      "Epoch 4/10 | Train Loss: 0.6949, Train Acc: 0.4981 | Val Loss: 0.6939, Val Acc: 0.4883\n",
      "Epoch 5/10 | Train Loss: 0.6944, Train Acc: 0.5032 | Val Loss: 0.6952, Val Acc: 0.5117\n",
      "Epoch 6/10 | Train Loss: 0.6939, Train Acc: 0.4983 | Val Loss: 0.6938, Val Acc: 0.5117\n",
      "Epoch 7/10 | Train Loss: 0.6941, Train Acc: 0.4967 | Val Loss: 0.6929, Val Acc: 0.5117\n",
      "Epoch 8/10 | Train Loss: 0.6939, Train Acc: 0.4986 | Val Loss: 0.6932, Val Acc: 0.5117\n",
      "Epoch 9/10 | Train Loss: 0.6938, Train Acc: 0.4974 | Val Loss: 0.6932, Val Acc: 0.4883\n",
      "Epoch 10/10 | Train Loss: 0.6938, Train Acc: 0.5047 | Val Loss: 0.6932, Val Acc: 0.5117\n",
      "Saved model for experiment 1.\n",
      "Experiment 1: Final Validation Accuracy: 0.5117\n",
      "Experiment 1: Test Accuracy: 0.4996\n",
      "Experiment 2: 데이터 구간 [5000:30000]\n",
      "                            open         high          low        close  \\\n",
      "2017-09-25 13:00:00    4222000.0    4235000.0    4208000.0    4235000.0   \n",
      "2017-09-25 17:00:00    4235000.0    4235000.0    4235000.0    4235000.0   \n",
      "2017-09-25 19:00:00    4213000.0    4213000.0    4187000.0    4204000.0   \n",
      "2017-09-25 20:00:00    4204000.0    4204000.0    4175000.0    4191000.0   \n",
      "2017-09-25 21:00:00    4191000.0    4218000.0    4183000.0    4211000.0   \n",
      "...                          ...          ...          ...          ...   \n",
      "2024-12-18 12:00:00  153867008.0  153870000.0  153000000.0  153452000.0   \n",
      "2024-12-18 13:00:00  153451008.0  153452000.0  150348000.0  152200000.0   \n",
      "2024-12-18 14:00:00  152200000.0  152800000.0  151382000.0  151994000.0   \n",
      "2024-12-18 15:00:00  151904992.0  152500000.0  151524000.0  152184992.0   \n",
      "2024-12-18 16:00:00  152186000.0  152696992.0  151999008.0  152559008.0   \n",
      "\n",
      "                     open_pct  high_pct   low_pct  close_pct  open_pct_Bin_0  \\\n",
      "2017-09-25 13:00:00  0.499881 -0.212064  0.405631   0.785340             0.0   \n",
      "2017-09-25 17:00:00  0.307911  0.000000  0.641635   0.000000             0.0   \n",
      "2017-09-25 19:00:00 -0.519481 -0.519481 -1.133412  -0.731995             0.0   \n",
      "2017-09-25 20:00:00 -0.213624 -0.213624 -0.286601  -0.309229             0.0   \n",
      "2017-09-25 21:00:00 -0.309229  0.333016  0.191617   0.477213             0.0   \n",
      "...                       ...       ...       ...        ...             ...   \n",
      "2024-12-18 12:00:00 -0.342625 -0.536522 -0.552486  -0.268417             0.0   \n",
      "2024-12-18 13:00:00 -0.270363 -0.271658 -1.733333  -0.815890             0.0   \n",
      "2024-12-18 14:00:00 -0.815244 -0.424889  0.687738  -0.135348             0.0   \n",
      "2024-12-18 15:00:00 -0.193824 -0.196335  0.093802   0.125663             0.0   \n",
      "2024-12-18 16:00:00  0.184984  0.129180  0.313482   0.245754             0.0   \n",
      "\n",
      "                     open_pct_Bin_1  ...  close_pct_Bin_91  close_pct_Bin_92  \\\n",
      "2017-09-25 13:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 17:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 19:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 20:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 21:00:00             0.0  ...               0.0               0.0   \n",
      "...                             ...  ...               ...               ...   \n",
      "2024-12-18 12:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 13:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 14:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 15:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 16:00:00             0.0  ...               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_93  close_pct_Bin_94  close_pct_Bin_95  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 12:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 13:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 14:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 15:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_96  close_pct_Bin_97  close_pct_Bin_98  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 12:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 13:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 14:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 15:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_99  close_target  \n",
      "2017-09-25 13:00:00               0.0     4235000.0  \n",
      "2017-09-25 17:00:00               0.0     4235000.0  \n",
      "2017-09-25 19:00:00               0.0     4204000.0  \n",
      "2017-09-25 20:00:00               0.0     4191000.0  \n",
      "2017-09-25 21:00:00               0.0     4211000.0  \n",
      "...                               ...           ...  \n",
      "2024-12-18 12:00:00               0.0   153452000.0  \n",
      "2024-12-18 13:00:00               0.0   152200000.0  \n",
      "2024-12-18 14:00:00               0.0   151994000.0  \n",
      "2024-12-18 15:00:00               0.0   152184992.0  \n",
      "2024-12-18 16:00:00               0.0   152559008.0  \n",
      "\n",
      "[63282 rows x 409 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 1 for fine-tuning.\n",
      "Experiment 2: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6942, Train Acc: 0.4957 | Val Loss: 0.6938, Val Acc: 0.4996\n",
      "Epoch 2/10 | Train Loss: 0.6942, Train Acc: 0.5015 | Val Loss: 0.6937, Val Acc: 0.4996\n",
      "Epoch 3/10 | Train Loss: 0.6938, Train Acc: 0.5005 | Val Loss: 0.6958, Val Acc: 0.4996\n",
      "Epoch 4/10 | Train Loss: 0.6941, Train Acc: 0.5050 | Val Loss: 0.6932, Val Acc: 0.4996\n",
      "Epoch 5/10 | Train Loss: 0.6940, Train Acc: 0.5012 | Val Loss: 0.6947, Val Acc: 0.5004\n",
      "Epoch 6/10 | Train Loss: 0.6938, Train Acc: 0.5009 | Val Loss: 0.6944, Val Acc: 0.4996\n",
      "Epoch 7/10 | Train Loss: 0.6940, Train Acc: 0.4993 | Val Loss: 0.6932, Val Acc: 0.4996\n",
      "Epoch 8/10 | Train Loss: 0.6935, Train Acc: 0.5012 | Val Loss: 0.6941, Val Acc: 0.4996\n",
      "Epoch 9/10 | Train Loss: 0.6937, Train Acc: 0.5045 | Val Loss: 0.6932, Val Acc: 0.4996\n",
      "Epoch 10/10 | Train Loss: 0.6938, Train Acc: 0.4972 | Val Loss: 0.6936, Val Acc: 0.4996\n",
      "Saved model for experiment 2.\n",
      "Experiment 2: Final Validation Accuracy: 0.4996\n",
      "Experiment 2: Test Accuracy: 0.4891\n",
      "Experiment 3: 데이터 구간 [7500:32500]\n",
      "                            open         high          low        close  \\\n",
      "2017-09-25 13:00:00    4222000.0    4235000.0    4208000.0    4235000.0   \n",
      "2017-09-25 17:00:00    4235000.0    4235000.0    4235000.0    4235000.0   \n",
      "2017-09-25 19:00:00    4213000.0    4213000.0    4187000.0    4204000.0   \n",
      "2017-09-25 20:00:00    4204000.0    4204000.0    4175000.0    4191000.0   \n",
      "2017-09-25 21:00:00    4191000.0    4218000.0    4183000.0    4211000.0   \n",
      "...                          ...          ...          ...          ...   \n",
      "2024-12-18 12:00:00  153867008.0  153870000.0  153000000.0  153452000.0   \n",
      "2024-12-18 13:00:00  153451008.0  153452000.0  150348000.0  152200000.0   \n",
      "2024-12-18 14:00:00  152200000.0  152800000.0  151382000.0  151994000.0   \n",
      "2024-12-18 15:00:00  151904992.0  152500000.0  151524000.0  152184992.0   \n",
      "2024-12-18 16:00:00  152186000.0  152696992.0  151999008.0  152559008.0   \n",
      "\n",
      "                     open_pct  high_pct   low_pct  close_pct  open_pct_Bin_0  \\\n",
      "2017-09-25 13:00:00  0.499881 -0.212064  0.405631   0.785340             0.0   \n",
      "2017-09-25 17:00:00  0.307911  0.000000  0.641635   0.000000             0.0   \n",
      "2017-09-25 19:00:00 -0.519481 -0.519481 -1.133412  -0.731995             0.0   \n",
      "2017-09-25 20:00:00 -0.213624 -0.213624 -0.286601  -0.309229             0.0   \n",
      "2017-09-25 21:00:00 -0.309229  0.333016  0.191617   0.477213             0.0   \n",
      "...                       ...       ...       ...        ...             ...   \n",
      "2024-12-18 12:00:00 -0.342625 -0.536522 -0.552486  -0.268417             0.0   \n",
      "2024-12-18 13:00:00 -0.270363 -0.271658 -1.733333  -0.815890             0.0   \n",
      "2024-12-18 14:00:00 -0.815244 -0.424889  0.687738  -0.135348             0.0   \n",
      "2024-12-18 15:00:00 -0.193824 -0.196335  0.093802   0.125663             0.0   \n",
      "2024-12-18 16:00:00  0.184984  0.129180  0.313482   0.245754             0.0   \n",
      "\n",
      "                     open_pct_Bin_1  ...  close_pct_Bin_91  close_pct_Bin_92  \\\n",
      "2017-09-25 13:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 17:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 19:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 20:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 21:00:00             0.0  ...               0.0               0.0   \n",
      "...                             ...  ...               ...               ...   \n",
      "2024-12-18 12:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 13:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 14:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 15:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 16:00:00             0.0  ...               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_93  close_pct_Bin_94  close_pct_Bin_95  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 12:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 13:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 14:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 15:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_96  close_pct_Bin_97  close_pct_Bin_98  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 12:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 13:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 14:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 15:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_99  close_target  \n",
      "2017-09-25 13:00:00               0.0     4235000.0  \n",
      "2017-09-25 17:00:00               0.0     4235000.0  \n",
      "2017-09-25 19:00:00               0.0     4204000.0  \n",
      "2017-09-25 20:00:00               0.0     4191000.0  \n",
      "2017-09-25 21:00:00               0.0     4211000.0  \n",
      "...                               ...           ...  \n",
      "2024-12-18 12:00:00               0.0   153452000.0  \n",
      "2024-12-18 13:00:00               0.0   152200000.0  \n",
      "2024-12-18 14:00:00               0.0   151994000.0  \n",
      "2024-12-18 15:00:00               0.0   152184992.0  \n",
      "2024-12-18 16:00:00               0.0   152559008.0  \n",
      "\n",
      "[63282 rows x 409 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 2 for fine-tuning.\n",
      "Experiment 3: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6939, Train Acc: 0.4954 | Val Loss: 0.6931, Val Acc: 0.5109\n",
      "Epoch 2/10 | Train Loss: 0.6937, Train Acc: 0.4997 | Val Loss: 0.6930, Val Acc: 0.5109\n",
      "Epoch 3/10 | Train Loss: 0.6938, Train Acc: 0.5036 | Val Loss: 0.6931, Val Acc: 0.5109\n",
      "Epoch 4/10 | Train Loss: 0.6939, Train Acc: 0.4971 | Val Loss: 0.6935, Val Acc: 0.4891\n",
      "Epoch 5/10 | Train Loss: 0.6938, Train Acc: 0.4990 | Val Loss: 0.6934, Val Acc: 0.4891\n",
      "Epoch 6/10 | Train Loss: 0.6936, Train Acc: 0.5021 | Val Loss: 0.6932, Val Acc: 0.4891\n",
      "Epoch 7/10 | Train Loss: 0.6936, Train Acc: 0.5004 | Val Loss: 0.6930, Val Acc: 0.5109\n",
      "Epoch 8/10 | Train Loss: 0.6936, Train Acc: 0.4987 | Val Loss: 0.6930, Val Acc: 0.5109\n",
      "Epoch 9/10 | Train Loss: 0.6931, Train Acc: 0.5103 | Val Loss: 0.6938, Val Acc: 0.4891\n",
      "Epoch 10/10 | Train Loss: 0.6933, Train Acc: 0.5005 | Val Loss: 0.6937, Val Acc: 0.4891\n",
      "Saved model for experiment 3.\n",
      "Experiment 3: Final Validation Accuracy: 0.4891\n",
      "Experiment 3: Test Accuracy: 0.4943\n",
      "Experiment 4: 데이터 구간 [10000:35000]\n",
      "                            open         high          low        close  \\\n",
      "2017-09-25 13:00:00    4222000.0    4235000.0    4208000.0    4235000.0   \n",
      "2017-09-25 17:00:00    4235000.0    4235000.0    4235000.0    4235000.0   \n",
      "2017-09-25 19:00:00    4213000.0    4213000.0    4187000.0    4204000.0   \n",
      "2017-09-25 20:00:00    4204000.0    4204000.0    4175000.0    4191000.0   \n",
      "2017-09-25 21:00:00    4191000.0    4218000.0    4183000.0    4211000.0   \n",
      "...                          ...          ...          ...          ...   \n",
      "2024-12-18 12:00:00  153867008.0  153870000.0  153000000.0  153452000.0   \n",
      "2024-12-18 13:00:00  153451008.0  153452000.0  150348000.0  152200000.0   \n",
      "2024-12-18 14:00:00  152200000.0  152800000.0  151382000.0  151994000.0   \n",
      "2024-12-18 15:00:00  151904992.0  152500000.0  151524000.0  152184992.0   \n",
      "2024-12-18 16:00:00  152186000.0  152696992.0  151999008.0  152559008.0   \n",
      "\n",
      "                     open_pct  high_pct   low_pct  close_pct  open_pct_Bin_0  \\\n",
      "2017-09-25 13:00:00  0.499881 -0.212064  0.405631   0.785340             0.0   \n",
      "2017-09-25 17:00:00  0.307911  0.000000  0.641635   0.000000             0.0   \n",
      "2017-09-25 19:00:00 -0.519481 -0.519481 -1.133412  -0.731995             0.0   \n",
      "2017-09-25 20:00:00 -0.213624 -0.213624 -0.286601  -0.309229             0.0   \n",
      "2017-09-25 21:00:00 -0.309229  0.333016  0.191617   0.477213             0.0   \n",
      "...                       ...       ...       ...        ...             ...   \n",
      "2024-12-18 12:00:00 -0.342625 -0.536522 -0.552486  -0.268417             0.0   \n",
      "2024-12-18 13:00:00 -0.270363 -0.271658 -1.733333  -0.815890             0.0   \n",
      "2024-12-18 14:00:00 -0.815244 -0.424889  0.687738  -0.135348             0.0   \n",
      "2024-12-18 15:00:00 -0.193824 -0.196335  0.093802   0.125663             0.0   \n",
      "2024-12-18 16:00:00  0.184984  0.129180  0.313482   0.245754             0.0   \n",
      "\n",
      "                     open_pct_Bin_1  ...  close_pct_Bin_91  close_pct_Bin_92  \\\n",
      "2017-09-25 13:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 17:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 19:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 20:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 21:00:00             0.0  ...               0.0               0.0   \n",
      "...                             ...  ...               ...               ...   \n",
      "2024-12-18 12:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 13:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 14:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 15:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 16:00:00             0.0  ...               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_93  close_pct_Bin_94  close_pct_Bin_95  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 12:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 13:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 14:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 15:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_96  close_pct_Bin_97  close_pct_Bin_98  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 12:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 13:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 14:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 15:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_99  close_target  \n",
      "2017-09-25 13:00:00               0.0     4235000.0  \n",
      "2017-09-25 17:00:00               0.0     4235000.0  \n",
      "2017-09-25 19:00:00               0.0     4204000.0  \n",
      "2017-09-25 20:00:00               0.0     4191000.0  \n",
      "2017-09-25 21:00:00               0.0     4211000.0  \n",
      "...                               ...           ...  \n",
      "2024-12-18 12:00:00               0.0   153452000.0  \n",
      "2024-12-18 13:00:00               0.0   152200000.0  \n",
      "2024-12-18 14:00:00               0.0   151994000.0  \n",
      "2024-12-18 15:00:00               0.0   152184992.0  \n",
      "2024-12-18 16:00:00               0.0   152559008.0  \n",
      "\n",
      "[63282 rows x 409 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 3 for fine-tuning.\n",
      "Experiment 4: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6943, Train Acc: 0.4976 | Val Loss: 0.6931, Val Acc: 0.5057\n",
      "Epoch 2/10 | Train Loss: 0.6936, Train Acc: 0.5027 | Val Loss: 0.6956, Val Acc: 0.4943\n",
      "Epoch 3/10 | Train Loss: 0.6941, Train Acc: 0.4942 | Val Loss: 0.6967, Val Acc: 0.4943\n",
      "Epoch 4/10 | Train Loss: 0.6937, Train Acc: 0.5010 | Val Loss: 0.6941, Val Acc: 0.5057\n",
      "Epoch 5/10 | Train Loss: 0.6934, Train Acc: 0.5057 | Val Loss: 0.6956, Val Acc: 0.5057\n",
      "Epoch 6/10 | Train Loss: 0.6938, Train Acc: 0.4996 | Val Loss: 0.6931, Val Acc: 0.5057\n",
      "Epoch 7/10 | Train Loss: 0.6936, Train Acc: 0.4990 | Val Loss: 0.6932, Val Acc: 0.4943\n",
      "Epoch 8/10 | Train Loss: 0.6934, Train Acc: 0.5031 | Val Loss: 0.6931, Val Acc: 0.5057\n",
      "Epoch 9/10 | Train Loss: 0.6936, Train Acc: 0.4985 | Val Loss: 0.6934, Val Acc: 0.4943\n",
      "Epoch 10/10 | Train Loss: 0.6933, Train Acc: 0.4999 | Val Loss: 0.6933, Val Acc: 0.4943\n",
      "Saved model for experiment 4.\n",
      "Experiment 4: Final Validation Accuracy: 0.4943\n",
      "Experiment 4: Test Accuracy: 0.4952\n",
      "Experiment 5: 데이터 구간 [12500:37500]\n",
      "                            open         high          low        close  \\\n",
      "2017-09-25 13:00:00    4222000.0    4235000.0    4208000.0    4235000.0   \n",
      "2017-09-25 17:00:00    4235000.0    4235000.0    4235000.0    4235000.0   \n",
      "2017-09-25 19:00:00    4213000.0    4213000.0    4187000.0    4204000.0   \n",
      "2017-09-25 20:00:00    4204000.0    4204000.0    4175000.0    4191000.0   \n",
      "2017-09-25 21:00:00    4191000.0    4218000.0    4183000.0    4211000.0   \n",
      "...                          ...          ...          ...          ...   \n",
      "2024-12-18 12:00:00  153867008.0  153870000.0  153000000.0  153452000.0   \n",
      "2024-12-18 13:00:00  153451008.0  153452000.0  150348000.0  152200000.0   \n",
      "2024-12-18 14:00:00  152200000.0  152800000.0  151382000.0  151994000.0   \n",
      "2024-12-18 15:00:00  151904992.0  152500000.0  151524000.0  152184992.0   \n",
      "2024-12-18 16:00:00  152186000.0  152696992.0  151999008.0  152559008.0   \n",
      "\n",
      "                     open_pct  high_pct   low_pct  close_pct  open_pct_Bin_0  \\\n",
      "2017-09-25 13:00:00  0.499881 -0.212064  0.405631   0.785340             0.0   \n",
      "2017-09-25 17:00:00  0.307911  0.000000  0.641635   0.000000             0.0   \n",
      "2017-09-25 19:00:00 -0.519481 -0.519481 -1.133412  -0.731995             0.0   \n",
      "2017-09-25 20:00:00 -0.213624 -0.213624 -0.286601  -0.309229             0.0   \n",
      "2017-09-25 21:00:00 -0.309229  0.333016  0.191617   0.477213             0.0   \n",
      "...                       ...       ...       ...        ...             ...   \n",
      "2024-12-18 12:00:00 -0.342625 -0.536522 -0.552486  -0.268417             0.0   \n",
      "2024-12-18 13:00:00 -0.270363 -0.271658 -1.733333  -0.815890             0.0   \n",
      "2024-12-18 14:00:00 -0.815244 -0.424889  0.687738  -0.135348             0.0   \n",
      "2024-12-18 15:00:00 -0.193824 -0.196335  0.093802   0.125663             0.0   \n",
      "2024-12-18 16:00:00  0.184984  0.129180  0.313482   0.245754             0.0   \n",
      "\n",
      "                     open_pct_Bin_1  ...  close_pct_Bin_91  close_pct_Bin_92  \\\n",
      "2017-09-25 13:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 17:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 19:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 20:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 21:00:00             0.0  ...               0.0               0.0   \n",
      "...                             ...  ...               ...               ...   \n",
      "2024-12-18 12:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 13:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 14:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 15:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 16:00:00             0.0  ...               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_93  close_pct_Bin_94  close_pct_Bin_95  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 12:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 13:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 14:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 15:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_96  close_pct_Bin_97  close_pct_Bin_98  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 12:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 13:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 14:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 15:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_99  close_target  \n",
      "2017-09-25 13:00:00               0.0     4235000.0  \n",
      "2017-09-25 17:00:00               0.0     4235000.0  \n",
      "2017-09-25 19:00:00               0.0     4204000.0  \n",
      "2017-09-25 20:00:00               0.0     4191000.0  \n",
      "2017-09-25 21:00:00               0.0     4211000.0  \n",
      "...                               ...           ...  \n",
      "2024-12-18 12:00:00               0.0   153452000.0  \n",
      "2024-12-18 13:00:00               0.0   152200000.0  \n",
      "2024-12-18 14:00:00               0.0   151994000.0  \n",
      "2024-12-18 15:00:00               0.0   152184992.0  \n",
      "2024-12-18 16:00:00               0.0   152559008.0  \n",
      "\n",
      "[63282 rows x 409 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 4 for fine-tuning.\n",
      "Experiment 5: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6938, Train Acc: 0.4985 | Val Loss: 0.6943, Val Acc: 0.5048\n",
      "Epoch 2/10 | Train Loss: 0.6938, Train Acc: 0.5017 | Val Loss: 0.6934, Val Acc: 0.4952\n",
      "Epoch 3/10 | Train Loss: 0.6941, Train Acc: 0.4978 | Val Loss: 0.6932, Val Acc: 0.5048\n",
      "Epoch 4/10 | Train Loss: 0.6940, Train Acc: 0.4973 | Val Loss: 0.6935, Val Acc: 0.4952\n",
      "Epoch 5/10 | Train Loss: 0.6941, Train Acc: 0.4987 | Val Loss: 0.6931, Val Acc: 0.5048\n",
      "Epoch 6/10 | Train Loss: 0.6940, Train Acc: 0.4966 | Val Loss: 0.6932, Val Acc: 0.5048\n",
      "Epoch 7/10 | Train Loss: 0.6936, Train Acc: 0.4990 | Val Loss: 0.6939, Val Acc: 0.4952\n",
      "Epoch 8/10 | Train Loss: 0.6933, Train Acc: 0.5034 | Val Loss: 0.6931, Val Acc: 0.5048\n",
      "Epoch 9/10 | Train Loss: 0.6936, Train Acc: 0.4957 | Val Loss: 0.6932, Val Acc: 0.4952\n",
      "Epoch 10/10 | Train Loss: 0.6939, Train Acc: 0.4947 | Val Loss: 0.6931, Val Acc: 0.5048\n",
      "Saved model for experiment 5.\n",
      "Experiment 5: Final Validation Accuracy: 0.5048\n",
      "Experiment 5: Test Accuracy: 0.4758\n",
      "Experiment 6: 데이터 구간 [15000:40000]\n",
      "                            open         high          low        close  \\\n",
      "2017-09-25 13:00:00    4222000.0    4235000.0    4208000.0    4235000.0   \n",
      "2017-09-25 17:00:00    4235000.0    4235000.0    4235000.0    4235000.0   \n",
      "2017-09-25 19:00:00    4213000.0    4213000.0    4187000.0    4204000.0   \n",
      "2017-09-25 20:00:00    4204000.0    4204000.0    4175000.0    4191000.0   \n",
      "2017-09-25 21:00:00    4191000.0    4218000.0    4183000.0    4211000.0   \n",
      "...                          ...          ...          ...          ...   \n",
      "2024-12-18 12:00:00  153867008.0  153870000.0  153000000.0  153452000.0   \n",
      "2024-12-18 13:00:00  153451008.0  153452000.0  150348000.0  152200000.0   \n",
      "2024-12-18 14:00:00  152200000.0  152800000.0  151382000.0  151994000.0   \n",
      "2024-12-18 15:00:00  151904992.0  152500000.0  151524000.0  152184992.0   \n",
      "2024-12-18 16:00:00  152186000.0  152696992.0  151999008.0  152559008.0   \n",
      "\n",
      "                     open_pct  high_pct   low_pct  close_pct  open_pct_Bin_0  \\\n",
      "2017-09-25 13:00:00  0.499881 -0.212064  0.405631   0.785340             0.0   \n",
      "2017-09-25 17:00:00  0.307911  0.000000  0.641635   0.000000             0.0   \n",
      "2017-09-25 19:00:00 -0.519481 -0.519481 -1.133412  -0.731995             0.0   \n",
      "2017-09-25 20:00:00 -0.213624 -0.213624 -0.286601  -0.309229             0.0   \n",
      "2017-09-25 21:00:00 -0.309229  0.333016  0.191617   0.477213             0.0   \n",
      "...                       ...       ...       ...        ...             ...   \n",
      "2024-12-18 12:00:00 -0.342625 -0.536522 -0.552486  -0.268417             0.0   \n",
      "2024-12-18 13:00:00 -0.270363 -0.271658 -1.733333  -0.815890             0.0   \n",
      "2024-12-18 14:00:00 -0.815244 -0.424889  0.687738  -0.135348             0.0   \n",
      "2024-12-18 15:00:00 -0.193824 -0.196335  0.093802   0.125663             0.0   \n",
      "2024-12-18 16:00:00  0.184984  0.129180  0.313482   0.245754             0.0   \n",
      "\n",
      "                     open_pct_Bin_1  ...  close_pct_Bin_91  close_pct_Bin_92  \\\n",
      "2017-09-25 13:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 17:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 19:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 20:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 21:00:00             0.0  ...               0.0               0.0   \n",
      "...                             ...  ...               ...               ...   \n",
      "2024-12-18 12:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 13:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 14:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 15:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 16:00:00             0.0  ...               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_93  close_pct_Bin_94  close_pct_Bin_95  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 12:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 13:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 14:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 15:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_96  close_pct_Bin_97  close_pct_Bin_98  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 12:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 13:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 14:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 15:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_99  close_target  \n",
      "2017-09-25 13:00:00               0.0     4235000.0  \n",
      "2017-09-25 17:00:00               0.0     4235000.0  \n",
      "2017-09-25 19:00:00               0.0     4204000.0  \n",
      "2017-09-25 20:00:00               0.0     4191000.0  \n",
      "2017-09-25 21:00:00               0.0     4211000.0  \n",
      "...                               ...           ...  \n",
      "2024-12-18 12:00:00               0.0   153452000.0  \n",
      "2024-12-18 13:00:00               0.0   152200000.0  \n",
      "2024-12-18 14:00:00               0.0   151994000.0  \n",
      "2024-12-18 15:00:00               0.0   152184992.0  \n",
      "2024-12-18 16:00:00               0.0   152559008.0  \n",
      "\n",
      "[63282 rows x 409 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 5 for fine-tuning.\n",
      "Experiment 6: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6938, Train Acc: 0.5002 | Val Loss: 0.6961, Val Acc: 0.4758\n",
      "Epoch 2/10 | Train Loss: 0.6937, Train Acc: 0.5069 | Val Loss: 0.6920, Val Acc: 0.5242\n",
      "Epoch 3/10 | Train Loss: 0.6936, Train Acc: 0.4993 | Val Loss: 0.6937, Val Acc: 0.4758\n",
      "Epoch 4/10 | Train Loss: 0.6944, Train Acc: 0.4958 | Val Loss: 0.6956, Val Acc: 0.4758\n",
      "Epoch 5/10 | Train Loss: 0.6938, Train Acc: 0.4967 | Val Loss: 0.6922, Val Acc: 0.5242\n",
      "Epoch 6/10 | Train Loss: 0.6936, Train Acc: 0.5041 | Val Loss: 0.6934, Val Acc: 0.4758\n",
      "Epoch 7/10 | Train Loss: 0.6934, Train Acc: 0.5033 | Val Loss: 0.6933, Val Acc: 0.4758\n",
      "Epoch 8/10 | Train Loss: 0.6934, Train Acc: 0.5028 | Val Loss: 0.6932, Val Acc: 0.4758\n",
      "Epoch 9/10 | Train Loss: 0.6935, Train Acc: 0.5005 | Val Loss: 0.6929, Val Acc: 0.5242\n",
      "Epoch 10/10 | Train Loss: 0.6934, Train Acc: 0.4993 | Val Loss: 0.6928, Val Acc: 0.5242\n",
      "Saved model for experiment 6.\n",
      "Experiment 6: Final Validation Accuracy: 0.5242\n",
      "Experiment 6: Test Accuracy: 0.5000\n",
      "Experiment 7: 데이터 구간 [17500:42500]\n",
      "                            open         high          low        close  \\\n",
      "2017-09-25 13:00:00    4222000.0    4235000.0    4208000.0    4235000.0   \n",
      "2017-09-25 17:00:00    4235000.0    4235000.0    4235000.0    4235000.0   \n",
      "2017-09-25 19:00:00    4213000.0    4213000.0    4187000.0    4204000.0   \n",
      "2017-09-25 20:00:00    4204000.0    4204000.0    4175000.0    4191000.0   \n",
      "2017-09-25 21:00:00    4191000.0    4218000.0    4183000.0    4211000.0   \n",
      "...                          ...          ...          ...          ...   \n",
      "2024-12-18 12:00:00  153867008.0  153870000.0  153000000.0  153452000.0   \n",
      "2024-12-18 13:00:00  153451008.0  153452000.0  150348000.0  152200000.0   \n",
      "2024-12-18 14:00:00  152200000.0  152800000.0  151382000.0  151994000.0   \n",
      "2024-12-18 15:00:00  151904992.0  152500000.0  151524000.0  152184992.0   \n",
      "2024-12-18 16:00:00  152186000.0  152696992.0  151999008.0  152559008.0   \n",
      "\n",
      "                     open_pct  high_pct   low_pct  close_pct  open_pct_Bin_0  \\\n",
      "2017-09-25 13:00:00  0.499881 -0.212064  0.405631   0.785340             0.0   \n",
      "2017-09-25 17:00:00  0.307911  0.000000  0.641635   0.000000             0.0   \n",
      "2017-09-25 19:00:00 -0.519481 -0.519481 -1.133412  -0.731995             0.0   \n",
      "2017-09-25 20:00:00 -0.213624 -0.213624 -0.286601  -0.309229             0.0   \n",
      "2017-09-25 21:00:00 -0.309229  0.333016  0.191617   0.477213             0.0   \n",
      "...                       ...       ...       ...        ...             ...   \n",
      "2024-12-18 12:00:00 -0.342625 -0.536522 -0.552486  -0.268417             0.0   \n",
      "2024-12-18 13:00:00 -0.270363 -0.271658 -1.733333  -0.815890             0.0   \n",
      "2024-12-18 14:00:00 -0.815244 -0.424889  0.687738  -0.135348             0.0   \n",
      "2024-12-18 15:00:00 -0.193824 -0.196335  0.093802   0.125663             0.0   \n",
      "2024-12-18 16:00:00  0.184984  0.129180  0.313482   0.245754             0.0   \n",
      "\n",
      "                     open_pct_Bin_1  ...  close_pct_Bin_91  close_pct_Bin_92  \\\n",
      "2017-09-25 13:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 17:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 19:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 20:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 21:00:00             0.0  ...               0.0               0.0   \n",
      "...                             ...  ...               ...               ...   \n",
      "2024-12-18 12:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 13:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 14:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 15:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 16:00:00             0.0  ...               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_93  close_pct_Bin_94  close_pct_Bin_95  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 12:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 13:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 14:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 15:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_96  close_pct_Bin_97  close_pct_Bin_98  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 12:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 13:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 14:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 15:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_99  close_target  \n",
      "2017-09-25 13:00:00               0.0     4235000.0  \n",
      "2017-09-25 17:00:00               0.0     4235000.0  \n",
      "2017-09-25 19:00:00               0.0     4204000.0  \n",
      "2017-09-25 20:00:00               0.0     4191000.0  \n",
      "2017-09-25 21:00:00               0.0     4211000.0  \n",
      "...                               ...           ...  \n",
      "2024-12-18 12:00:00               0.0   153452000.0  \n",
      "2024-12-18 13:00:00               0.0   152200000.0  \n",
      "2024-12-18 14:00:00               0.0   151994000.0  \n",
      "2024-12-18 15:00:00               0.0   152184992.0  \n",
      "2024-12-18 16:00:00               0.0   152559008.0  \n",
      "\n",
      "[63282 rows x 409 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 6 for fine-tuning.\n",
      "Experiment 7: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6939, Train Acc: 0.4966 | Val Loss: 0.6932, Val Acc: 0.5000\n",
      "Epoch 2/10 | Train Loss: 0.6939, Train Acc: 0.4959 | Val Loss: 0.6938, Val Acc: 0.5000\n",
      "Epoch 3/10 | Train Loss: 0.6938, Train Acc: 0.5005 | Val Loss: 0.6934, Val Acc: 0.5000\n",
      "Epoch 4/10 | Train Loss: 0.6939, Train Acc: 0.4971 | Val Loss: 0.6933, Val Acc: 0.5000\n",
      "Epoch 5/10 | Train Loss: 0.6934, Train Acc: 0.5049 | Val Loss: 0.6934, Val Acc: 0.5000\n",
      "Epoch 6/10 | Train Loss: 0.6934, Train Acc: 0.5001 | Val Loss: 0.6932, Val Acc: 0.5000\n",
      "Epoch 7/10 | Train Loss: 0.6938, Train Acc: 0.5010 | Val Loss: 0.6932, Val Acc: 0.5000\n",
      "Epoch 8/10 | Train Loss: 0.6935, Train Acc: 0.5003 | Val Loss: 0.6934, Val Acc: 0.5000\n",
      "Epoch 9/10 | Train Loss: 0.6935, Train Acc: 0.4959 | Val Loss: 0.6932, Val Acc: 0.5000\n",
      "Epoch 10/10 | Train Loss: 0.6934, Train Acc: 0.4947 | Val Loss: 0.6932, Val Acc: 0.5000\n",
      "Saved model for experiment 7.\n",
      "Experiment 7: Final Validation Accuracy: 0.5000\n",
      "Experiment 7: Test Accuracy: 0.5133\n",
      "Experiment 8: 데이터 구간 [20000:45000]\n",
      "                            open         high          low        close  \\\n",
      "2017-09-25 13:00:00    4222000.0    4235000.0    4208000.0    4235000.0   \n",
      "2017-09-25 17:00:00    4235000.0    4235000.0    4235000.0    4235000.0   \n",
      "2017-09-25 19:00:00    4213000.0    4213000.0    4187000.0    4204000.0   \n",
      "2017-09-25 20:00:00    4204000.0    4204000.0    4175000.0    4191000.0   \n",
      "2017-09-25 21:00:00    4191000.0    4218000.0    4183000.0    4211000.0   \n",
      "...                          ...          ...          ...          ...   \n",
      "2024-12-18 12:00:00  153867008.0  153870000.0  153000000.0  153452000.0   \n",
      "2024-12-18 13:00:00  153451008.0  153452000.0  150348000.0  152200000.0   \n",
      "2024-12-18 14:00:00  152200000.0  152800000.0  151382000.0  151994000.0   \n",
      "2024-12-18 15:00:00  151904992.0  152500000.0  151524000.0  152184992.0   \n",
      "2024-12-18 16:00:00  152186000.0  152696992.0  151999008.0  152559008.0   \n",
      "\n",
      "                     open_pct  high_pct   low_pct  close_pct  open_pct_Bin_0  \\\n",
      "2017-09-25 13:00:00  0.499881 -0.212064  0.405631   0.785340             0.0   \n",
      "2017-09-25 17:00:00  0.307911  0.000000  0.641635   0.000000             0.0   \n",
      "2017-09-25 19:00:00 -0.519481 -0.519481 -1.133412  -0.731995             0.0   \n",
      "2017-09-25 20:00:00 -0.213624 -0.213624 -0.286601  -0.309229             0.0   \n",
      "2017-09-25 21:00:00 -0.309229  0.333016  0.191617   0.477213             0.0   \n",
      "...                       ...       ...       ...        ...             ...   \n",
      "2024-12-18 12:00:00 -0.342625 -0.536522 -0.552486  -0.268417             0.0   \n",
      "2024-12-18 13:00:00 -0.270363 -0.271658 -1.733333  -0.815890             0.0   \n",
      "2024-12-18 14:00:00 -0.815244 -0.424889  0.687738  -0.135348             0.0   \n",
      "2024-12-18 15:00:00 -0.193824 -0.196335  0.093802   0.125663             0.0   \n",
      "2024-12-18 16:00:00  0.184984  0.129180  0.313482   0.245754             0.0   \n",
      "\n",
      "                     open_pct_Bin_1  ...  close_pct_Bin_91  close_pct_Bin_92  \\\n",
      "2017-09-25 13:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 17:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 19:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 20:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 21:00:00             0.0  ...               0.0               0.0   \n",
      "...                             ...  ...               ...               ...   \n",
      "2024-12-18 12:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 13:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 14:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 15:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 16:00:00             0.0  ...               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_93  close_pct_Bin_94  close_pct_Bin_95  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 12:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 13:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 14:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 15:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_96  close_pct_Bin_97  close_pct_Bin_98  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 12:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 13:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 14:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 15:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_99  close_target  \n",
      "2017-09-25 13:00:00               0.0     4235000.0  \n",
      "2017-09-25 17:00:00               0.0     4235000.0  \n",
      "2017-09-25 19:00:00               0.0     4204000.0  \n",
      "2017-09-25 20:00:00               0.0     4191000.0  \n",
      "2017-09-25 21:00:00               0.0     4211000.0  \n",
      "...                               ...           ...  \n",
      "2024-12-18 12:00:00               0.0   153452000.0  \n",
      "2024-12-18 13:00:00               0.0   152200000.0  \n",
      "2024-12-18 14:00:00               0.0   151994000.0  \n",
      "2024-12-18 15:00:00               0.0   152184992.0  \n",
      "2024-12-18 16:00:00               0.0   152559008.0  \n",
      "\n",
      "[63282 rows x 409 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 7 for fine-tuning.\n",
      "Experiment 8: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6938, Train Acc: 0.4996 | Val Loss: 0.6930, Val Acc: 0.5133\n",
      "Epoch 2/10 | Train Loss: 0.6941, Train Acc: 0.4952 | Val Loss: 0.6941, Val Acc: 0.4867\n",
      "Epoch 3/10 | Train Loss: 0.6938, Train Acc: 0.4991 | Val Loss: 0.6933, Val Acc: 0.4867\n",
      "Epoch 4/10 | Train Loss: 0.6941, Train Acc: 0.4930 | Val Loss: 0.6929, Val Acc: 0.5133\n",
      "Epoch 5/10 | Train Loss: 0.6945, Train Acc: 0.4959 | Val Loss: 0.6929, Val Acc: 0.5133\n",
      "Epoch 6/10 | Train Loss: 0.6941, Train Acc: 0.4947 | Val Loss: 0.6928, Val Acc: 0.5133\n",
      "Epoch 7/10 | Train Loss: 0.6937, Train Acc: 0.5016 | Val Loss: 0.6929, Val Acc: 0.5133\n",
      "Epoch 8/10 | Train Loss: 0.6939, Train Acc: 0.4919 | Val Loss: 0.6953, Val Acc: 0.4867\n",
      "Epoch 9/10 | Train Loss: 0.6938, Train Acc: 0.5023 | Val Loss: 0.6931, Val Acc: 0.5133\n",
      "Epoch 10/10 | Train Loss: 0.6936, Train Acc: 0.5025 | Val Loss: 0.6935, Val Acc: 0.4867\n",
      "Saved model for experiment 8.\n",
      "Experiment 8: Final Validation Accuracy: 0.4867\n",
      "Experiment 8: Test Accuracy: 0.4927\n",
      "Experiment 9: 데이터 구간 [22500:47500]\n",
      "                            open         high          low        close  \\\n",
      "2017-09-25 13:00:00    4222000.0    4235000.0    4208000.0    4235000.0   \n",
      "2017-09-25 17:00:00    4235000.0    4235000.0    4235000.0    4235000.0   \n",
      "2017-09-25 19:00:00    4213000.0    4213000.0    4187000.0    4204000.0   \n",
      "2017-09-25 20:00:00    4204000.0    4204000.0    4175000.0    4191000.0   \n",
      "2017-09-25 21:00:00    4191000.0    4218000.0    4183000.0    4211000.0   \n",
      "...                          ...          ...          ...          ...   \n",
      "2024-12-18 12:00:00  153867008.0  153870000.0  153000000.0  153452000.0   \n",
      "2024-12-18 13:00:00  153451008.0  153452000.0  150348000.0  152200000.0   \n",
      "2024-12-18 14:00:00  152200000.0  152800000.0  151382000.0  151994000.0   \n",
      "2024-12-18 15:00:00  151904992.0  152500000.0  151524000.0  152184992.0   \n",
      "2024-12-18 16:00:00  152186000.0  152696992.0  151999008.0  152559008.0   \n",
      "\n",
      "                     open_pct  high_pct   low_pct  close_pct  open_pct_Bin_0  \\\n",
      "2017-09-25 13:00:00  0.499881 -0.212064  0.405631   0.785340             0.0   \n",
      "2017-09-25 17:00:00  0.307911  0.000000  0.641635   0.000000             0.0   \n",
      "2017-09-25 19:00:00 -0.519481 -0.519481 -1.133412  -0.731995             0.0   \n",
      "2017-09-25 20:00:00 -0.213624 -0.213624 -0.286601  -0.309229             0.0   \n",
      "2017-09-25 21:00:00 -0.309229  0.333016  0.191617   0.477213             0.0   \n",
      "...                       ...       ...       ...        ...             ...   \n",
      "2024-12-18 12:00:00 -0.342625 -0.536522 -0.552486  -0.268417             0.0   \n",
      "2024-12-18 13:00:00 -0.270363 -0.271658 -1.733333  -0.815890             0.0   \n",
      "2024-12-18 14:00:00 -0.815244 -0.424889  0.687738  -0.135348             0.0   \n",
      "2024-12-18 15:00:00 -0.193824 -0.196335  0.093802   0.125663             0.0   \n",
      "2024-12-18 16:00:00  0.184984  0.129180  0.313482   0.245754             0.0   \n",
      "\n",
      "                     open_pct_Bin_1  ...  close_pct_Bin_91  close_pct_Bin_92  \\\n",
      "2017-09-25 13:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 17:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 19:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 20:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 21:00:00             0.0  ...               0.0               0.0   \n",
      "...                             ...  ...               ...               ...   \n",
      "2024-12-18 12:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 13:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 14:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 15:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 16:00:00             0.0  ...               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_93  close_pct_Bin_94  close_pct_Bin_95  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 12:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 13:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 14:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 15:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_96  close_pct_Bin_97  close_pct_Bin_98  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 12:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 13:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 14:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 15:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_99  close_target  \n",
      "2017-09-25 13:00:00               0.0     4235000.0  \n",
      "2017-09-25 17:00:00               0.0     4235000.0  \n",
      "2017-09-25 19:00:00               0.0     4204000.0  \n",
      "2017-09-25 20:00:00               0.0     4191000.0  \n",
      "2017-09-25 21:00:00               0.0     4211000.0  \n",
      "...                               ...           ...  \n",
      "2024-12-18 12:00:00               0.0   153452000.0  \n",
      "2024-12-18 13:00:00               0.0   152200000.0  \n",
      "2024-12-18 14:00:00               0.0   151994000.0  \n",
      "2024-12-18 15:00:00               0.0   152184992.0  \n",
      "2024-12-18 16:00:00               0.0   152559008.0  \n",
      "\n",
      "[63282 rows x 409 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 8 for fine-tuning.\n",
      "Experiment 9: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6938, Train Acc: 0.5065 | Val Loss: 0.6971, Val Acc: 0.4927\n",
      "Epoch 2/10 | Train Loss: 0.6940, Train Acc: 0.4989 | Val Loss: 0.6943, Val Acc: 0.5073\n",
      "Epoch 3/10 | Train Loss: 0.6941, Train Acc: 0.5003 | Val Loss: 0.6965, Val Acc: 0.4927\n",
      "Epoch 4/10 | Train Loss: 0.6940, Train Acc: 0.5004 | Val Loss: 0.6942, Val Acc: 0.5073\n",
      "Epoch 5/10 | Train Loss: 0.6937, Train Acc: 0.5041 | Val Loss: 0.6932, Val Acc: 0.5073\n",
      "Epoch 6/10 | Train Loss: 0.6940, Train Acc: 0.4957 | Val Loss: 0.6933, Val Acc: 0.4927\n",
      "Epoch 7/10 | Train Loss: 0.6937, Train Acc: 0.5036 | Val Loss: 0.6955, Val Acc: 0.4927\n",
      "Epoch 8/10 | Train Loss: 0.6940, Train Acc: 0.5009 | Val Loss: 0.6934, Val Acc: 0.4927\n",
      "Epoch 9/10 | Train Loss: 0.6934, Train Acc: 0.5059 | Val Loss: 0.6932, Val Acc: 0.5073\n",
      "Epoch 10/10 | Train Loss: 0.6937, Train Acc: 0.5023 | Val Loss: 0.6931, Val Acc: 0.5073\n",
      "Saved model for experiment 9.\n",
      "Experiment 9: Final Validation Accuracy: 0.5073\n",
      "Experiment 9: Test Accuracy: 0.4947\n",
      "Experiment 10: 데이터 구간 [25000:50000]\n",
      "                            open         high          low        close  \\\n",
      "2017-09-25 13:00:00    4222000.0    4235000.0    4208000.0    4235000.0   \n",
      "2017-09-25 17:00:00    4235000.0    4235000.0    4235000.0    4235000.0   \n",
      "2017-09-25 19:00:00    4213000.0    4213000.0    4187000.0    4204000.0   \n",
      "2017-09-25 20:00:00    4204000.0    4204000.0    4175000.0    4191000.0   \n",
      "2017-09-25 21:00:00    4191000.0    4218000.0    4183000.0    4211000.0   \n",
      "...                          ...          ...          ...          ...   \n",
      "2024-12-18 12:00:00  153867008.0  153870000.0  153000000.0  153452000.0   \n",
      "2024-12-18 13:00:00  153451008.0  153452000.0  150348000.0  152200000.0   \n",
      "2024-12-18 14:00:00  152200000.0  152800000.0  151382000.0  151994000.0   \n",
      "2024-12-18 15:00:00  151904992.0  152500000.0  151524000.0  152184992.0   \n",
      "2024-12-18 16:00:00  152186000.0  152696992.0  151999008.0  152559008.0   \n",
      "\n",
      "                     open_pct  high_pct   low_pct  close_pct  open_pct_Bin_0  \\\n",
      "2017-09-25 13:00:00  0.499881 -0.212064  0.405631   0.785340             0.0   \n",
      "2017-09-25 17:00:00  0.307911  0.000000  0.641635   0.000000             0.0   \n",
      "2017-09-25 19:00:00 -0.519481 -0.519481 -1.133412  -0.731995             0.0   \n",
      "2017-09-25 20:00:00 -0.213624 -0.213624 -0.286601  -0.309229             0.0   \n",
      "2017-09-25 21:00:00 -0.309229  0.333016  0.191617   0.477213             0.0   \n",
      "...                       ...       ...       ...        ...             ...   \n",
      "2024-12-18 12:00:00 -0.342625 -0.536522 -0.552486  -0.268417             0.0   \n",
      "2024-12-18 13:00:00 -0.270363 -0.271658 -1.733333  -0.815890             0.0   \n",
      "2024-12-18 14:00:00 -0.815244 -0.424889  0.687738  -0.135348             0.0   \n",
      "2024-12-18 15:00:00 -0.193824 -0.196335  0.093802   0.125663             0.0   \n",
      "2024-12-18 16:00:00  0.184984  0.129180  0.313482   0.245754             0.0   \n",
      "\n",
      "                     open_pct_Bin_1  ...  close_pct_Bin_91  close_pct_Bin_92  \\\n",
      "2017-09-25 13:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 17:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 19:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 20:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 21:00:00             0.0  ...               0.0               0.0   \n",
      "...                             ...  ...               ...               ...   \n",
      "2024-12-18 12:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 13:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 14:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 15:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 16:00:00             0.0  ...               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_93  close_pct_Bin_94  close_pct_Bin_95  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 12:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 13:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 14:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 15:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_96  close_pct_Bin_97  close_pct_Bin_98  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 12:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 13:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 14:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 15:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_99  close_target  \n",
      "2017-09-25 13:00:00               0.0     4235000.0  \n",
      "2017-09-25 17:00:00               0.0     4235000.0  \n",
      "2017-09-25 19:00:00               0.0     4204000.0  \n",
      "2017-09-25 20:00:00               0.0     4191000.0  \n",
      "2017-09-25 21:00:00               0.0     4211000.0  \n",
      "...                               ...           ...  \n",
      "2024-12-18 12:00:00               0.0   153452000.0  \n",
      "2024-12-18 13:00:00               0.0   152200000.0  \n",
      "2024-12-18 14:00:00               0.0   151994000.0  \n",
      "2024-12-18 15:00:00               0.0   152184992.0  \n",
      "2024-12-18 16:00:00               0.0   152559008.0  \n",
      "\n",
      "[63282 rows x 409 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 9 for fine-tuning.\n",
      "Experiment 10: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6939, Train Acc: 0.4958 | Val Loss: 0.6936, Val Acc: 0.4947\n",
      "Epoch 2/10 | Train Loss: 0.6939, Train Acc: 0.5039 | Val Loss: 0.6932, Val Acc: 0.4947\n",
      "Epoch 3/10 | Train Loss: 0.6941, Train Acc: 0.5002 | Val Loss: 0.6948, Val Acc: 0.4947\n",
      "Epoch 4/10 | Train Loss: 0.6939, Train Acc: 0.4965 | Val Loss: 0.6960, Val Acc: 0.5053\n",
      "Epoch 5/10 | Train Loss: 0.6940, Train Acc: 0.4978 | Val Loss: 0.6935, Val Acc: 0.5053\n",
      "Epoch 6/10 | Train Loss: 0.6932, Train Acc: 0.5085 | Val Loss: 0.6931, Val Acc: 0.5053\n",
      "Epoch 7/10 | Train Loss: 0.6936, Train Acc: 0.5018 | Val Loss: 0.6933, Val Acc: 0.5053\n",
      "Epoch 8/10 | Train Loss: 0.6937, Train Acc: 0.4954 | Val Loss: 0.6931, Val Acc: 0.5053\n",
      "Epoch 9/10 | Train Loss: 0.6935, Train Acc: 0.5021 | Val Loss: 0.6944, Val Acc: 0.4947\n",
      "Epoch 10/10 | Train Loss: 0.6933, Train Acc: 0.5007 | Val Loss: 0.6937, Val Acc: 0.4947\n",
      "Saved model for experiment 10.\n",
      "Experiment 10: Final Validation Accuracy: 0.4947\n",
      "Experiment 10: Test Accuracy: 0.5137\n",
      "Experiment 11: 데이터 구간 [27500:52500]\n",
      "                            open         high          low        close  \\\n",
      "2017-09-25 13:00:00    4222000.0    4235000.0    4208000.0    4235000.0   \n",
      "2017-09-25 17:00:00    4235000.0    4235000.0    4235000.0    4235000.0   \n",
      "2017-09-25 19:00:00    4213000.0    4213000.0    4187000.0    4204000.0   \n",
      "2017-09-25 20:00:00    4204000.0    4204000.0    4175000.0    4191000.0   \n",
      "2017-09-25 21:00:00    4191000.0    4218000.0    4183000.0    4211000.0   \n",
      "...                          ...          ...          ...          ...   \n",
      "2024-12-18 12:00:00  153867008.0  153870000.0  153000000.0  153452000.0   \n",
      "2024-12-18 13:00:00  153451008.0  153452000.0  150348000.0  152200000.0   \n",
      "2024-12-18 14:00:00  152200000.0  152800000.0  151382000.0  151994000.0   \n",
      "2024-12-18 15:00:00  151904992.0  152500000.0  151524000.0  152184992.0   \n",
      "2024-12-18 16:00:00  152186000.0  152696992.0  151999008.0  152559008.0   \n",
      "\n",
      "                     open_pct  high_pct   low_pct  close_pct  open_pct_Bin_0  \\\n",
      "2017-09-25 13:00:00  0.499881 -0.212064  0.405631   0.785340             0.0   \n",
      "2017-09-25 17:00:00  0.307911  0.000000  0.641635   0.000000             0.0   \n",
      "2017-09-25 19:00:00 -0.519481 -0.519481 -1.133412  -0.731995             0.0   \n",
      "2017-09-25 20:00:00 -0.213624 -0.213624 -0.286601  -0.309229             0.0   \n",
      "2017-09-25 21:00:00 -0.309229  0.333016  0.191617   0.477213             0.0   \n",
      "...                       ...       ...       ...        ...             ...   \n",
      "2024-12-18 12:00:00 -0.342625 -0.536522 -0.552486  -0.268417             0.0   \n",
      "2024-12-18 13:00:00 -0.270363 -0.271658 -1.733333  -0.815890             0.0   \n",
      "2024-12-18 14:00:00 -0.815244 -0.424889  0.687738  -0.135348             0.0   \n",
      "2024-12-18 15:00:00 -0.193824 -0.196335  0.093802   0.125663             0.0   \n",
      "2024-12-18 16:00:00  0.184984  0.129180  0.313482   0.245754             0.0   \n",
      "\n",
      "                     open_pct_Bin_1  ...  close_pct_Bin_91  close_pct_Bin_92  \\\n",
      "2017-09-25 13:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 17:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 19:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 20:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 21:00:00             0.0  ...               0.0               0.0   \n",
      "...                             ...  ...               ...               ...   \n",
      "2024-12-18 12:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 13:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 14:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 15:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 16:00:00             0.0  ...               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_93  close_pct_Bin_94  close_pct_Bin_95  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 12:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 13:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 14:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 15:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_96  close_pct_Bin_97  close_pct_Bin_98  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 12:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 13:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 14:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 15:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_99  close_target  \n",
      "2017-09-25 13:00:00               0.0     4235000.0  \n",
      "2017-09-25 17:00:00               0.0     4235000.0  \n",
      "2017-09-25 19:00:00               0.0     4204000.0  \n",
      "2017-09-25 20:00:00               0.0     4191000.0  \n",
      "2017-09-25 21:00:00               0.0     4211000.0  \n",
      "...                               ...           ...  \n",
      "2024-12-18 12:00:00               0.0   153452000.0  \n",
      "2024-12-18 13:00:00               0.0   152200000.0  \n",
      "2024-12-18 14:00:00               0.0   151994000.0  \n",
      "2024-12-18 15:00:00               0.0   152184992.0  \n",
      "2024-12-18 16:00:00               0.0   152559008.0  \n",
      "\n",
      "[63282 rows x 409 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 10 for fine-tuning.\n",
      "Experiment 11: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6937, Train Acc: 0.4969 | Val Loss: 0.6934, Val Acc: 0.4863\n",
      "Epoch 2/10 | Train Loss: 0.6939, Train Acc: 0.4988 | Val Loss: 0.6928, Val Acc: 0.5137\n",
      "Epoch 3/10 | Train Loss: 0.6940, Train Acc: 0.5039 | Val Loss: 0.6995, Val Acc: 0.4863\n",
      "Epoch 4/10 | Train Loss: 0.6939, Train Acc: 0.4933 | Val Loss: 0.6928, Val Acc: 0.5137\n",
      "Epoch 5/10 | Train Loss: 0.6938, Train Acc: 0.4995 | Val Loss: 0.6928, Val Acc: 0.5137\n",
      "Epoch 6/10 | Train Loss: 0.6936, Train Acc: 0.4991 | Val Loss: 0.6928, Val Acc: 0.5137\n",
      "Epoch 7/10 | Train Loss: 0.6934, Train Acc: 0.4992 | Val Loss: 0.6941, Val Acc: 0.4863\n",
      "Epoch 8/10 | Train Loss: 0.6933, Train Acc: 0.4996 | Val Loss: 0.6928, Val Acc: 0.5137\n",
      "Epoch 9/10 | Train Loss: 0.6933, Train Acc: 0.4996 | Val Loss: 0.6934, Val Acc: 0.4863\n",
      "Epoch 10/10 | Train Loss: 0.6935, Train Acc: 0.5009 | Val Loss: 0.6929, Val Acc: 0.5137\n",
      "Saved model for experiment 11.\n",
      "Experiment 11: Final Validation Accuracy: 0.5137\n",
      "Experiment 11: Test Accuracy: 0.5040\n",
      "Experiment 12: 데이터 구간 [30000:55000]\n",
      "                            open         high          low        close  \\\n",
      "2017-09-25 13:00:00    4222000.0    4235000.0    4208000.0    4235000.0   \n",
      "2017-09-25 17:00:00    4235000.0    4235000.0    4235000.0    4235000.0   \n",
      "2017-09-25 19:00:00    4213000.0    4213000.0    4187000.0    4204000.0   \n",
      "2017-09-25 20:00:00    4204000.0    4204000.0    4175000.0    4191000.0   \n",
      "2017-09-25 21:00:00    4191000.0    4218000.0    4183000.0    4211000.0   \n",
      "...                          ...          ...          ...          ...   \n",
      "2024-12-18 12:00:00  153867008.0  153870000.0  153000000.0  153452000.0   \n",
      "2024-12-18 13:00:00  153451008.0  153452000.0  150348000.0  152200000.0   \n",
      "2024-12-18 14:00:00  152200000.0  152800000.0  151382000.0  151994000.0   \n",
      "2024-12-18 15:00:00  151904992.0  152500000.0  151524000.0  152184992.0   \n",
      "2024-12-18 16:00:00  152186000.0  152696992.0  151999008.0  152559008.0   \n",
      "\n",
      "                     open_pct  high_pct   low_pct  close_pct  open_pct_Bin_0  \\\n",
      "2017-09-25 13:00:00  0.499881 -0.212064  0.405631   0.785340             0.0   \n",
      "2017-09-25 17:00:00  0.307911  0.000000  0.641635   0.000000             0.0   \n",
      "2017-09-25 19:00:00 -0.519481 -0.519481 -1.133412  -0.731995             0.0   \n",
      "2017-09-25 20:00:00 -0.213624 -0.213624 -0.286601  -0.309229             0.0   \n",
      "2017-09-25 21:00:00 -0.309229  0.333016  0.191617   0.477213             0.0   \n",
      "...                       ...       ...       ...        ...             ...   \n",
      "2024-12-18 12:00:00 -0.342625 -0.536522 -0.552486  -0.268417             0.0   \n",
      "2024-12-18 13:00:00 -0.270363 -0.271658 -1.733333  -0.815890             0.0   \n",
      "2024-12-18 14:00:00 -0.815244 -0.424889  0.687738  -0.135348             0.0   \n",
      "2024-12-18 15:00:00 -0.193824 -0.196335  0.093802   0.125663             0.0   \n",
      "2024-12-18 16:00:00  0.184984  0.129180  0.313482   0.245754             0.0   \n",
      "\n",
      "                     open_pct_Bin_1  ...  close_pct_Bin_91  close_pct_Bin_92  \\\n",
      "2017-09-25 13:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 17:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 19:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 20:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 21:00:00             0.0  ...               0.0               0.0   \n",
      "...                             ...  ...               ...               ...   \n",
      "2024-12-18 12:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 13:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 14:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 15:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 16:00:00             0.0  ...               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_93  close_pct_Bin_94  close_pct_Bin_95  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 12:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 13:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 14:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 15:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_96  close_pct_Bin_97  close_pct_Bin_98  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 12:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 13:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 14:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 15:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_99  close_target  \n",
      "2017-09-25 13:00:00               0.0     4235000.0  \n",
      "2017-09-25 17:00:00               0.0     4235000.0  \n",
      "2017-09-25 19:00:00               0.0     4204000.0  \n",
      "2017-09-25 20:00:00               0.0     4191000.0  \n",
      "2017-09-25 21:00:00               0.0     4211000.0  \n",
      "...                               ...           ...  \n",
      "2024-12-18 12:00:00               0.0   153452000.0  \n",
      "2024-12-18 13:00:00               0.0   152200000.0  \n",
      "2024-12-18 14:00:00               0.0   151994000.0  \n",
      "2024-12-18 15:00:00               0.0   152184992.0  \n",
      "2024-12-18 16:00:00               0.0   152559008.0  \n",
      "\n",
      "[63282 rows x 409 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 11 for fine-tuning.\n",
      "Experiment 12: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6939, Train Acc: 0.4988 | Val Loss: 0.6934, Val Acc: 0.5040\n",
      "Epoch 2/10 | Train Loss: 0.6939, Train Acc: 0.4987 | Val Loss: 0.6935, Val Acc: 0.4960\n",
      "Epoch 3/10 | Train Loss: 0.6940, Train Acc: 0.4979 | Val Loss: 0.6932, Val Acc: 0.5040\n",
      "Epoch 4/10 | Train Loss: 0.6935, Train Acc: 0.5029 | Val Loss: 0.6932, Val Acc: 0.4960\n",
      "Epoch 5/10 | Train Loss: 0.6937, Train Acc: 0.4953 | Val Loss: 0.6934, Val Acc: 0.5040\n",
      "Epoch 6/10 | Train Loss: 0.6937, Train Acc: 0.4985 | Val Loss: 0.6932, Val Acc: 0.4960\n",
      "Epoch 7/10 | Train Loss: 0.6934, Train Acc: 0.5005 | Val Loss: 0.6933, Val Acc: 0.5040\n",
      "Epoch 8/10 | Train Loss: 0.6936, Train Acc: 0.5033 | Val Loss: 0.6933, Val Acc: 0.4960\n",
      "Epoch 9/10 | Train Loss: 0.6935, Train Acc: 0.4992 | Val Loss: 0.6931, Val Acc: 0.5040\n",
      "Epoch 10/10 | Train Loss: 0.6935, Train Acc: 0.4975 | Val Loss: 0.6931, Val Acc: 0.5040\n",
      "Saved model for experiment 12.\n",
      "Experiment 12: Final Validation Accuracy: 0.5040\n",
      "Experiment 12: Test Accuracy: 0.4818\n",
      "Experiment 13: 데이터 구간 [32500:57500]\n",
      "                            open         high          low        close  \\\n",
      "2017-09-25 13:00:00    4222000.0    4235000.0    4208000.0    4235000.0   \n",
      "2017-09-25 17:00:00    4235000.0    4235000.0    4235000.0    4235000.0   \n",
      "2017-09-25 19:00:00    4213000.0    4213000.0    4187000.0    4204000.0   \n",
      "2017-09-25 20:00:00    4204000.0    4204000.0    4175000.0    4191000.0   \n",
      "2017-09-25 21:00:00    4191000.0    4218000.0    4183000.0    4211000.0   \n",
      "...                          ...          ...          ...          ...   \n",
      "2024-12-18 12:00:00  153867008.0  153870000.0  153000000.0  153452000.0   \n",
      "2024-12-18 13:00:00  153451008.0  153452000.0  150348000.0  152200000.0   \n",
      "2024-12-18 14:00:00  152200000.0  152800000.0  151382000.0  151994000.0   \n",
      "2024-12-18 15:00:00  151904992.0  152500000.0  151524000.0  152184992.0   \n",
      "2024-12-18 16:00:00  152186000.0  152696992.0  151999008.0  152559008.0   \n",
      "\n",
      "                     open_pct  high_pct   low_pct  close_pct  open_pct_Bin_0  \\\n",
      "2017-09-25 13:00:00  0.499881 -0.212064  0.405631   0.785340             0.0   \n",
      "2017-09-25 17:00:00  0.307911  0.000000  0.641635   0.000000             0.0   \n",
      "2017-09-25 19:00:00 -0.519481 -0.519481 -1.133412  -0.731995             0.0   \n",
      "2017-09-25 20:00:00 -0.213624 -0.213624 -0.286601  -0.309229             0.0   \n",
      "2017-09-25 21:00:00 -0.309229  0.333016  0.191617   0.477213             0.0   \n",
      "...                       ...       ...       ...        ...             ...   \n",
      "2024-12-18 12:00:00 -0.342625 -0.536522 -0.552486  -0.268417             0.0   \n",
      "2024-12-18 13:00:00 -0.270363 -0.271658 -1.733333  -0.815890             0.0   \n",
      "2024-12-18 14:00:00 -0.815244 -0.424889  0.687738  -0.135348             0.0   \n",
      "2024-12-18 15:00:00 -0.193824 -0.196335  0.093802   0.125663             0.0   \n",
      "2024-12-18 16:00:00  0.184984  0.129180  0.313482   0.245754             0.0   \n",
      "\n",
      "                     open_pct_Bin_1  ...  close_pct_Bin_91  close_pct_Bin_92  \\\n",
      "2017-09-25 13:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 17:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 19:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 20:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 21:00:00             0.0  ...               0.0               0.0   \n",
      "...                             ...  ...               ...               ...   \n",
      "2024-12-18 12:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 13:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 14:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 15:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 16:00:00             0.0  ...               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_93  close_pct_Bin_94  close_pct_Bin_95  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 12:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 13:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 14:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 15:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_96  close_pct_Bin_97  close_pct_Bin_98  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 12:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 13:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 14:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 15:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_99  close_target  \n",
      "2017-09-25 13:00:00               0.0     4235000.0  \n",
      "2017-09-25 17:00:00               0.0     4235000.0  \n",
      "2017-09-25 19:00:00               0.0     4204000.0  \n",
      "2017-09-25 20:00:00               0.0     4191000.0  \n",
      "2017-09-25 21:00:00               0.0     4211000.0  \n",
      "...                               ...           ...  \n",
      "2024-12-18 12:00:00               0.0   153452000.0  \n",
      "2024-12-18 13:00:00               0.0   152200000.0  \n",
      "2024-12-18 14:00:00               0.0   151994000.0  \n",
      "2024-12-18 15:00:00               0.0   152184992.0  \n",
      "2024-12-18 16:00:00               0.0   152559008.0  \n",
      "\n",
      "[63282 rows x 409 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 12 for fine-tuning.\n",
      "Experiment 13: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6939, Train Acc: 0.5002 | Val Loss: 0.6940, Val Acc: 0.4818\n",
      "Epoch 2/10 | Train Loss: 0.6941, Train Acc: 0.5003 | Val Loss: 0.6934, Val Acc: 0.4818\n",
      "Epoch 3/10 | Train Loss: 0.6938, Train Acc: 0.4999 | Val Loss: 0.6928, Val Acc: 0.5182\n",
      "Epoch 4/10 | Train Loss: 0.6934, Train Acc: 0.4985 | Val Loss: 0.6933, Val Acc: 0.4818\n",
      "Epoch 5/10 | Train Loss: 0.6937, Train Acc: 0.5043 | Val Loss: 0.6939, Val Acc: 0.4818\n",
      "Epoch 6/10 | Train Loss: 0.6937, Train Acc: 0.4995 | Val Loss: 0.6947, Val Acc: 0.4818\n",
      "Epoch 7/10 | Train Loss: 0.6935, Train Acc: 0.5006 | Val Loss: 0.6934, Val Acc: 0.4818\n",
      "Epoch 8/10 | Train Loss: 0.6936, Train Acc: 0.5007 | Val Loss: 0.6933, Val Acc: 0.4818\n",
      "Epoch 9/10 | Train Loss: 0.6936, Train Acc: 0.4972 | Val Loss: 0.6942, Val Acc: 0.4818\n",
      "Epoch 10/10 | Train Loss: 0.6932, Train Acc: 0.5018 | Val Loss: 0.6941, Val Acc: 0.4818\n",
      "Saved model for experiment 13.\n",
      "Experiment 13: Final Validation Accuracy: 0.4818\n",
      "Experiment 13: Test Accuracy: 0.4907\n",
      "Experiment 14: 데이터 구간 [35000:60000]\n",
      "                            open         high          low        close  \\\n",
      "2017-09-25 13:00:00    4222000.0    4235000.0    4208000.0    4235000.0   \n",
      "2017-09-25 17:00:00    4235000.0    4235000.0    4235000.0    4235000.0   \n",
      "2017-09-25 19:00:00    4213000.0    4213000.0    4187000.0    4204000.0   \n",
      "2017-09-25 20:00:00    4204000.0    4204000.0    4175000.0    4191000.0   \n",
      "2017-09-25 21:00:00    4191000.0    4218000.0    4183000.0    4211000.0   \n",
      "...                          ...          ...          ...          ...   \n",
      "2024-12-18 12:00:00  153867008.0  153870000.0  153000000.0  153452000.0   \n",
      "2024-12-18 13:00:00  153451008.0  153452000.0  150348000.0  152200000.0   \n",
      "2024-12-18 14:00:00  152200000.0  152800000.0  151382000.0  151994000.0   \n",
      "2024-12-18 15:00:00  151904992.0  152500000.0  151524000.0  152184992.0   \n",
      "2024-12-18 16:00:00  152186000.0  152696992.0  151999008.0  152559008.0   \n",
      "\n",
      "                     open_pct  high_pct   low_pct  close_pct  open_pct_Bin_0  \\\n",
      "2017-09-25 13:00:00  0.499881 -0.212064  0.405631   0.785340             0.0   \n",
      "2017-09-25 17:00:00  0.307911  0.000000  0.641635   0.000000             0.0   \n",
      "2017-09-25 19:00:00 -0.519481 -0.519481 -1.133412  -0.731995             0.0   \n",
      "2017-09-25 20:00:00 -0.213624 -0.213624 -0.286601  -0.309229             0.0   \n",
      "2017-09-25 21:00:00 -0.309229  0.333016  0.191617   0.477213             0.0   \n",
      "...                       ...       ...       ...        ...             ...   \n",
      "2024-12-18 12:00:00 -0.342625 -0.536522 -0.552486  -0.268417             0.0   \n",
      "2024-12-18 13:00:00 -0.270363 -0.271658 -1.733333  -0.815890             0.0   \n",
      "2024-12-18 14:00:00 -0.815244 -0.424889  0.687738  -0.135348             0.0   \n",
      "2024-12-18 15:00:00 -0.193824 -0.196335  0.093802   0.125663             0.0   \n",
      "2024-12-18 16:00:00  0.184984  0.129180  0.313482   0.245754             0.0   \n",
      "\n",
      "                     open_pct_Bin_1  ...  close_pct_Bin_91  close_pct_Bin_92  \\\n",
      "2017-09-25 13:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 17:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 19:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 20:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 21:00:00             0.0  ...               0.0               0.0   \n",
      "...                             ...  ...               ...               ...   \n",
      "2024-12-18 12:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 13:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 14:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 15:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 16:00:00             0.0  ...               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_93  close_pct_Bin_94  close_pct_Bin_95  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 12:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 13:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 14:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 15:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_96  close_pct_Bin_97  close_pct_Bin_98  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 12:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 13:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 14:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 15:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_99  close_target  \n",
      "2017-09-25 13:00:00               0.0     4235000.0  \n",
      "2017-09-25 17:00:00               0.0     4235000.0  \n",
      "2017-09-25 19:00:00               0.0     4204000.0  \n",
      "2017-09-25 20:00:00               0.0     4191000.0  \n",
      "2017-09-25 21:00:00               0.0     4211000.0  \n",
      "...                               ...           ...  \n",
      "2024-12-18 12:00:00               0.0   153452000.0  \n",
      "2024-12-18 13:00:00               0.0   152200000.0  \n",
      "2024-12-18 14:00:00               0.0   151994000.0  \n",
      "2024-12-18 15:00:00               0.0   152184992.0  \n",
      "2024-12-18 16:00:00               0.0   152559008.0  \n",
      "\n",
      "[63282 rows x 409 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 13 for fine-tuning.\n",
      "Experiment 14: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6939, Train Acc: 0.4946 | Val Loss: 0.6948, Val Acc: 0.4907\n",
      "Epoch 2/10 | Train Loss: 0.6939, Train Acc: 0.4918 | Val Loss: 0.6941, Val Acc: 0.4907\n",
      "Epoch 3/10 | Train Loss: 0.6938, Train Acc: 0.4978 | Val Loss: 0.6942, Val Acc: 0.4907\n",
      "Epoch 4/10 | Train Loss: 0.6938, Train Acc: 0.5019 | Val Loss: 0.6931, Val Acc: 0.5093\n",
      "Epoch 5/10 | Train Loss: 0.6935, Train Acc: 0.5005 | Val Loss: 0.6941, Val Acc: 0.4907\n",
      "Epoch 6/10 | Train Loss: 0.6938, Train Acc: 0.5013 | Val Loss: 0.6937, Val Acc: 0.5093\n",
      "Epoch 7/10 | Train Loss: 0.6934, Train Acc: 0.5057 | Val Loss: 0.6930, Val Acc: 0.5093\n",
      "Epoch 8/10 | Train Loss: 0.6938, Train Acc: 0.5032 | Val Loss: 0.6951, Val Acc: 0.4907\n",
      "Epoch 9/10 | Train Loss: 0.6938, Train Acc: 0.4986 | Val Loss: 0.6930, Val Acc: 0.5093\n",
      "Epoch 10/10 | Train Loss: 0.6939, Train Acc: 0.4964 | Val Loss: 0.6937, Val Acc: 0.4907\n",
      "Saved model for experiment 14.\n",
      "Experiment 14: Final Validation Accuracy: 0.4907\n",
      "Experiment 14: Test Accuracy: 0.5000\n",
      "Experiment 15: 데이터 구간 [37500:62500]\n",
      "                            open         high          low        close  \\\n",
      "2017-09-25 13:00:00    4222000.0    4235000.0    4208000.0    4235000.0   \n",
      "2017-09-25 17:00:00    4235000.0    4235000.0    4235000.0    4235000.0   \n",
      "2017-09-25 19:00:00    4213000.0    4213000.0    4187000.0    4204000.0   \n",
      "2017-09-25 20:00:00    4204000.0    4204000.0    4175000.0    4191000.0   \n",
      "2017-09-25 21:00:00    4191000.0    4218000.0    4183000.0    4211000.0   \n",
      "...                          ...          ...          ...          ...   \n",
      "2024-12-18 12:00:00  153867008.0  153870000.0  153000000.0  153452000.0   \n",
      "2024-12-18 13:00:00  153451008.0  153452000.0  150348000.0  152200000.0   \n",
      "2024-12-18 14:00:00  152200000.0  152800000.0  151382000.0  151994000.0   \n",
      "2024-12-18 15:00:00  151904992.0  152500000.0  151524000.0  152184992.0   \n",
      "2024-12-18 16:00:00  152186000.0  152696992.0  151999008.0  152559008.0   \n",
      "\n",
      "                     open_pct  high_pct   low_pct  close_pct  open_pct_Bin_0  \\\n",
      "2017-09-25 13:00:00  0.499881 -0.212064  0.405631   0.785340             0.0   \n",
      "2017-09-25 17:00:00  0.307911  0.000000  0.641635   0.000000             0.0   \n",
      "2017-09-25 19:00:00 -0.519481 -0.519481 -1.133412  -0.731995             0.0   \n",
      "2017-09-25 20:00:00 -0.213624 -0.213624 -0.286601  -0.309229             0.0   \n",
      "2017-09-25 21:00:00 -0.309229  0.333016  0.191617   0.477213             0.0   \n",
      "...                       ...       ...       ...        ...             ...   \n",
      "2024-12-18 12:00:00 -0.342625 -0.536522 -0.552486  -0.268417             0.0   \n",
      "2024-12-18 13:00:00 -0.270363 -0.271658 -1.733333  -0.815890             0.0   \n",
      "2024-12-18 14:00:00 -0.815244 -0.424889  0.687738  -0.135348             0.0   \n",
      "2024-12-18 15:00:00 -0.193824 -0.196335  0.093802   0.125663             0.0   \n",
      "2024-12-18 16:00:00  0.184984  0.129180  0.313482   0.245754             0.0   \n",
      "\n",
      "                     open_pct_Bin_1  ...  close_pct_Bin_91  close_pct_Bin_92  \\\n",
      "2017-09-25 13:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 17:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 19:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 20:00:00             0.0  ...               0.0               0.0   \n",
      "2017-09-25 21:00:00             0.0  ...               0.0               0.0   \n",
      "...                             ...  ...               ...               ...   \n",
      "2024-12-18 12:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 13:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 14:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 15:00:00             0.0  ...               0.0               0.0   \n",
      "2024-12-18 16:00:00             0.0  ...               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_93  close_pct_Bin_94  close_pct_Bin_95  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 12:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 13:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 14:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 15:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_96  close_pct_Bin_97  close_pct_Bin_98  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 12:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 13:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 14:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 15:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_99  close_target  \n",
      "2017-09-25 13:00:00               0.0     4235000.0  \n",
      "2017-09-25 17:00:00               0.0     4235000.0  \n",
      "2017-09-25 19:00:00               0.0     4204000.0  \n",
      "2017-09-25 20:00:00               0.0     4191000.0  \n",
      "2017-09-25 21:00:00               0.0     4211000.0  \n",
      "...                               ...           ...  \n",
      "2024-12-18 12:00:00               0.0   153452000.0  \n",
      "2024-12-18 13:00:00               0.0   152200000.0  \n",
      "2024-12-18 14:00:00               0.0   151994000.0  \n",
      "2024-12-18 15:00:00               0.0   152184992.0  \n",
      "2024-12-18 16:00:00               0.0   152559008.0  \n",
      "\n",
      "[63282 rows x 409 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 14 for fine-tuning.\n",
      "Experiment 15: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6938, Train Acc: 0.5009 | Val Loss: 0.6932, Val Acc: 0.5000\n",
      "Epoch 2/10 | Train Loss: 0.6936, Train Acc: 0.4987 | Val Loss: 0.6945, Val Acc: 0.5000\n",
      "Epoch 3/10 | Train Loss: 0.6937, Train Acc: 0.4997 | Val Loss: 0.6931, Val Acc: 0.5000\n",
      "Epoch 4/10 | Train Loss: 0.6937, Train Acc: 0.4992 | Val Loss: 0.6936, Val Acc: 0.5000\n",
      "Epoch 5/10 | Train Loss: 0.6935, Train Acc: 0.5010 | Val Loss: 0.6933, Val Acc: 0.5000\n",
      "Epoch 6/10 | Train Loss: 0.6934, Train Acc: 0.5052 | Val Loss: 0.6937, Val Acc: 0.5000\n",
      "Epoch 7/10 | Train Loss: 0.6937, Train Acc: 0.4945 | Val Loss: 0.6933, Val Acc: 0.5000\n",
      "Epoch 8/10 | Train Loss: 0.6933, Train Acc: 0.5017 | Val Loss: 0.6932, Val Acc: 0.5000\n",
      "Epoch 9/10 | Train Loss: 0.6935, Train Acc: 0.4956 | Val Loss: 0.6932, Val Acc: 0.5000\n",
      "Epoch 10/10 | Train Loss: 0.6933, Train Acc: 0.5033 | Val Loss: 0.6932, Val Acc: 0.5000\n",
      "Saved model for experiment 15.\n",
      "Experiment 15: Final Validation Accuracy: 0.5000\n",
      "Experiment 15: Test Accuracy: 0.4847\n",
      "\n",
      "Final Average Validation Accuracy: 0.5005\n",
      "Final Average Test Accuracy: 0.4949\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from math import sqrt\n",
    "import math\n",
    "\n",
    "# 동일한 rolling_minmax_scale 및 bin_and_encode 함수 사용 (위와 동일)\n",
    "def rolling_minmax_scale(series, window=24):\n",
    "    roll_min = series.rolling(window=window, min_periods=window).min()\n",
    "    roll_max = series.rolling(window=window, min_periods=window).max()\n",
    "    scaled = (series - roll_min) / ((roll_max - roll_min) + 1e-8)\n",
    "    scaled = scaled.replace([np.inf, -np.inf], np.nan)\n",
    "    scaled = scaled.fillna(1.0)\n",
    "    return scaled.clip(upper=1.0)\n",
    "\n",
    "def bin_and_encode(data, features, bins=100, drop_original=True):\n",
    "    for feature in features:\n",
    "        data[f'{feature}_Bin'] = pd.cut(data[feature], bins=bins, labels=False)\n",
    "        one_hot = pd.get_dummies(data[f'{feature}_Bin'], prefix=f'{feature}_Bin').astype(np.int32)\n",
    "        expected_columns = [f'{feature}_Bin_{i}' for i in range(bins)]\n",
    "        one_hot = one_hot.reindex(columns=expected_columns, fill_value=0)\n",
    "        data = pd.concat([data, one_hot], axis=1)\n",
    "        if drop_original:\n",
    "            data.drop(columns=[f'{feature}_Bin'], inplace=True)\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    for col in numeric_cols:\n",
    "        data[col] = data[col].astype(np.float32)\n",
    "    return data\n",
    "\n",
    "####################################\n",
    "# Dataset 및 모델 정의는 Variant 1와 동일\n",
    "####################################\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, input_data, target_data, lookback=24):\n",
    "        self.input_data = input_data.values\n",
    "        self.target_data = target_data.values\n",
    "        self.lookback = lookback\n",
    "    def __len__(self):\n",
    "        return len(self.input_data) - self.lookback\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.input_data[idx: idx + self.lookback, :]\n",
    "        y = self.target_data[idx + self.lookback, 0]\n",
    "        y_prev = self.target_data[idx + self.lookback - 1, 0]\n",
    "        y_target = 1 if y > y_prev else 0\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y_target, dtype=torch.long)\n",
    "\n",
    "class EncoderOnlyTransformerCustom(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=512, num_layers=6, nhead=8, ffn_dim=2048, num_classes=2, max_seq_len=24):\n",
    "        super(EncoderOnlyTransformerCustom, self).__init__()\n",
    "        self.token_embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embedding_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=nhead, dim_feedforward=ffn_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x = self.token_embedding(x)\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        pos_emb = self.position_embedding(positions)\n",
    "        x = x + pos_emb\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.transformer_encoder(x)\n",
    "        return self.fc(x[-1, :, :])\n",
    "\n",
    "####################################\n",
    "# Variant 2 실행: OHLC pct_change 기반\n",
    "####################################\n",
    "# CSV 파일 로드 – OHLC 데이터만 사용\n",
    "data = pd.read_csv(\"BTC_upbit_KRW_min60.csv\", index_col=0)\n",
    "data = data[['open','high','low','close']]\n",
    "data.index = pd.to_datetime(data.index)\n",
    "\n",
    "# 계산: 각 feature에 대해 pct_change * 100\n",
    "ohlc_features = ['open','high','low','close']\n",
    "for feature in ohlc_features:\n",
    "    data[feature + '_pct'] = data[feature].pct_change() * 100\n",
    "# 클립: (-25, 25) 구간으로\n",
    "for feature in ohlc_features:\n",
    "    col = feature + '_pct'\n",
    "    data[col] = data[col].clip(lower=-25, upper=25)\n",
    "    \n",
    "# one-hot 인코딩: pd.cut의 bins 인자는 np.linspace(-25,25,101)를 사용하여 100구간 생성\n",
    "features_pct = [f + '_pct' for f in ohlc_features]\n",
    "data = data.dropna()  # pct_change로 인해 생긴 결측 제거\n",
    "data = bin_and_encode(data, features_pct, bins=100, drop_original=True)\n",
    "\n",
    "# 타깃은 원본 close 값 사용 (원하는 경우 shift할 수도 있음)\n",
    "data['close_target'] = data['close']\n",
    "data = data.dropna()\n",
    "\n",
    "# 최종 입력: _Bin_ 열들만 사용\n",
    "final_input_columns = [col for col in data.columns if '_Bin_' in col]\n",
    "final_target_column = ['close_target']\n",
    "\n",
    "data_input = data[final_input_columns]\n",
    "data_target = data[final_target_column]\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return total_loss / len(data_loader), correct / total\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, lr, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_state = None\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, device)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = model.state_dict()\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(data, num_experiments=16, lookback=24, num_epochs=10):\n",
    "    # 최종 입력: 모든 _Bin_ 컬럼 + datetime one-hot + positional encoding + order 인코딩 컬럼\n",
    "    final_input_columns = [col for col in data.columns if '_Bin_' in col]\n",
    "    # datetime_onehot_features = [col for col in data.columns if col.startswith('Hour_') or \n",
    "    #                               col.startswith('Day_') or col.startswith('Week_') or \n",
    "    #                               col.startswith('Month_')]\n",
    "    # datetime_positional_features = ['hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'week_sin', 'week_cos', 'month_sin', 'month_cos']\n",
    "    # order_encoding_features = [col for col in data.columns if '_order_' in col]\n",
    "    # final_input_columns.extend(datetime_onehot_features)\n",
    "    # final_input_columns.extend(datetime_positional_features)\n",
    "    # final_input_columns.extend(order_encoding_features)\n",
    "    target_cols = ['close_target']\n",
    "    \n",
    "    data_input = data[final_input_columns]\n",
    "    data_target = data[target_cols]\n",
    "    \n",
    "    data_input = data_input.apply(pd.to_numeric).astype(np.float32)\n",
    "    data_target = data_target.apply(pd.to_numeric).astype(np.float32)\n",
    "    \n",
    "    step_size = 2500\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    val_acc_list = []\n",
    "    test_acc_list = []\n",
    "    \n",
    "    for exp in range(num_experiments):\n",
    "        train_start = exp * step_size\n",
    "        train_end = train_start + step_size * 8\n",
    "        val_end = train_end + step_size\n",
    "        test_end = val_end + step_size\n",
    "        if test_end > len(data_input):\n",
    "            break\n",
    "        print(f\"Experiment {exp}: 데이터 구간 [{train_start}:{test_end}]\")\n",
    "        print(data)\n",
    "        data.to_csv('AAPL_one_hot.csv')\n",
    "        \n",
    "        train_input = data_input.iloc[train_start:train_end]\n",
    "        train_target = data_target.iloc[train_start:train_end]\n",
    "        val_input = data_input.iloc[train_end:val_end]\n",
    "        val_target = data_target.iloc[train_end:val_end]\n",
    "        test_input = data_input.iloc[val_end:test_end]\n",
    "        test_target = data_target.iloc[val_end:test_end]\n",
    "        \n",
    "        train_dataset = TimeSeriesDataset(train_input, train_target, lookback=lookback)\n",
    "        val_dataset = TimeSeriesDataset(val_input, val_target, lookback=lookback)\n",
    "        test_dataset = TimeSeriesDataset(test_input, test_target, lookback=lookback)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        lr = 1e-4\n",
    "        input_dim = data_input.shape[1]\n",
    "        model = EncoderOnlyTransformerCustom(input_dim=input_dim, embedding_dim=512, num_layers=6, nhead=8,\n",
    "                                               ffn_dim=2048, num_classes=2, max_seq_len=lookback).to(device)\n",
    "        model_path = f\"model_experiment_{exp}.pth\"\n",
    "        if exp > 0:\n",
    "            try:\n",
    "                model.load_state_dict(torch.load(f\"model_experiment_{exp - 1}.pth\"))\n",
    "                print(f\"Loaded model from experiment {exp - 1} for fine-tuning.\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Model file for experiment {exp - 1} not found. Starting fresh training.\")\n",
    "        \n",
    "        print(f\"Experiment {exp}: Training with lr={lr} (Fine-Tuning)\")\n",
    "        model = train_model(model, train_loader, val_loader, num_epochs, lr, device)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Saved model for experiment {exp}.\")\n",
    "        \n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, device)\n",
    "        val_acc_list.append(val_acc)\n",
    "        print(f\"Experiment {exp}: Final Validation Accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "        test_loss, test_acc = evaluate_model(model, test_loader, device)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(f\"Experiment {exp}: Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    if val_acc_list:\n",
    "        avg_val_acc = sum(val_acc_list) / len(val_acc_list)\n",
    "        avg_test_acc = sum(test_acc_list) / len(test_acc_list)\n",
    "        print(f\"\\nFinal Average Validation Accuracy: {avg_val_acc:.4f}\")\n",
    "        print(f\"Final Average Test Accuracy: {avg_test_acc:.4f}\")\n",
    "    else:\n",
    "        print(\"실험이 한 번도 실행되지 않았습니다.\")\n",
    "\n",
    "# 참조용 리스트 (최종 구성에는 _Bin_ 컬럼들이 포함됨)\n",
    "# features_pct = [f + '_pct' for f in features_to_bin]\n",
    "# features_minmax = [f + '_minmax' for f in features_to_bin]\n",
    "\n",
    "train_and_evaluate(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_12720\\777871736.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[feature + '_pct'] = data[feature].pct_change() * 100\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_12720\\777871736.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[feature + '_pct'] = data[feature].pct_change() * 100\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_12720\\777871736.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[feature + '_pct'] = data[feature].pct_change() * 100\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_12720\\777871736.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[feature + '_pct'] = data[feature].pct_change() * 100\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_12720\\777871736.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{feature}_Bin'] = pd.cut(data[feature], bins=bins, labels=False)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_12720\\777871736.py:97: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['close_target'] = data['close']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 0: 데이터 구간 [0:25000]\n",
      "                         open      high       low     close  open_Bin_0  \\\n",
      "2017-09-25 13:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 17:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 19:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 20:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 21:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "...                       ...       ...       ...       ...         ...   \n",
      "2024-12-18 08:00:00  0.418090  0.421513  0.496974  0.467394         0.0   \n",
      "2024-12-18 09:00:00  0.470346  0.519246  0.298104  0.226233         0.0   \n",
      "2024-12-18 10:00:00  0.226700  0.344051  0.375151  0.006791         0.0   \n",
      "2024-12-18 11:00:00  0.008636  0.000000  0.322711  0.000000         1.0   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.109052         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 17:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 19:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 20:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 21:00:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 08:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 09:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 10:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 11:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:00:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_pct_Bin_91  close_pct_Bin_92  \\\n",
      "2017-09-25 13:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 17:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 19:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 20:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 21:00:00         0.0  ...               0.0               0.0   \n",
      "...                         ...  ...               ...               ...   \n",
      "2024-12-18 08:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 09:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 10:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 11:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 16:00:00         1.0  ...               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_93  close_pct_Bin_94  close_pct_Bin_95  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 08:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 09:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 10:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 11:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_96  close_pct_Bin_97  close_pct_Bin_98  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 08:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 09:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 10:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 11:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_99  close_target  \n",
      "2017-09-25 13:00:00               0.0      1.000000  \n",
      "2017-09-25 17:00:00               0.0      1.000000  \n",
      "2017-09-25 19:00:00               0.0      1.000000  \n",
      "2017-09-25 20:00:00               0.0      1.000000  \n",
      "2017-09-25 21:00:00               0.0      1.000000  \n",
      "...                               ...           ...  \n",
      "2024-12-18 08:00:00               0.0      0.467394  \n",
      "2024-12-18 09:00:00               0.0      0.226233  \n",
      "2024-12-18 10:00:00               0.0      0.006791  \n",
      "2024-12-18 11:00:00               0.0      0.000000  \n",
      "2024-12-18 16:00:00               1.0      0.109052  \n",
      "\n",
      "[57286 rows x 809 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 0: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.7195, Train Acc: 0.5131 | Val Loss: 0.6981, Val Acc: 0.4580\n",
      "Epoch 2/10 | Train Loss: 0.6956, Train Acc: 0.5186 | Val Loss: 0.7001, Val Acc: 0.4580\n",
      "Epoch 3/10 | Train Loss: 0.6948, Train Acc: 0.5147 | Val Loss: 0.6901, Val Acc: 0.5420\n",
      "Epoch 4/10 | Train Loss: 0.6887, Train Acc: 0.5208 | Val Loss: 0.6600, Val Acc: 0.5420\n",
      "Epoch 5/10 | Train Loss: 0.6796, Train Acc: 0.5380 | Val Loss: 0.6722, Val Acc: 0.6026\n",
      "Epoch 6/10 | Train Loss: 0.6914, Train Acc: 0.5318 | Val Loss: 0.6896, Val Acc: 0.5420\n",
      "Epoch 7/10 | Train Loss: 0.6930, Train Acc: 0.5193 | Val Loss: 0.6897, Val Acc: 0.5420\n",
      "Epoch 8/10 | Train Loss: 0.6922, Train Acc: 0.5212 | Val Loss: 0.6908, Val Acc: 0.5420\n",
      "Epoch 9/10 | Train Loss: 0.6925, Train Acc: 0.5234 | Val Loss: 0.6895, Val Acc: 0.5420\n",
      "Epoch 10/10 | Train Loss: 0.6916, Train Acc: 0.5249 | Val Loss: 0.6896, Val Acc: 0.5420\n",
      "Saved model for experiment 0.\n",
      "Experiment 0: Final Validation Accuracy: 0.5420\n",
      "Experiment 0: Test Accuracy: 0.5315\n",
      "Experiment 1: 데이터 구간 [2500:27500]\n",
      "                         open      high       low     close  open_Bin_0  \\\n",
      "2017-09-25 13:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 17:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 19:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 20:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 21:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "...                       ...       ...       ...       ...         ...   \n",
      "2024-12-18 08:00:00  0.418090  0.421513  0.496974  0.467394         0.0   \n",
      "2024-12-18 09:00:00  0.470346  0.519246  0.298104  0.226233         0.0   \n",
      "2024-12-18 10:00:00  0.226700  0.344051  0.375151  0.006791         0.0   \n",
      "2024-12-18 11:00:00  0.008636  0.000000  0.322711  0.000000         1.0   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.109052         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 17:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 19:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 20:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 21:00:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 08:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 09:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 10:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 11:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:00:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_pct_Bin_91  close_pct_Bin_92  \\\n",
      "2017-09-25 13:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 17:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 19:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 20:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 21:00:00         0.0  ...               0.0               0.0   \n",
      "...                         ...  ...               ...               ...   \n",
      "2024-12-18 08:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 09:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 10:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 11:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 16:00:00         1.0  ...               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_93  close_pct_Bin_94  close_pct_Bin_95  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 08:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 09:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 10:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 11:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_96  close_pct_Bin_97  close_pct_Bin_98  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 08:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 09:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 10:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 11:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_99  close_target  \n",
      "2017-09-25 13:00:00               0.0      1.000000  \n",
      "2017-09-25 17:00:00               0.0      1.000000  \n",
      "2017-09-25 19:00:00               0.0      1.000000  \n",
      "2017-09-25 20:00:00               0.0      1.000000  \n",
      "2017-09-25 21:00:00               0.0      1.000000  \n",
      "...                               ...           ...  \n",
      "2024-12-18 08:00:00               0.0      0.467394  \n",
      "2024-12-18 09:00:00               0.0      0.226233  \n",
      "2024-12-18 10:00:00               0.0      0.006791  \n",
      "2024-12-18 11:00:00               0.0      0.000000  \n",
      "2024-12-18 16:00:00               1.0      0.109052  \n",
      "\n",
      "[57286 rows x 809 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 0 for fine-tuning.\n",
      "Experiment 1: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6927, Train Acc: 0.5260 | Val Loss: 0.6933, Val Acc: 0.5315\n",
      "Epoch 2/10 | Train Loss: 0.6935, Train Acc: 0.5198 | Val Loss: 0.6911, Val Acc: 0.5315\n",
      "Epoch 3/10 | Train Loss: 0.6931, Train Acc: 0.5287 | Val Loss: 0.6912, Val Acc: 0.5315\n",
      "Epoch 4/10 | Train Loss: 0.6925, Train Acc: 0.5241 | Val Loss: 0.6912, Val Acc: 0.5315\n",
      "Epoch 5/10 | Train Loss: 0.6926, Train Acc: 0.5242 | Val Loss: 0.6921, Val Acc: 0.5315\n",
      "Epoch 6/10 | Train Loss: 0.6918, Train Acc: 0.5311 | Val Loss: 0.6911, Val Acc: 0.5315\n",
      "Epoch 7/10 | Train Loss: 0.6920, Train Acc: 0.5308 | Val Loss: 0.6913, Val Acc: 0.5315\n",
      "Epoch 8/10 | Train Loss: 0.6918, Train Acc: 0.5270 | Val Loss: 0.6913, Val Acc: 0.5315\n",
      "Epoch 9/10 | Train Loss: 0.6912, Train Acc: 0.5318 | Val Loss: 0.6914, Val Acc: 0.5315\n",
      "Epoch 10/10 | Train Loss: 0.6911, Train Acc: 0.5340 | Val Loss: 0.6642, Val Acc: 0.5626\n",
      "Saved model for experiment 1.\n",
      "Experiment 1: Final Validation Accuracy: 0.5626\n",
      "Experiment 1: Test Accuracy: 0.5691\n",
      "Experiment 2: 데이터 구간 [5000:30000]\n",
      "                         open      high       low     close  open_Bin_0  \\\n",
      "2017-09-25 13:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 17:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 19:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 20:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 21:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "...                       ...       ...       ...       ...         ...   \n",
      "2024-12-18 08:00:00  0.418090  0.421513  0.496974  0.467394         0.0   \n",
      "2024-12-18 09:00:00  0.470346  0.519246  0.298104  0.226233         0.0   \n",
      "2024-12-18 10:00:00  0.226700  0.344051  0.375151  0.006791         0.0   \n",
      "2024-12-18 11:00:00  0.008636  0.000000  0.322711  0.000000         1.0   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.109052         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 17:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 19:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 20:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 21:00:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 08:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 09:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 10:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 11:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:00:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_pct_Bin_91  close_pct_Bin_92  \\\n",
      "2017-09-25 13:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 17:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 19:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 20:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 21:00:00         0.0  ...               0.0               0.0   \n",
      "...                         ...  ...               ...               ...   \n",
      "2024-12-18 08:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 09:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 10:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 11:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 16:00:00         1.0  ...               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_93  close_pct_Bin_94  close_pct_Bin_95  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 08:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 09:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 10:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 11:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_96  close_pct_Bin_97  close_pct_Bin_98  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 08:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 09:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 10:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 11:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_99  close_target  \n",
      "2017-09-25 13:00:00               0.0      1.000000  \n",
      "2017-09-25 17:00:00               0.0      1.000000  \n",
      "2017-09-25 19:00:00               0.0      1.000000  \n",
      "2017-09-25 20:00:00               0.0      1.000000  \n",
      "2017-09-25 21:00:00               0.0      1.000000  \n",
      "...                               ...           ...  \n",
      "2024-12-18 08:00:00               0.0      0.467394  \n",
      "2024-12-18 09:00:00               0.0      0.226233  \n",
      "2024-12-18 10:00:00               0.0      0.006791  \n",
      "2024-12-18 11:00:00               0.0      0.000000  \n",
      "2024-12-18 16:00:00               1.0      0.109052  \n",
      "\n",
      "[57286 rows x 809 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 1 for fine-tuning.\n",
      "Experiment 2: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6660, Train Acc: 0.5283 | Val Loss: 0.6438, Val Acc: 0.5497\n",
      "Epoch 2/10 | Train Loss: 0.6654, Train Acc: 0.5233 | Val Loss: 0.6449, Val Acc: 0.5497\n",
      "Epoch 3/10 | Train Loss: 0.6735, Train Acc: 0.5263 | Val Loss: 0.6901, Val Acc: 0.5497\n",
      "Epoch 4/10 | Train Loss: 0.6923, Train Acc: 0.5251 | Val Loss: 0.6883, Val Acc: 0.5497\n",
      "Epoch 5/10 | Train Loss: 0.6914, Train Acc: 0.5326 | Val Loss: 0.6883, Val Acc: 0.5497\n",
      "Epoch 6/10 | Train Loss: 0.6915, Train Acc: 0.5335 | Val Loss: 0.6890, Val Acc: 0.5497\n",
      "Epoch 7/10 | Train Loss: 0.6915, Train Acc: 0.5330 | Val Loss: 0.6884, Val Acc: 0.5497\n",
      "Epoch 8/10 | Train Loss: 0.6914, Train Acc: 0.5332 | Val Loss: 0.6883, Val Acc: 0.5497\n",
      "Epoch 9/10 | Train Loss: 0.6909, Train Acc: 0.5330 | Val Loss: 0.6884, Val Acc: 0.5497\n",
      "Epoch 10/10 | Train Loss: 0.6914, Train Acc: 0.5335 | Val Loss: 0.6883, Val Acc: 0.5497\n",
      "Saved model for experiment 2.\n",
      "Experiment 2: Final Validation Accuracy: 0.5497\n",
      "Experiment 2: Test Accuracy: 0.5158\n",
      "Experiment 3: 데이터 구간 [7500:32500]\n",
      "                         open      high       low     close  open_Bin_0  \\\n",
      "2017-09-25 13:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 17:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 19:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 20:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 21:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "...                       ...       ...       ...       ...         ...   \n",
      "2024-12-18 08:00:00  0.418090  0.421513  0.496974  0.467394         0.0   \n",
      "2024-12-18 09:00:00  0.470346  0.519246  0.298104  0.226233         0.0   \n",
      "2024-12-18 10:00:00  0.226700  0.344051  0.375151  0.006791         0.0   \n",
      "2024-12-18 11:00:00  0.008636  0.000000  0.322711  0.000000         1.0   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.109052         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 17:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 19:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 20:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 21:00:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 08:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 09:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 10:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 11:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:00:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_pct_Bin_91  close_pct_Bin_92  \\\n",
      "2017-09-25 13:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 17:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 19:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 20:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 21:00:00         0.0  ...               0.0               0.0   \n",
      "...                         ...  ...               ...               ...   \n",
      "2024-12-18 08:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 09:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 10:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 11:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 16:00:00         1.0  ...               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_93  close_pct_Bin_94  close_pct_Bin_95  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 08:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 09:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 10:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 11:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_96  close_pct_Bin_97  close_pct_Bin_98  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 08:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 09:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 10:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 11:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_99  close_target  \n",
      "2017-09-25 13:00:00               0.0      1.000000  \n",
      "2017-09-25 17:00:00               0.0      1.000000  \n",
      "2017-09-25 19:00:00               0.0      1.000000  \n",
      "2017-09-25 20:00:00               0.0      1.000000  \n",
      "2017-09-25 21:00:00               0.0      1.000000  \n",
      "...                               ...           ...  \n",
      "2024-12-18 08:00:00               0.0      0.467394  \n",
      "2024-12-18 09:00:00               0.0      0.226233  \n",
      "2024-12-18 10:00:00               0.0      0.006791  \n",
      "2024-12-18 11:00:00               0.0      0.000000  \n",
      "2024-12-18 16:00:00               1.0      0.109052  \n",
      "\n",
      "[57286 rows x 809 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 2 for fine-tuning.\n",
      "Experiment 3: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6913, Train Acc: 0.5340 | Val Loss: 0.7040, Val Acc: 0.5158\n",
      "Epoch 2/10 | Train Loss: 0.6909, Train Acc: 0.5360 | Val Loss: 0.6903, Val Acc: 0.5307\n",
      "Epoch 3/10 | Train Loss: 0.6877, Train Acc: 0.5410 | Val Loss: 0.6849, Val Acc: 0.5359\n",
      "Epoch 4/10 | Train Loss: 0.6908, Train Acc: 0.5355 | Val Loss: 0.6927, Val Acc: 0.5158\n",
      "Epoch 5/10 | Train Loss: 0.6919, Train Acc: 0.5307 | Val Loss: 0.6932, Val Acc: 0.5158\n",
      "Epoch 6/10 | Train Loss: 0.6913, Train Acc: 0.5331 | Val Loss: 0.6940, Val Acc: 0.5158\n",
      "Epoch 7/10 | Train Loss: 0.6911, Train Acc: 0.5361 | Val Loss: 0.6927, Val Acc: 0.5158\n",
      "Epoch 8/10 | Train Loss: 0.6911, Train Acc: 0.5358 | Val Loss: 0.6929, Val Acc: 0.5158\n",
      "Epoch 9/10 | Train Loss: 0.6911, Train Acc: 0.5357 | Val Loss: 0.6946, Val Acc: 0.5158\n",
      "Epoch 10/10 | Train Loss: 0.6908, Train Acc: 0.5361 | Val Loss: 0.6937, Val Acc: 0.5158\n",
      "Saved model for experiment 3.\n",
      "Experiment 3: Final Validation Accuracy: 0.5158\n",
      "Experiment 3: Test Accuracy: 0.5452\n",
      "Experiment 4: 데이터 구간 [10000:35000]\n",
      "                         open      high       low     close  open_Bin_0  \\\n",
      "2017-09-25 13:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 17:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 19:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 20:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 21:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "...                       ...       ...       ...       ...         ...   \n",
      "2024-12-18 08:00:00  0.418090  0.421513  0.496974  0.467394         0.0   \n",
      "2024-12-18 09:00:00  0.470346  0.519246  0.298104  0.226233         0.0   \n",
      "2024-12-18 10:00:00  0.226700  0.344051  0.375151  0.006791         0.0   \n",
      "2024-12-18 11:00:00  0.008636  0.000000  0.322711  0.000000         1.0   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.109052         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 17:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 19:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 20:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 21:00:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 08:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 09:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 10:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 11:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:00:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_pct_Bin_91  close_pct_Bin_92  \\\n",
      "2017-09-25 13:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 17:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 19:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 20:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 21:00:00         0.0  ...               0.0               0.0   \n",
      "...                         ...  ...               ...               ...   \n",
      "2024-12-18 08:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 09:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 10:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 11:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 16:00:00         1.0  ...               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_93  close_pct_Bin_94  close_pct_Bin_95  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 08:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 09:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 10:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 11:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_96  close_pct_Bin_97  close_pct_Bin_98  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 08:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 09:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 10:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 11:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_99  close_target  \n",
      "2017-09-25 13:00:00               0.0      1.000000  \n",
      "2017-09-25 17:00:00               0.0      1.000000  \n",
      "2017-09-25 19:00:00               0.0      1.000000  \n",
      "2017-09-25 20:00:00               0.0      1.000000  \n",
      "2017-09-25 21:00:00               0.0      1.000000  \n",
      "...                               ...           ...  \n",
      "2024-12-18 08:00:00               0.0      0.467394  \n",
      "2024-12-18 09:00:00               0.0      0.226233  \n",
      "2024-12-18 10:00:00               0.0      0.006791  \n",
      "2024-12-18 11:00:00               0.0      0.000000  \n",
      "2024-12-18 16:00:00               1.0      0.109052  \n",
      "\n",
      "[57286 rows x 809 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 3 for fine-tuning.\n",
      "Experiment 4: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6919, Train Acc: 0.5300 | Val Loss: 0.6891, Val Acc: 0.5452\n",
      "Epoch 2/10 | Train Loss: 0.6916, Train Acc: 0.5304 | Val Loss: 0.6891, Val Acc: 0.5452\n",
      "Epoch 3/10 | Train Loss: 0.6920, Train Acc: 0.5290 | Val Loss: 0.6894, Val Acc: 0.5452\n",
      "Epoch 4/10 | Train Loss: 0.6920, Train Acc: 0.5273 | Val Loss: 0.6892, Val Acc: 0.5452\n",
      "Epoch 5/10 | Train Loss: 0.6915, Train Acc: 0.5332 | Val Loss: 0.6899, Val Acc: 0.5452\n",
      "Epoch 6/10 | Train Loss: 0.6914, Train Acc: 0.5326 | Val Loss: 0.6897, Val Acc: 0.5452\n",
      "Epoch 7/10 | Train Loss: 0.6913, Train Acc: 0.5333 | Val Loss: 0.6905, Val Acc: 0.5452\n",
      "Epoch 8/10 | Train Loss: 0.6912, Train Acc: 0.5337 | Val Loss: 0.6908, Val Acc: 0.5452\n",
      "Epoch 9/10 | Train Loss: 0.6912, Train Acc: 0.5335 | Val Loss: 0.6892, Val Acc: 0.5452\n",
      "Epoch 10/10 | Train Loss: 0.6910, Train Acc: 0.5327 | Val Loss: 0.6897, Val Acc: 0.5452\n",
      "Saved model for experiment 4.\n",
      "Experiment 4: Final Validation Accuracy: 0.5452\n",
      "Experiment 4: Test Accuracy: 0.5343\n",
      "Experiment 5: 데이터 구간 [12500:37500]\n",
      "                         open      high       low     close  open_Bin_0  \\\n",
      "2017-09-25 13:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 17:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 19:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 20:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 21:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "...                       ...       ...       ...       ...         ...   \n",
      "2024-12-18 08:00:00  0.418090  0.421513  0.496974  0.467394         0.0   \n",
      "2024-12-18 09:00:00  0.470346  0.519246  0.298104  0.226233         0.0   \n",
      "2024-12-18 10:00:00  0.226700  0.344051  0.375151  0.006791         0.0   \n",
      "2024-12-18 11:00:00  0.008636  0.000000  0.322711  0.000000         1.0   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.109052         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 17:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 19:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 20:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 21:00:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 08:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 09:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 10:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 11:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:00:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_pct_Bin_91  close_pct_Bin_92  \\\n",
      "2017-09-25 13:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 17:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 19:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 20:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 21:00:00         0.0  ...               0.0               0.0   \n",
      "...                         ...  ...               ...               ...   \n",
      "2024-12-18 08:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 09:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 10:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 11:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 16:00:00         1.0  ...               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_93  close_pct_Bin_94  close_pct_Bin_95  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 08:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 09:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 10:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 11:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_96  close_pct_Bin_97  close_pct_Bin_98  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 08:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 09:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 10:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 11:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_99  close_target  \n",
      "2017-09-25 13:00:00               0.0      1.000000  \n",
      "2017-09-25 17:00:00               0.0      1.000000  \n",
      "2017-09-25 19:00:00               0.0      1.000000  \n",
      "2017-09-25 20:00:00               0.0      1.000000  \n",
      "2017-09-25 21:00:00               0.0      1.000000  \n",
      "...                               ...           ...  \n",
      "2024-12-18 08:00:00               0.0      0.467394  \n",
      "2024-12-18 09:00:00               0.0      0.226233  \n",
      "2024-12-18 10:00:00               0.0      0.006791  \n",
      "2024-12-18 11:00:00               0.0      0.000000  \n",
      "2024-12-18 16:00:00               1.0      0.109052  \n",
      "\n",
      "[57286 rows x 809 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 4 for fine-tuning.\n",
      "Experiment 5: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6917, Train Acc: 0.5336 | Val Loss: 0.6910, Val Acc: 0.5343\n",
      "Epoch 2/10 | Train Loss: 0.6913, Train Acc: 0.5344 | Val Loss: 0.6911, Val Acc: 0.5343\n",
      "Epoch 3/10 | Train Loss: 0.6914, Train Acc: 0.5347 | Val Loss: 0.6911, Val Acc: 0.5343\n",
      "Epoch 4/10 | Train Loss: 0.6914, Train Acc: 0.5341 | Val Loss: 0.6911, Val Acc: 0.5343\n",
      "Epoch 5/10 | Train Loss: 0.6913, Train Acc: 0.5355 | Val Loss: 0.6912, Val Acc: 0.5343\n",
      "Epoch 6/10 | Train Loss: 0.6909, Train Acc: 0.5356 | Val Loss: 0.6916, Val Acc: 0.5343\n",
      "Epoch 7/10 | Train Loss: 0.6908, Train Acc: 0.5354 | Val Loss: 0.6911, Val Acc: 0.5343\n",
      "Epoch 8/10 | Train Loss: 0.6909, Train Acc: 0.5341 | Val Loss: 0.6914, Val Acc: 0.5343\n",
      "Epoch 9/10 | Train Loss: 0.6909, Train Acc: 0.5354 | Val Loss: 0.6910, Val Acc: 0.5343\n",
      "Epoch 10/10 | Train Loss: 0.6908, Train Acc: 0.5355 | Val Loss: 0.6917, Val Acc: 0.5343\n",
      "Saved model for experiment 5.\n",
      "Experiment 5: Final Validation Accuracy: 0.5343\n",
      "Experiment 5: Test Accuracy: 0.5295\n",
      "Experiment 6: 데이터 구간 [15000:40000]\n",
      "                         open      high       low     close  open_Bin_0  \\\n",
      "2017-09-25 13:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 17:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 19:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 20:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 21:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "...                       ...       ...       ...       ...         ...   \n",
      "2024-12-18 08:00:00  0.418090  0.421513  0.496974  0.467394         0.0   \n",
      "2024-12-18 09:00:00  0.470346  0.519246  0.298104  0.226233         0.0   \n",
      "2024-12-18 10:00:00  0.226700  0.344051  0.375151  0.006791         0.0   \n",
      "2024-12-18 11:00:00  0.008636  0.000000  0.322711  0.000000         1.0   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.109052         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 17:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 19:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 20:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 21:00:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 08:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 09:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 10:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 11:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:00:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_pct_Bin_91  close_pct_Bin_92  \\\n",
      "2017-09-25 13:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 17:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 19:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 20:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 21:00:00         0.0  ...               0.0               0.0   \n",
      "...                         ...  ...               ...               ...   \n",
      "2024-12-18 08:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 09:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 10:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 11:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 16:00:00         1.0  ...               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_93  close_pct_Bin_94  close_pct_Bin_95  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 08:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 09:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 10:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 11:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_96  close_pct_Bin_97  close_pct_Bin_98  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 08:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 09:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 10:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 11:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_99  close_target  \n",
      "2017-09-25 13:00:00               0.0      1.000000  \n",
      "2017-09-25 17:00:00               0.0      1.000000  \n",
      "2017-09-25 19:00:00               0.0      1.000000  \n",
      "2017-09-25 20:00:00               0.0      1.000000  \n",
      "2017-09-25 21:00:00               0.0      1.000000  \n",
      "...                               ...           ...  \n",
      "2024-12-18 08:00:00               0.0      0.467394  \n",
      "2024-12-18 09:00:00               0.0      0.226233  \n",
      "2024-12-18 10:00:00               0.0      0.006791  \n",
      "2024-12-18 11:00:00               0.0      0.000000  \n",
      "2024-12-18 16:00:00               1.0      0.109052  \n",
      "\n",
      "[57286 rows x 809 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 5 for fine-tuning.\n",
      "Experiment 6: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6915, Train Acc: 0.5299 | Val Loss: 0.6916, Val Acc: 0.5295\n",
      "Epoch 2/10 | Train Loss: 0.6918, Train Acc: 0.5317 | Val Loss: 0.6916, Val Acc: 0.5295\n",
      "Epoch 3/10 | Train Loss: 0.6917, Train Acc: 0.5344 | Val Loss: 0.6922, Val Acc: 0.5295\n",
      "Epoch 4/10 | Train Loss: 0.6919, Train Acc: 0.5268 | Val Loss: 0.6959, Val Acc: 0.5295\n",
      "Epoch 5/10 | Train Loss: 0.6913, Train Acc: 0.5337 | Val Loss: 0.6917, Val Acc: 0.5295\n",
      "Epoch 6/10 | Train Loss: 0.6914, Train Acc: 0.5337 | Val Loss: 0.6916, Val Acc: 0.5295\n",
      "Epoch 7/10 | Train Loss: 0.6911, Train Acc: 0.5345 | Val Loss: 0.6916, Val Acc: 0.5295\n",
      "Epoch 8/10 | Train Loss: 0.6913, Train Acc: 0.5335 | Val Loss: 0.6927, Val Acc: 0.5295\n",
      "Epoch 9/10 | Train Loss: 0.6912, Train Acc: 0.5340 | Val Loss: 0.6920, Val Acc: 0.5295\n",
      "Epoch 10/10 | Train Loss: 0.6913, Train Acc: 0.5340 | Val Loss: 0.6920, Val Acc: 0.5295\n",
      "Saved model for experiment 6.\n",
      "Experiment 6: Final Validation Accuracy: 0.5295\n",
      "Experiment 6: Test Accuracy: 0.5141\n",
      "Experiment 7: 데이터 구간 [17500:42500]\n",
      "                         open      high       low     close  open_Bin_0  \\\n",
      "2017-09-25 13:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 17:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 19:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 20:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 21:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "...                       ...       ...       ...       ...         ...   \n",
      "2024-12-18 08:00:00  0.418090  0.421513  0.496974  0.467394         0.0   \n",
      "2024-12-18 09:00:00  0.470346  0.519246  0.298104  0.226233         0.0   \n",
      "2024-12-18 10:00:00  0.226700  0.344051  0.375151  0.006791         0.0   \n",
      "2024-12-18 11:00:00  0.008636  0.000000  0.322711  0.000000         1.0   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.109052         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 17:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 19:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 20:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 21:00:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 08:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 09:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 10:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 11:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:00:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_pct_Bin_91  close_pct_Bin_92  \\\n",
      "2017-09-25 13:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 17:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 19:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 20:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 21:00:00         0.0  ...               0.0               0.0   \n",
      "...                         ...  ...               ...               ...   \n",
      "2024-12-18 08:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 09:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 10:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 11:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 16:00:00         1.0  ...               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_93  close_pct_Bin_94  close_pct_Bin_95  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 08:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 09:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 10:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 11:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_96  close_pct_Bin_97  close_pct_Bin_98  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 08:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 09:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 10:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 11:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_99  close_target  \n",
      "2017-09-25 13:00:00               0.0      1.000000  \n",
      "2017-09-25 17:00:00               0.0      1.000000  \n",
      "2017-09-25 19:00:00               0.0      1.000000  \n",
      "2017-09-25 20:00:00               0.0      1.000000  \n",
      "2017-09-25 21:00:00               0.0      1.000000  \n",
      "...                               ...           ...  \n",
      "2024-12-18 08:00:00               0.0      0.467394  \n",
      "2024-12-18 09:00:00               0.0      0.226233  \n",
      "2024-12-18 10:00:00               0.0      0.006791  \n",
      "2024-12-18 11:00:00               0.0      0.000000  \n",
      "2024-12-18 16:00:00               1.0      0.109052  \n",
      "\n",
      "[57286 rows x 809 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 6 for fine-tuning.\n",
      "Experiment 7: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6915, Train Acc: 0.5306 | Val Loss: 0.6969, Val Acc: 0.5141\n",
      "Epoch 2/10 | Train Loss: 0.6917, Train Acc: 0.5339 | Val Loss: 0.6970, Val Acc: 0.5141\n",
      "Epoch 3/10 | Train Loss: 0.6919, Train Acc: 0.5315 | Val Loss: 0.6928, Val Acc: 0.5141\n",
      "Epoch 4/10 | Train Loss: 0.6921, Train Acc: 0.5300 | Val Loss: 0.6934, Val Acc: 0.5141\n",
      "Epoch 5/10 | Train Loss: 0.6916, Train Acc: 0.5310 | Val Loss: 0.6929, Val Acc: 0.5141\n",
      "Epoch 6/10 | Train Loss: 0.6914, Train Acc: 0.5317 | Val Loss: 0.6929, Val Acc: 0.5141\n",
      "Epoch 7/10 | Train Loss: 0.6911, Train Acc: 0.5337 | Val Loss: 0.6953, Val Acc: 0.5141\n",
      "Epoch 8/10 | Train Loss: 0.6913, Train Acc: 0.5335 | Val Loss: 0.6947, Val Acc: 0.5141\n",
      "Epoch 9/10 | Train Loss: 0.6910, Train Acc: 0.5345 | Val Loss: 0.6949, Val Acc: 0.5141\n",
      "Epoch 10/10 | Train Loss: 0.6909, Train Acc: 0.5345 | Val Loss: 0.6931, Val Acc: 0.5141\n",
      "Saved model for experiment 7.\n",
      "Experiment 7: Final Validation Accuracy: 0.5141\n",
      "Experiment 7: Test Accuracy: 0.5153\n",
      "Experiment 8: 데이터 구간 [20000:45000]\n",
      "                         open      high       low     close  open_Bin_0  \\\n",
      "2017-09-25 13:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 17:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 19:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 20:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 21:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "...                       ...       ...       ...       ...         ...   \n",
      "2024-12-18 08:00:00  0.418090  0.421513  0.496974  0.467394         0.0   \n",
      "2024-12-18 09:00:00  0.470346  0.519246  0.298104  0.226233         0.0   \n",
      "2024-12-18 10:00:00  0.226700  0.344051  0.375151  0.006791         0.0   \n",
      "2024-12-18 11:00:00  0.008636  0.000000  0.322711  0.000000         1.0   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.109052         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 17:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 19:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 20:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 21:00:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 08:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 09:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 10:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 11:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:00:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_pct_Bin_91  close_pct_Bin_92  \\\n",
      "2017-09-25 13:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 17:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 19:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 20:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 21:00:00         0.0  ...               0.0               0.0   \n",
      "...                         ...  ...               ...               ...   \n",
      "2024-12-18 08:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 09:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 10:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 11:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 16:00:00         1.0  ...               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_93  close_pct_Bin_94  close_pct_Bin_95  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 08:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 09:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 10:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 11:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_96  close_pct_Bin_97  close_pct_Bin_98  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 08:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 09:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 10:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 11:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_99  close_target  \n",
      "2017-09-25 13:00:00               0.0      1.000000  \n",
      "2017-09-25 17:00:00               0.0      1.000000  \n",
      "2017-09-25 19:00:00               0.0      1.000000  \n",
      "2017-09-25 20:00:00               0.0      1.000000  \n",
      "2017-09-25 21:00:00               0.0      1.000000  \n",
      "...                               ...           ...  \n",
      "2024-12-18 08:00:00               0.0      0.467394  \n",
      "2024-12-18 09:00:00               0.0      0.226233  \n",
      "2024-12-18 10:00:00               0.0      0.006791  \n",
      "2024-12-18 11:00:00               0.0      0.000000  \n",
      "2024-12-18 16:00:00               1.0      0.109052  \n",
      "\n",
      "[57286 rows x 809 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 7 for fine-tuning.\n",
      "Experiment 8: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6919, Train Acc: 0.5247 | Val Loss: 0.6942, Val Acc: 0.5153\n",
      "Epoch 2/10 | Train Loss: 0.6916, Train Acc: 0.5314 | Val Loss: 0.6943, Val Acc: 0.5153\n",
      "Epoch 3/10 | Train Loss: 0.6922, Train Acc: 0.5295 | Val Loss: 0.6963, Val Acc: 0.5153\n",
      "Epoch 4/10 | Train Loss: 0.6916, Train Acc: 0.5302 | Val Loss: 0.6932, Val Acc: 0.5153\n",
      "Epoch 5/10 | Train Loss: 0.6918, Train Acc: 0.5265 | Val Loss: 0.6972, Val Acc: 0.5153\n",
      "Epoch 6/10 | Train Loss: 0.6915, Train Acc: 0.5320 | Val Loss: 0.6932, Val Acc: 0.4847\n",
      "Epoch 7/10 | Train Loss: 0.6919, Train Acc: 0.5294 | Val Loss: 0.6928, Val Acc: 0.5153\n",
      "Epoch 8/10 | Train Loss: 0.6921, Train Acc: 0.5278 | Val Loss: 0.6971, Val Acc: 0.5153\n",
      "Epoch 9/10 | Train Loss: 0.6918, Train Acc: 0.5311 | Val Loss: 0.6934, Val Acc: 0.5153\n",
      "Epoch 10/10 | Train Loss: 0.6918, Train Acc: 0.5301 | Val Loss: 0.6928, Val Acc: 0.5153\n",
      "Saved model for experiment 8.\n",
      "Experiment 8: Final Validation Accuracy: 0.5153\n",
      "Experiment 8: Test Accuracy: 0.5327\n",
      "Experiment 9: 데이터 구간 [22500:47500]\n",
      "                         open      high       low     close  open_Bin_0  \\\n",
      "2017-09-25 13:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 17:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 19:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 20:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 21:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "...                       ...       ...       ...       ...         ...   \n",
      "2024-12-18 08:00:00  0.418090  0.421513  0.496974  0.467394         0.0   \n",
      "2024-12-18 09:00:00  0.470346  0.519246  0.298104  0.226233         0.0   \n",
      "2024-12-18 10:00:00  0.226700  0.344051  0.375151  0.006791         0.0   \n",
      "2024-12-18 11:00:00  0.008636  0.000000  0.322711  0.000000         1.0   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.109052         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 17:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 19:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 20:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 21:00:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 08:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 09:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 10:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 11:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:00:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_pct_Bin_91  close_pct_Bin_92  \\\n",
      "2017-09-25 13:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 17:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 19:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 20:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 21:00:00         0.0  ...               0.0               0.0   \n",
      "...                         ...  ...               ...               ...   \n",
      "2024-12-18 08:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 09:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 10:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 11:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 16:00:00         1.0  ...               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_93  close_pct_Bin_94  close_pct_Bin_95  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 08:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 09:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 10:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 11:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_96  close_pct_Bin_97  close_pct_Bin_98  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 08:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 09:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 10:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 11:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_99  close_target  \n",
      "2017-09-25 13:00:00               0.0      1.000000  \n",
      "2017-09-25 17:00:00               0.0      1.000000  \n",
      "2017-09-25 19:00:00               0.0      1.000000  \n",
      "2017-09-25 20:00:00               0.0      1.000000  \n",
      "2017-09-25 21:00:00               0.0      1.000000  \n",
      "...                               ...           ...  \n",
      "2024-12-18 08:00:00               0.0      0.467394  \n",
      "2024-12-18 09:00:00               0.0      0.226233  \n",
      "2024-12-18 10:00:00               0.0      0.006791  \n",
      "2024-12-18 11:00:00               0.0      0.000000  \n",
      "2024-12-18 16:00:00               1.0      0.109052  \n",
      "\n",
      "[57286 rows x 809 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 8 for fine-tuning.\n",
      "Experiment 9: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6920, Train Acc: 0.5281 | Val Loss: 0.6910, Val Acc: 0.5327\n",
      "Epoch 2/10 | Train Loss: 0.6921, Train Acc: 0.5255 | Val Loss: 0.6919, Val Acc: 0.5327\n",
      "Epoch 3/10 | Train Loss: 0.6920, Train Acc: 0.5249 | Val Loss: 0.6912, Val Acc: 0.5327\n",
      "Epoch 4/10 | Train Loss: 0.6923, Train Acc: 0.5243 | Val Loss: 0.6910, Val Acc: 0.5327\n",
      "Epoch 5/10 | Train Loss: 0.6919, Train Acc: 0.5249 | Val Loss: 0.6910, Val Acc: 0.5327\n",
      "Epoch 6/10 | Train Loss: 0.6920, Train Acc: 0.5289 | Val Loss: 0.6910, Val Acc: 0.5327\n",
      "Epoch 7/10 | Train Loss: 0.6919, Train Acc: 0.5288 | Val Loss: 0.6910, Val Acc: 0.5327\n",
      "Epoch 8/10 | Train Loss: 0.6917, Train Acc: 0.5293 | Val Loss: 0.6911, Val Acc: 0.5327\n",
      "Epoch 9/10 | Train Loss: 0.6919, Train Acc: 0.5296 | Val Loss: 0.6910, Val Acc: 0.5327\n",
      "Epoch 10/10 | Train Loss: 0.6917, Train Acc: 0.5296 | Val Loss: 0.6911, Val Acc: 0.5327\n",
      "Saved model for experiment 9.\n",
      "Experiment 9: Final Validation Accuracy: 0.5327\n",
      "Experiment 9: Test Accuracy: 0.5234\n",
      "Experiment 10: 데이터 구간 [25000:50000]\n",
      "                         open      high       low     close  open_Bin_0  \\\n",
      "2017-09-25 13:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 17:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 19:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 20:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 21:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "...                       ...       ...       ...       ...         ...   \n",
      "2024-12-18 08:00:00  0.418090  0.421513  0.496974  0.467394         0.0   \n",
      "2024-12-18 09:00:00  0.470346  0.519246  0.298104  0.226233         0.0   \n",
      "2024-12-18 10:00:00  0.226700  0.344051  0.375151  0.006791         0.0   \n",
      "2024-12-18 11:00:00  0.008636  0.000000  0.322711  0.000000         1.0   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.109052         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 17:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 19:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 20:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 21:00:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 08:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 09:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 10:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 11:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:00:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_pct_Bin_91  close_pct_Bin_92  \\\n",
      "2017-09-25 13:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 17:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 19:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 20:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 21:00:00         0.0  ...               0.0               0.0   \n",
      "...                         ...  ...               ...               ...   \n",
      "2024-12-18 08:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 09:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 10:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 11:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 16:00:00         1.0  ...               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_93  close_pct_Bin_94  close_pct_Bin_95  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 08:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 09:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 10:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 11:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_96  close_pct_Bin_97  close_pct_Bin_98  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 08:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 09:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 10:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 11:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_99  close_target  \n",
      "2017-09-25 13:00:00               0.0      1.000000  \n",
      "2017-09-25 17:00:00               0.0      1.000000  \n",
      "2017-09-25 19:00:00               0.0      1.000000  \n",
      "2017-09-25 20:00:00               0.0      1.000000  \n",
      "2017-09-25 21:00:00               0.0      1.000000  \n",
      "...                               ...           ...  \n",
      "2024-12-18 08:00:00               0.0      0.467394  \n",
      "2024-12-18 09:00:00               0.0      0.226233  \n",
      "2024-12-18 10:00:00               0.0      0.006791  \n",
      "2024-12-18 11:00:00               0.0      0.000000  \n",
      "2024-12-18 16:00:00               1.0      0.109052  \n",
      "\n",
      "[57286 rows x 809 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 9 for fine-tuning.\n",
      "Experiment 10: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6921, Train Acc: 0.5266 | Val Loss: 0.6956, Val Acc: 0.5234\n",
      "Epoch 2/10 | Train Loss: 0.6923, Train Acc: 0.5259 | Val Loss: 0.6936, Val Acc: 0.5234\n",
      "Epoch 3/10 | Train Loss: 0.6919, Train Acc: 0.5286 | Val Loss: 0.6924, Val Acc: 0.5234\n",
      "Epoch 4/10 | Train Loss: 0.6926, Train Acc: 0.5286 | Val Loss: 0.6925, Val Acc: 0.5234\n",
      "Epoch 5/10 | Train Loss: 0.6919, Train Acc: 0.5238 | Val Loss: 0.6954, Val Acc: 0.5234\n",
      "Epoch 6/10 | Train Loss: 0.6920, Train Acc: 0.5288 | Val Loss: 0.6941, Val Acc: 0.4766\n",
      "Epoch 7/10 | Train Loss: 0.6919, Train Acc: 0.5275 | Val Loss: 0.6925, Val Acc: 0.5234\n",
      "Epoch 8/10 | Train Loss: 0.6919, Train Acc: 0.5298 | Val Loss: 0.6924, Val Acc: 0.5234\n",
      "Epoch 9/10 | Train Loss: 0.6917, Train Acc: 0.5300 | Val Loss: 0.6922, Val Acc: 0.5234\n",
      "Epoch 10/10 | Train Loss: 0.6914, Train Acc: 0.5287 | Val Loss: 0.6923, Val Acc: 0.5234\n",
      "Saved model for experiment 10.\n",
      "Experiment 10: Final Validation Accuracy: 0.5234\n",
      "Experiment 10: Test Accuracy: 0.5295\n",
      "Experiment 11: 데이터 구간 [27500:52500]\n",
      "                         open      high       low     close  open_Bin_0  \\\n",
      "2017-09-25 13:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 17:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 19:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 20:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 21:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "...                       ...       ...       ...       ...         ...   \n",
      "2024-12-18 08:00:00  0.418090  0.421513  0.496974  0.467394         0.0   \n",
      "2024-12-18 09:00:00  0.470346  0.519246  0.298104  0.226233         0.0   \n",
      "2024-12-18 10:00:00  0.226700  0.344051  0.375151  0.006791         0.0   \n",
      "2024-12-18 11:00:00  0.008636  0.000000  0.322711  0.000000         1.0   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.109052         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 17:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 19:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 20:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 21:00:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 08:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 09:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 10:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 11:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:00:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_pct_Bin_91  close_pct_Bin_92  \\\n",
      "2017-09-25 13:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 17:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 19:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 20:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 21:00:00         0.0  ...               0.0               0.0   \n",
      "...                         ...  ...               ...               ...   \n",
      "2024-12-18 08:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 09:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 10:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 11:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 16:00:00         1.0  ...               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_93  close_pct_Bin_94  close_pct_Bin_95  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 08:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 09:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 10:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 11:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_96  close_pct_Bin_97  close_pct_Bin_98  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 08:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 09:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 10:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 11:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_99  close_target  \n",
      "2017-09-25 13:00:00               0.0      1.000000  \n",
      "2017-09-25 17:00:00               0.0      1.000000  \n",
      "2017-09-25 19:00:00               0.0      1.000000  \n",
      "2017-09-25 20:00:00               0.0      1.000000  \n",
      "2017-09-25 21:00:00               0.0      1.000000  \n",
      "...                               ...           ...  \n",
      "2024-12-18 08:00:00               0.0      0.467394  \n",
      "2024-12-18 09:00:00               0.0      0.226233  \n",
      "2024-12-18 10:00:00               0.0      0.006791  \n",
      "2024-12-18 11:00:00               0.0      0.000000  \n",
      "2024-12-18 16:00:00               1.0      0.109052  \n",
      "\n",
      "[57286 rows x 809 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 10 for fine-tuning.\n",
      "Experiment 11: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6925, Train Acc: 0.5216 | Val Loss: 0.6940, Val Acc: 0.5295\n",
      "Epoch 2/10 | Train Loss: 0.6928, Train Acc: 0.5243 | Val Loss: 0.6914, Val Acc: 0.5295\n",
      "Epoch 3/10 | Train Loss: 0.6923, Train Acc: 0.5243 | Val Loss: 0.6914, Val Acc: 0.5295\n",
      "Epoch 4/10 | Train Loss: 0.6926, Train Acc: 0.5235 | Val Loss: 0.6915, Val Acc: 0.5295\n",
      "Epoch 5/10 | Train Loss: 0.6922, Train Acc: 0.5239 | Val Loss: 0.6914, Val Acc: 0.5295\n",
      "Epoch 6/10 | Train Loss: 0.6926, Train Acc: 0.5209 | Val Loss: 0.6914, Val Acc: 0.5295\n",
      "Epoch 7/10 | Train Loss: 0.6921, Train Acc: 0.5225 | Val Loss: 0.6914, Val Acc: 0.5295\n",
      "Epoch 8/10 | Train Loss: 0.6925, Train Acc: 0.5210 | Val Loss: 0.6918, Val Acc: 0.5295\n",
      "Epoch 9/10 | Train Loss: 0.6924, Train Acc: 0.5259 | Val Loss: 0.6914, Val Acc: 0.5295\n",
      "Epoch 10/10 | Train Loss: 0.6921, Train Acc: 0.5237 | Val Loss: 0.6919, Val Acc: 0.5295\n",
      "Saved model for experiment 11.\n",
      "Experiment 11: Final Validation Accuracy: 0.5295\n",
      "Experiment 11: Test Accuracy: 0.5226\n",
      "Experiment 12: 데이터 구간 [30000:55000]\n",
      "                         open      high       low     close  open_Bin_0  \\\n",
      "2017-09-25 13:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 17:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 19:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 20:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "2017-09-25 21:00:00  1.000000  1.000000  1.000000  1.000000         0.0   \n",
      "...                       ...       ...       ...       ...         ...   \n",
      "2024-12-18 08:00:00  0.418090  0.421513  0.496974  0.467394         0.0   \n",
      "2024-12-18 09:00:00  0.470346  0.519246  0.298104  0.226233         0.0   \n",
      "2024-12-18 10:00:00  0.226700  0.344051  0.375151  0.006791         0.0   \n",
      "2024-12-18 11:00:00  0.008636  0.000000  0.322711  0.000000         1.0   \n",
      "2024-12-18 16:00:00  0.053565  0.041933  0.318664  0.109052         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 13:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 17:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 19:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 20:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 21:00:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 08:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 09:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 10:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 11:00:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:00:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_pct_Bin_91  close_pct_Bin_92  \\\n",
      "2017-09-25 13:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 17:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 19:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 20:00:00         0.0  ...               0.0               0.0   \n",
      "2017-09-25 21:00:00         0.0  ...               0.0               0.0   \n",
      "...                         ...  ...               ...               ...   \n",
      "2024-12-18 08:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 09:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 10:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 11:00:00         0.0  ...               0.0               0.0   \n",
      "2024-12-18 16:00:00         1.0  ...               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_93  close_pct_Bin_94  close_pct_Bin_95  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 08:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 09:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 10:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 11:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_96  close_pct_Bin_97  close_pct_Bin_98  \\\n",
      "2017-09-25 13:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 17:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 19:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 20:00:00               0.0               0.0               0.0   \n",
      "2017-09-25 21:00:00               0.0               0.0               0.0   \n",
      "...                               ...               ...               ...   \n",
      "2024-12-18 08:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 09:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 10:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 11:00:00               0.0               0.0               0.0   \n",
      "2024-12-18 16:00:00               0.0               0.0               0.0   \n",
      "\n",
      "                     close_pct_Bin_99  close_target  \n",
      "2017-09-25 13:00:00               0.0      1.000000  \n",
      "2017-09-25 17:00:00               0.0      1.000000  \n",
      "2017-09-25 19:00:00               0.0      1.000000  \n",
      "2017-09-25 20:00:00               0.0      1.000000  \n",
      "2017-09-25 21:00:00               0.0      1.000000  \n",
      "...                               ...           ...  \n",
      "2024-12-18 08:00:00               0.0      0.467394  \n",
      "2024-12-18 09:00:00               0.0      0.226233  \n",
      "2024-12-18 10:00:00               0.0      0.006791  \n",
      "2024-12-18 11:00:00               0.0      0.000000  \n",
      "2024-12-18 16:00:00               1.0      0.109052  \n",
      "\n",
      "[57286 rows x 809 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from experiment 11 for fine-tuning.\n",
      "Experiment 12: Training with lr=0.0001 (Fine-Tuning)\n",
      "Epoch 1/10 | Train Loss: 0.6923, Train Acc: 0.5195 | Val Loss: 0.6935, Val Acc: 0.5226\n",
      "Epoch 2/10 | Train Loss: 0.6923, Train Acc: 0.5260 | Val Loss: 0.6933, Val Acc: 0.5226\n",
      "Epoch 3/10 | Train Loss: 0.6923, Train Acc: 0.5227 | Val Loss: 0.6929, Val Acc: 0.5226\n",
      "Epoch 4/10 | Train Loss: 0.6923, Train Acc: 0.5219 | Val Loss: 0.6922, Val Acc: 0.5226\n",
      "Epoch 5/10 | Train Loss: 0.6924, Train Acc: 0.5244 | Val Loss: 0.6948, Val Acc: 0.5226\n",
      "Epoch 6/10 | Train Loss: 0.6923, Train Acc: 0.5231 | Val Loss: 0.6927, Val Acc: 0.5226\n",
      "Epoch 7/10 | Train Loss: 0.6921, Train Acc: 0.5244 | Val Loss: 0.6930, Val Acc: 0.5226\n",
      "Epoch 8/10 | Train Loss: 0.6918, Train Acc: 0.5279 | Val Loss: 0.6922, Val Acc: 0.5226\n",
      "Epoch 9/10 | Train Loss: 0.6919, Train Acc: 0.5265 | Val Loss: 0.6924, Val Acc: 0.5226\n",
      "Epoch 10/10 | Train Loss: 0.6921, Train Acc: 0.5261 | Val Loss: 0.6923, Val Acc: 0.5226\n",
      "Saved model for experiment 12.\n",
      "Experiment 12: Final Validation Accuracy: 0.5226\n",
      "Experiment 12: Test Accuracy: 0.5182\n",
      "\n",
      "Final Average Validation Accuracy: 0.5321\n",
      "Final Average Test Accuracy: 0.5293\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from math import sqrt\n",
    "import math\n",
    "\n",
    "# 동일한 rolling_minmax_scale 및 bin_and_encode 함수 사용 (위와 동일)\n",
    "def rolling_minmax_scale(series, window=24):\n",
    "    roll_min = series.rolling(window=window, min_periods=window).min()\n",
    "    roll_max = series.rolling(window=window, min_periods=window).max()\n",
    "    scaled = (series - roll_min) / ((roll_max - roll_min) + 1e-8)\n",
    "    scaled = scaled.replace([np.inf, -np.inf], np.nan)\n",
    "    scaled = scaled.fillna(1.0)\n",
    "    return scaled.clip(upper=1.0)\n",
    "\n",
    "def bin_and_encode(data, features, bins=100, drop_original=True):\n",
    "    for feature in features:\n",
    "        data[f'{feature}_Bin'] = pd.cut(data[feature], bins=bins, labels=False)\n",
    "        one_hot = pd.get_dummies(data[f'{feature}_Bin'], prefix=f'{feature}_Bin').astype(np.int32)\n",
    "        expected_columns = [f'{feature}_Bin_{i}' for i in range(bins)]\n",
    "        one_hot = one_hot.reindex(columns=expected_columns, fill_value=0)\n",
    "        data = pd.concat([data, one_hot], axis=1)\n",
    "        if drop_original:\n",
    "            data.drop(columns=[f'{feature}_Bin'], inplace=True)\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    for col in numeric_cols:\n",
    "        data[col] = data[col].astype(np.float32)\n",
    "    return data\n",
    "\n",
    "####################################\n",
    "# Dataset 및 모델 정의는 Variant 1와 동일\n",
    "####################################\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, input_data, target_data, lookback=24):\n",
    "        self.input_data = input_data.values\n",
    "        self.target_data = target_data.values\n",
    "        self.lookback = lookback\n",
    "    def __len__(self):\n",
    "        return len(self.input_data) - self.lookback\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.input_data[idx: idx + self.lookback, :]\n",
    "        y = self.target_data[idx + self.lookback, 0]\n",
    "        y_prev = self.target_data[idx + self.lookback - 1, 0]\n",
    "        y_target = 1 if y > y_prev else 0\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y_target, dtype=torch.long)\n",
    "\n",
    "class EncoderOnlyTransformerCustom(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=512, num_layers=6, nhead=8, ffn_dim=2048, num_classes=2, max_seq_len=24):\n",
    "        super(EncoderOnlyTransformerCustom, self).__init__()\n",
    "        self.token_embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embedding_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=nhead, dim_feedforward=ffn_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x = self.token_embedding(x)\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        pos_emb = self.position_embedding(positions)\n",
    "        x = x + pos_emb\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.transformer_encoder(x)\n",
    "        return self.fc(x[-1, :, :])\n",
    "\n",
    "####################################\n",
    "# Variant 2 실행: OHLC pct_change 기반\n",
    "####################################\n",
    "# CSV 파일 로드 – OHLC 데이터만 사용\n",
    "data = pd.read_csv(\"BTC_upbit_KRW_min60.csv\", index_col=0)\n",
    "data = data[['open','high','low','close']]\n",
    "data.index = pd.to_datetime(data.index)\n",
    "\n",
    "# 계산: 각 feature에 대해 pct_change * 100\n",
    "ohlc_features = ['open','high','low','close']\n",
    "# 각 OHLC 열에 대해 rolling minmax scaling (window=24)\n",
    "for feature in ohlc_features:\n",
    "    data[feature] = rolling_minmax_scale(data[feature], window=24)\n",
    "\n",
    "# One-hot 인코딩: 각 OHLC 열은 100구간으로 나눔\n",
    "data = bin_and_encode(data, ohlc_features, bins=100, drop_original=True)\n",
    "\n",
    "for feature in ohlc_features:\n",
    "    data[feature + '_pct'] = data[feature].pct_change() * 100\n",
    "# 클립: (-25, 25) 구간으로\n",
    "for feature in ohlc_features:\n",
    "    col = feature + '_pct'\n",
    "    data[col] = data[col].clip(lower=-25, upper=25)\n",
    "    \n",
    "# one-hot 인코딩: pd.cut의 bins 인자는 np.linspace(-25,25,101)를 사용하여 100구간 생성\n",
    "features_pct = [f + '_pct' for f in ohlc_features]\n",
    "data = data.dropna()  # pct_change로 인해 생긴 결측 제거\n",
    "data = bin_and_encode(data, features_pct, bins=100, drop_original=True)\n",
    "\n",
    "# 타깃은 원본 close 값 사용 (원하는 경우 shift할 수도 있음)\n",
    "data['close_target'] = data['close']\n",
    "data = data.dropna()\n",
    "\n",
    "# 최종 입력: _Bin_ 열들만 사용\n",
    "final_input_columns = [col for col in data.columns if '_Bin_' in col]\n",
    "final_target_column = ['close_target']\n",
    "\n",
    "data_input = data[final_input_columns]\n",
    "data_target = data[final_target_column]\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return total_loss / len(data_loader), correct / total\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, lr, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_state = None\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, device)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = model.state_dict()\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(data, num_experiments=16, lookback=24, num_epochs=10):\n",
    "    # 최종 입력: 모든 _Bin_ 컬럼 + datetime one-hot + positional encoding + order 인코딩 컬럼\n",
    "    final_input_columns = [col for col in data.columns if '_Bin_' in col]\n",
    "    # datetime_onehot_features = [col for col in data.columns if col.startswith('Hour_') or \n",
    "    #                               col.startswith('Day_') or col.startswith('Week_') or \n",
    "    #                               col.startswith('Month_')]\n",
    "    # datetime_positional_features = ['hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'week_sin', 'week_cos', 'month_sin', 'month_cos']\n",
    "    # order_encoding_features = [col for col in data.columns if '_order_' in col]\n",
    "    # final_input_columns.extend(datetime_onehot_features)\n",
    "    # final_input_columns.extend(datetime_positional_features)\n",
    "    # final_input_columns.extend(order_encoding_features)\n",
    "    target_cols = ['close_target']\n",
    "    \n",
    "    data_input = data[final_input_columns]\n",
    "    data_target = data[target_cols]\n",
    "    \n",
    "    data_input = data_input.apply(pd.to_numeric).astype(np.float32)\n",
    "    data_target = data_target.apply(pd.to_numeric).astype(np.float32)\n",
    "    \n",
    "    step_size = 2500\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    val_acc_list = []\n",
    "    test_acc_list = []\n",
    "    \n",
    "    for exp in range(num_experiments):\n",
    "        train_start = exp * step_size\n",
    "        train_end = train_start + step_size * 8\n",
    "        val_end = train_end + step_size\n",
    "        test_end = val_end + step_size\n",
    "        if test_end > len(data_input):\n",
    "            break\n",
    "        print(f\"Experiment {exp}: 데이터 구간 [{train_start}:{test_end}]\")\n",
    "        print(data)\n",
    "        data.to_csv('AAPL_one_hot.csv')\n",
    "        \n",
    "        train_input = data_input.iloc[train_start:train_end]\n",
    "        train_target = data_target.iloc[train_start:train_end]\n",
    "        val_input = data_input.iloc[train_end:val_end]\n",
    "        val_target = data_target.iloc[train_end:val_end]\n",
    "        test_input = data_input.iloc[val_end:test_end]\n",
    "        test_target = data_target.iloc[val_end:test_end]\n",
    "        \n",
    "        train_dataset = TimeSeriesDataset(train_input, train_target, lookback=lookback)\n",
    "        val_dataset = TimeSeriesDataset(val_input, val_target, lookback=lookback)\n",
    "        test_dataset = TimeSeriesDataset(test_input, test_target, lookback=lookback)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        lr = 1e-4\n",
    "        input_dim = data_input.shape[1]\n",
    "        model = EncoderOnlyTransformerCustom(input_dim=input_dim, embedding_dim=512, num_layers=6, nhead=8,\n",
    "                                               ffn_dim=2048, num_classes=2, max_seq_len=lookback).to(device)\n",
    "        model_path = f\"model_experiment_{exp}.pth\"\n",
    "        if exp > 0:\n",
    "            try:\n",
    "                model.load_state_dict(torch.load(f\"model_experiment_{exp - 1}.pth\"))\n",
    "                print(f\"Loaded model from experiment {exp - 1} for fine-tuning.\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Model file for experiment {exp - 1} not found. Starting fresh training.\")\n",
    "        \n",
    "        print(f\"Experiment {exp}: Training with lr={lr} (Fine-Tuning)\")\n",
    "        model = train_model(model, train_loader, val_loader, num_epochs, lr, device)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Saved model for experiment {exp}.\")\n",
    "        \n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, device)\n",
    "        val_acc_list.append(val_acc)\n",
    "        print(f\"Experiment {exp}: Final Validation Accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "        test_loss, test_acc = evaluate_model(model, test_loader, device)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(f\"Experiment {exp}: Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    if val_acc_list:\n",
    "        avg_val_acc = sum(val_acc_list) / len(val_acc_list)\n",
    "        avg_test_acc = sum(test_acc_list) / len(test_acc_list)\n",
    "        print(f\"\\nFinal Average Validation Accuracy: {avg_val_acc:.4f}\")\n",
    "        print(f\"Final Average Test Accuracy: {avg_test_acc:.4f}\")\n",
    "    else:\n",
    "        print(\"실험이 한 번도 실행되지 않았습니다.\")\n",
    "\n",
    "# 참조용 리스트 (최종 구성에는 _Bin_ 컬럼들이 포함됨)\n",
    "# features_pct = [f + '_pct' for f in features_to_bin]\n",
    "# features_minmax = [f + '_minmax' for f in features_to_bin]\n",
    "\n",
    "train_and_evaluate(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26012\\2793618170.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['close_target'] = data['close']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 0: 데이터 구간 [0:250000]\n",
      "                         open      high       low  close  open_Bin_0  \\\n",
      "2017-09-25 12:00:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:05:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:10:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:15:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:20:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "...                       ...       ...       ...    ...         ...   \n",
      "2024-12-18 16:05:00  0.200000  0.500000  0.462963   0.54         0.0   \n",
      "2024-12-18 16:10:00  0.490909  0.685185  0.648148   0.50         0.0   \n",
      "2024-12-18 16:15:00  0.472727  0.518519  0.759259   0.58         0.0   \n",
      "2024-12-18 16:20:00  0.527273  0.648148  0.814815   0.64         0.0   \n",
      "2024-12-18 16:25:00  0.581818  0.611111  0.870370   0.70         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:05:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:10:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:15:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:20:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 16:05:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:10:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:15:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:20:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:25:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_Bin_91  close_Bin_92  \\\n",
      "2017-09-25 12:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:05:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:10:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:15:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:20:00         0.0  ...           0.0           0.0   \n",
      "...                         ...  ...           ...           ...   \n",
      "2024-12-18 16:05:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:10:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:15:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:20:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:25:00         0.0  ...           0.0           0.0   \n",
      "\n",
      "                     close_Bin_93  close_Bin_94  close_Bin_95  close_Bin_96  \\\n",
      "2017-09-25 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:05:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:10:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:15:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:20:00           0.0           0.0           0.0           0.0   \n",
      "...                           ...           ...           ...           ...   \n",
      "2024-12-18 16:05:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:10:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:15:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:20:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:25:00           0.0           0.0           0.0           0.0   \n",
      "\n",
      "                     close_Bin_97  close_Bin_98  close_Bin_99  close_target  \n",
      "2017-09-25 12:00:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:05:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:10:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:15:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:20:00           0.0           0.0           1.0          1.00  \n",
      "...                           ...           ...           ...           ...  \n",
      "2024-12-18 16:05:00           0.0           0.0           0.0          0.54  \n",
      "2024-12-18 16:10:00           0.0           0.0           0.0          0.50  \n",
      "2024-12-18 16:15:00           0.0           0.0           0.0          0.58  \n",
      "2024-12-18 16:20:00           0.0           0.0           0.0          0.64  \n",
      "2024-12-18 16:25:00           0.0           0.0           0.0          0.70  \n",
      "\n",
      "[754915 rows x 405 columns]\n",
      "Experiment 0: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.6845, Train Acc: 0.5763 | Val Loss: 0.6772, Val Acc: 0.5897\n",
      "Epoch 2/10 | Train Loss: 0.6811, Train Acc: 0.5810 | Val Loss: 0.6777, Val Acc: 0.5897\n",
      "Epoch 3/10 | Train Loss: 0.6807, Train Acc: 0.5810 | Val Loss: 0.6771, Val Acc: 0.5897\n",
      "Epoch 4/10 | Train Loss: 0.6809, Train Acc: 0.5810 | Val Loss: 0.6816, Val Acc: 0.5897\n",
      "Epoch 5/10 | Train Loss: 0.6809, Train Acc: 0.5811 | Val Loss: 0.6773, Val Acc: 0.5897\n",
      "Epoch 6/10 | Train Loss: 0.6808, Train Acc: 0.5811 | Val Loss: 0.6770, Val Acc: 0.5897\n",
      "Epoch 7/10 | Train Loss: 0.6807, Train Acc: 0.5810 | Val Loss: 0.6771, Val Acc: 0.5897\n",
      "Epoch 8/10 | Train Loss: 0.6808, Train Acc: 0.5811 | Val Loss: 0.6771, Val Acc: 0.5897\n",
      "Epoch 9/10 | Train Loss: 0.6807, Train Acc: 0.5811 | Val Loss: 0.6770, Val Acc: 0.5897\n",
      "Epoch 10/10 | Train Loss: 0.6805, Train Acc: 0.5811 | Val Loss: 0.6773, Val Acc: 0.5897\n",
      "Saved model for experiment 0.\n",
      "Experiment 0: Final Validation Accuracy: 0.5897\n",
      "Experiment 0: Test Accuracy: 0.5949\n",
      "Experiment 1: 데이터 구간 [25000:275000]\n",
      "                         open      high       low  close  open_Bin_0  \\\n",
      "2017-09-25 12:00:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:05:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:10:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:15:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:20:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "...                       ...       ...       ...    ...         ...   \n",
      "2024-12-18 16:05:00  0.200000  0.500000  0.462963   0.54         0.0   \n",
      "2024-12-18 16:10:00  0.490909  0.685185  0.648148   0.50         0.0   \n",
      "2024-12-18 16:15:00  0.472727  0.518519  0.759259   0.58         0.0   \n",
      "2024-12-18 16:20:00  0.527273  0.648148  0.814815   0.64         0.0   \n",
      "2024-12-18 16:25:00  0.581818  0.611111  0.870370   0.70         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:05:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:10:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:15:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:20:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 16:05:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:10:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:15:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:20:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:25:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_Bin_91  close_Bin_92  \\\n",
      "2017-09-25 12:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:05:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:10:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:15:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:20:00         0.0  ...           0.0           0.0   \n",
      "...                         ...  ...           ...           ...   \n",
      "2024-12-18 16:05:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:10:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:15:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:20:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:25:00         0.0  ...           0.0           0.0   \n",
      "\n",
      "                     close_Bin_93  close_Bin_94  close_Bin_95  close_Bin_96  \\\n",
      "2017-09-25 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:05:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:10:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:15:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:20:00           0.0           0.0           0.0           0.0   \n",
      "...                           ...           ...           ...           ...   \n",
      "2024-12-18 16:05:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:10:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:15:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:20:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:25:00           0.0           0.0           0.0           0.0   \n",
      "\n",
      "                     close_Bin_97  close_Bin_98  close_Bin_99  close_target  \n",
      "2017-09-25 12:00:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:05:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:10:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:15:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:20:00           0.0           0.0           1.0          1.00  \n",
      "...                           ...           ...           ...           ...  \n",
      "2024-12-18 16:05:00           0.0           0.0           0.0          0.54  \n",
      "2024-12-18 16:10:00           0.0           0.0           0.0          0.50  \n",
      "2024-12-18 16:15:00           0.0           0.0           0.0          0.58  \n",
      "2024-12-18 16:20:00           0.0           0.0           0.0          0.64  \n",
      "2024-12-18 16:25:00           0.0           0.0           0.0          0.70  \n",
      "\n",
      "[754915 rows x 405 columns]\n",
      "Loaded model from experiment 0 for fine-tuning.\n",
      "Experiment 1: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.6807, Train Acc: 0.5810 | Val Loss: 0.6760, Val Acc: 0.5949\n",
      "Epoch 2/10 | Train Loss: 0.6807, Train Acc: 0.5810 | Val Loss: 0.6755, Val Acc: 0.5949\n",
      "Epoch 3/10 | Train Loss: 0.6806, Train Acc: 0.5810 | Val Loss: 0.6755, Val Acc: 0.5949\n",
      "Epoch 4/10 | Train Loss: 0.6806, Train Acc: 0.5810 | Val Loss: 0.6750, Val Acc: 0.5949\n",
      "Epoch 5/10 | Train Loss: 0.6806, Train Acc: 0.5810 | Val Loss: 0.6751, Val Acc: 0.5949\n",
      "Epoch 6/10 | Train Loss: 0.6806, Train Acc: 0.5810 | Val Loss: 0.6750, Val Acc: 0.5949\n",
      "Epoch 7/10 | Train Loss: 0.6806, Train Acc: 0.5810 | Val Loss: 0.6757, Val Acc: 0.5949\n",
      "Epoch 8/10 | Train Loss: 0.6803, Train Acc: 0.5810 | Val Loss: 0.6751, Val Acc: 0.5949\n",
      "Epoch 9/10 | Train Loss: 0.6802, Train Acc: 0.5810 | Val Loss: 0.6760, Val Acc: 0.5949\n",
      "Epoch 10/10 | Train Loss: 0.6803, Train Acc: 0.5810 | Val Loss: 0.6755, Val Acc: 0.5949\n",
      "Saved model for experiment 1.\n",
      "Experiment 1: Final Validation Accuracy: 0.5949\n",
      "Experiment 1: Test Accuracy: 0.5710\n",
      "Experiment 2: 데이터 구간 [50000:300000]\n",
      "                         open      high       low  close  open_Bin_0  \\\n",
      "2017-09-25 12:00:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:05:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:10:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:15:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:20:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "...                       ...       ...       ...    ...         ...   \n",
      "2024-12-18 16:05:00  0.200000  0.500000  0.462963   0.54         0.0   \n",
      "2024-12-18 16:10:00  0.490909  0.685185  0.648148   0.50         0.0   \n",
      "2024-12-18 16:15:00  0.472727  0.518519  0.759259   0.58         0.0   \n",
      "2024-12-18 16:20:00  0.527273  0.648148  0.814815   0.64         0.0   \n",
      "2024-12-18 16:25:00  0.581818  0.611111  0.870370   0.70         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:05:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:10:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:15:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:20:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 16:05:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:10:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:15:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:20:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:25:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_Bin_91  close_Bin_92  \\\n",
      "2017-09-25 12:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:05:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:10:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:15:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:20:00         0.0  ...           0.0           0.0   \n",
      "...                         ...  ...           ...           ...   \n",
      "2024-12-18 16:05:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:10:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:15:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:20:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:25:00         0.0  ...           0.0           0.0   \n",
      "\n",
      "                     close_Bin_93  close_Bin_94  close_Bin_95  close_Bin_96  \\\n",
      "2017-09-25 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:05:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:10:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:15:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:20:00           0.0           0.0           0.0           0.0   \n",
      "...                           ...           ...           ...           ...   \n",
      "2024-12-18 16:05:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:10:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:15:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:20:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:25:00           0.0           0.0           0.0           0.0   \n",
      "\n",
      "                     close_Bin_97  close_Bin_98  close_Bin_99  close_target  \n",
      "2017-09-25 12:00:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:05:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:10:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:15:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:20:00           0.0           0.0           1.0          1.00  \n",
      "...                           ...           ...           ...           ...  \n",
      "2024-12-18 16:05:00           0.0           0.0           0.0          0.54  \n",
      "2024-12-18 16:10:00           0.0           0.0           0.0          0.50  \n",
      "2024-12-18 16:15:00           0.0           0.0           0.0          0.58  \n",
      "2024-12-18 16:20:00           0.0           0.0           0.0          0.64  \n",
      "2024-12-18 16:25:00           0.0           0.0           0.0          0.70  \n",
      "\n",
      "[754915 rows x 405 columns]\n",
      "Loaded model from experiment 1 for fine-tuning.\n",
      "Experiment 2: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.6793, Train Acc: 0.5845 | Val Loss: 0.6849, Val Acc: 0.5710\n",
      "Epoch 2/10 | Train Loss: 0.6794, Train Acc: 0.5845 | Val Loss: 0.6864, Val Acc: 0.5710\n",
      "Epoch 3/10 | Train Loss: 0.6793, Train Acc: 0.5845 | Val Loss: 0.6844, Val Acc: 0.5710\n",
      "Epoch 4/10 | Train Loss: 0.6794, Train Acc: 0.5845 | Val Loss: 0.6856, Val Acc: 0.5710\n",
      "Epoch 5/10 | Train Loss: 0.6793, Train Acc: 0.5845 | Val Loss: 0.6838, Val Acc: 0.5710\n",
      "Epoch 6/10 | Train Loss: 0.6793, Train Acc: 0.5845 | Val Loss: 0.6838, Val Acc: 0.5710\n",
      "Epoch 7/10 | Train Loss: 0.6792, Train Acc: 0.5845 | Val Loss: 0.6840, Val Acc: 0.5710\n",
      "Epoch 8/10 | Train Loss: 0.6793, Train Acc: 0.5845 | Val Loss: 0.6846, Val Acc: 0.5710\n",
      "Epoch 9/10 | Train Loss: 0.6791, Train Acc: 0.5845 | Val Loss: 0.6862, Val Acc: 0.5710\n",
      "Epoch 10/10 | Train Loss: 0.6791, Train Acc: 0.5845 | Val Loss: 0.6837, Val Acc: 0.5710\n",
      "Saved model for experiment 2.\n",
      "Experiment 2: Final Validation Accuracy: 0.5710\n",
      "Experiment 2: Test Accuracy: 0.5825\n",
      "Experiment 3: 데이터 구간 [75000:325000]\n",
      "                         open      high       low  close  open_Bin_0  \\\n",
      "2017-09-25 12:00:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:05:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:10:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:15:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:20:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "...                       ...       ...       ...    ...         ...   \n",
      "2024-12-18 16:05:00  0.200000  0.500000  0.462963   0.54         0.0   \n",
      "2024-12-18 16:10:00  0.490909  0.685185  0.648148   0.50         0.0   \n",
      "2024-12-18 16:15:00  0.472727  0.518519  0.759259   0.58         0.0   \n",
      "2024-12-18 16:20:00  0.527273  0.648148  0.814815   0.64         0.0   \n",
      "2024-12-18 16:25:00  0.581818  0.611111  0.870370   0.70         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:05:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:10:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:15:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:20:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 16:05:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:10:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:15:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:20:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:25:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_Bin_91  close_Bin_92  \\\n",
      "2017-09-25 12:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:05:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:10:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:15:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:20:00         0.0  ...           0.0           0.0   \n",
      "...                         ...  ...           ...           ...   \n",
      "2024-12-18 16:05:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:10:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:15:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:20:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:25:00         0.0  ...           0.0           0.0   \n",
      "\n",
      "                     close_Bin_93  close_Bin_94  close_Bin_95  close_Bin_96  \\\n",
      "2017-09-25 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:05:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:10:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:15:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:20:00           0.0           0.0           0.0           0.0   \n",
      "...                           ...           ...           ...           ...   \n",
      "2024-12-18 16:05:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:10:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:15:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:20:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:25:00           0.0           0.0           0.0           0.0   \n",
      "\n",
      "                     close_Bin_97  close_Bin_98  close_Bin_99  close_target  \n",
      "2017-09-25 12:00:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:05:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:10:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:15:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:20:00           0.0           0.0           1.0          1.00  \n",
      "...                           ...           ...           ...           ...  \n",
      "2024-12-18 16:05:00           0.0           0.0           0.0          0.54  \n",
      "2024-12-18 16:10:00           0.0           0.0           0.0          0.50  \n",
      "2024-12-18 16:15:00           0.0           0.0           0.0          0.58  \n",
      "2024-12-18 16:20:00           0.0           0.0           0.0          0.64  \n",
      "2024-12-18 16:25:00           0.0           0.0           0.0          0.70  \n",
      "\n",
      "[754915 rows x 405 columns]\n",
      "Loaded model from experiment 2 for fine-tuning.\n",
      "Experiment 3: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.6791, Train Acc: 0.5847 | Val Loss: 0.6795, Val Acc: 0.5825\n",
      "Epoch 2/10 | Train Loss: 0.6792, Train Acc: 0.5847 | Val Loss: 0.6795, Val Acc: 0.5825\n",
      "Epoch 3/10 | Train Loss: 0.6792, Train Acc: 0.5847 | Val Loss: 0.6798, Val Acc: 0.5825\n",
      "Epoch 4/10 | Train Loss: 0.6792, Train Acc: 0.5847 | Val Loss: 0.6795, Val Acc: 0.5825\n",
      "Epoch 5/10 | Train Loss: 0.6790, Train Acc: 0.5847 | Val Loss: 0.6800, Val Acc: 0.5825\n",
      "Epoch 6/10 | Train Loss: 0.6789, Train Acc: 0.5847 | Val Loss: 0.6795, Val Acc: 0.5825\n",
      "Epoch 7/10 | Train Loss: 0.6789, Train Acc: 0.5847 | Val Loss: 0.6797, Val Acc: 0.5825\n",
      "Epoch 8/10 | Train Loss: 0.6789, Train Acc: 0.5847 | Val Loss: 0.6796, Val Acc: 0.5825\n",
      "Epoch 9/10 | Train Loss: 0.6788, Train Acc: 0.5847 | Val Loss: 0.6795, Val Acc: 0.5825\n",
      "Epoch 10/10 | Train Loss: 0.6788, Train Acc: 0.5847 | Val Loss: 0.6795, Val Acc: 0.5825\n",
      "Saved model for experiment 3.\n",
      "Experiment 3: Final Validation Accuracy: 0.5825\n",
      "Experiment 3: Test Accuracy: 0.5723\n",
      "Experiment 4: 데이터 구간 [100000:350000]\n",
      "                         open      high       low  close  open_Bin_0  \\\n",
      "2017-09-25 12:00:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:05:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:10:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:15:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:20:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "...                       ...       ...       ...    ...         ...   \n",
      "2024-12-18 16:05:00  0.200000  0.500000  0.462963   0.54         0.0   \n",
      "2024-12-18 16:10:00  0.490909  0.685185  0.648148   0.50         0.0   \n",
      "2024-12-18 16:15:00  0.472727  0.518519  0.759259   0.58         0.0   \n",
      "2024-12-18 16:20:00  0.527273  0.648148  0.814815   0.64         0.0   \n",
      "2024-12-18 16:25:00  0.581818  0.611111  0.870370   0.70         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:05:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:10:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:15:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:20:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 16:05:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:10:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:15:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:20:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:25:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_Bin_91  close_Bin_92  \\\n",
      "2017-09-25 12:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:05:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:10:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:15:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:20:00         0.0  ...           0.0           0.0   \n",
      "...                         ...  ...           ...           ...   \n",
      "2024-12-18 16:05:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:10:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:15:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:20:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:25:00         0.0  ...           0.0           0.0   \n",
      "\n",
      "                     close_Bin_93  close_Bin_94  close_Bin_95  close_Bin_96  \\\n",
      "2017-09-25 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:05:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:10:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:15:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:20:00           0.0           0.0           0.0           0.0   \n",
      "...                           ...           ...           ...           ...   \n",
      "2024-12-18 16:05:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:10:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:15:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:20:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:25:00           0.0           0.0           0.0           0.0   \n",
      "\n",
      "                     close_Bin_97  close_Bin_98  close_Bin_99  close_target  \n",
      "2017-09-25 12:00:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:05:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:10:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:15:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:20:00           0.0           0.0           1.0          1.00  \n",
      "...                           ...           ...           ...           ...  \n",
      "2024-12-18 16:05:00           0.0           0.0           0.0          0.54  \n",
      "2024-12-18 16:10:00           0.0           0.0           0.0          0.50  \n",
      "2024-12-18 16:15:00           0.0           0.0           0.0          0.58  \n",
      "2024-12-18 16:20:00           0.0           0.0           0.0          0.64  \n",
      "2024-12-18 16:25:00           0.0           0.0           0.0          0.70  \n",
      "\n",
      "[754915 rows x 405 columns]\n",
      "Loaded model from experiment 3 for fine-tuning.\n",
      "Experiment 4: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.6788, Train Acc: 0.5853 | Val Loss: 0.6831, Val Acc: 0.5723\n",
      "Epoch 2/10 | Train Loss: 0.6789, Train Acc: 0.5853 | Val Loss: 0.6849, Val Acc: 0.5723\n",
      "Epoch 3/10 | Train Loss: 0.6789, Train Acc: 0.5853 | Val Loss: 0.6831, Val Acc: 0.5723\n",
      "Epoch 4/10 | Train Loss: 0.6789, Train Acc: 0.5853 | Val Loss: 0.6834, Val Acc: 0.5723\n",
      "Epoch 5/10 | Train Loss: 0.6788, Train Acc: 0.5853 | Val Loss: 0.6842, Val Acc: 0.5723\n",
      "Epoch 6/10 | Train Loss: 0.6788, Train Acc: 0.5853 | Val Loss: 0.6836, Val Acc: 0.5723\n",
      "Epoch 7/10 | Train Loss: 0.6787, Train Acc: 0.5853 | Val Loss: 0.6828, Val Acc: 0.5723\n",
      "Epoch 8/10 | Train Loss: 0.6787, Train Acc: 0.5853 | Val Loss: 0.6826, Val Acc: 0.5723\n",
      "Epoch 9/10 | Train Loss: 0.6788, Train Acc: 0.5853 | Val Loss: 0.6834, Val Acc: 0.5723\n",
      "Epoch 10/10 | Train Loss: 0.6787, Train Acc: 0.5853 | Val Loss: 0.6827, Val Acc: 0.5723\n",
      "Saved model for experiment 4.\n",
      "Experiment 4: Final Validation Accuracy: 0.5723\n",
      "Experiment 4: Test Accuracy: 0.5648\n",
      "Experiment 5: 데이터 구간 [125000:375000]\n",
      "                         open      high       low  close  open_Bin_0  \\\n",
      "2017-09-25 12:00:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:05:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:10:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:15:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:20:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "...                       ...       ...       ...    ...         ...   \n",
      "2024-12-18 16:05:00  0.200000  0.500000  0.462963   0.54         0.0   \n",
      "2024-12-18 16:10:00  0.490909  0.685185  0.648148   0.50         0.0   \n",
      "2024-12-18 16:15:00  0.472727  0.518519  0.759259   0.58         0.0   \n",
      "2024-12-18 16:20:00  0.527273  0.648148  0.814815   0.64         0.0   \n",
      "2024-12-18 16:25:00  0.581818  0.611111  0.870370   0.70         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:05:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:10:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:15:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:20:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 16:05:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:10:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:15:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:20:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:25:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_Bin_91  close_Bin_92  \\\n",
      "2017-09-25 12:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:05:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:10:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:15:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:20:00         0.0  ...           0.0           0.0   \n",
      "...                         ...  ...           ...           ...   \n",
      "2024-12-18 16:05:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:10:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:15:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:20:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:25:00         0.0  ...           0.0           0.0   \n",
      "\n",
      "                     close_Bin_93  close_Bin_94  close_Bin_95  close_Bin_96  \\\n",
      "2017-09-25 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:05:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:10:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:15:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:20:00           0.0           0.0           0.0           0.0   \n",
      "...                           ...           ...           ...           ...   \n",
      "2024-12-18 16:05:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:10:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:15:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:20:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:25:00           0.0           0.0           0.0           0.0   \n",
      "\n",
      "                     close_Bin_97  close_Bin_98  close_Bin_99  close_target  \n",
      "2017-09-25 12:00:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:05:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:10:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:15:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:20:00           0.0           0.0           1.0          1.00  \n",
      "...                           ...           ...           ...           ...  \n",
      "2024-12-18 16:05:00           0.0           0.0           0.0          0.54  \n",
      "2024-12-18 16:10:00           0.0           0.0           0.0          0.50  \n",
      "2024-12-18 16:15:00           0.0           0.0           0.0          0.58  \n",
      "2024-12-18 16:20:00           0.0           0.0           0.0          0.64  \n",
      "2024-12-18 16:25:00           0.0           0.0           0.0          0.70  \n",
      "\n",
      "[754915 rows x 405 columns]\n",
      "Loaded model from experiment 4 for fine-tuning.\n",
      "Experiment 5: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.6794, Train Acc: 0.5836 | Val Loss: 0.6854, Val Acc: 0.5648\n",
      "Epoch 2/10 | Train Loss: 0.6794, Train Acc: 0.5836 | Val Loss: 0.6862, Val Acc: 0.5648\n",
      "Epoch 3/10 | Train Loss: 0.6794, Train Acc: 0.5836 | Val Loss: 0.6854, Val Acc: 0.5648\n",
      "Epoch 4/10 | Train Loss: 0.6794, Train Acc: 0.5836 | Val Loss: 0.6856, Val Acc: 0.5648\n",
      "Epoch 5/10 | Train Loss: 0.6793, Train Acc: 0.5836 | Val Loss: 0.6854, Val Acc: 0.5648\n",
      "Epoch 6/10 | Train Loss: 0.6793, Train Acc: 0.5836 | Val Loss: 0.6856, Val Acc: 0.5648\n",
      "Epoch 7/10 | Train Loss: 0.6792, Train Acc: 0.5836 | Val Loss: 0.6855, Val Acc: 0.5648\n",
      "Epoch 8/10 | Train Loss: 0.6792, Train Acc: 0.5836 | Val Loss: 0.6855, Val Acc: 0.5648\n",
      "Epoch 9/10 | Train Loss: 0.6792, Train Acc: 0.5836 | Val Loss: 0.6860, Val Acc: 0.5648\n",
      "Epoch 10/10 | Train Loss: 0.6792, Train Acc: 0.5836 | Val Loss: 0.6863, Val Acc: 0.5648\n",
      "Saved model for experiment 5.\n",
      "Experiment 5: Final Validation Accuracy: 0.5648\n",
      "Experiment 5: Test Accuracy: 0.5797\n",
      "Experiment 6: 데이터 구간 [150000:400000]\n",
      "                         open      high       low  close  open_Bin_0  \\\n",
      "2017-09-25 12:00:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:05:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:10:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:15:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:20:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "...                       ...       ...       ...    ...         ...   \n",
      "2024-12-18 16:05:00  0.200000  0.500000  0.462963   0.54         0.0   \n",
      "2024-12-18 16:10:00  0.490909  0.685185  0.648148   0.50         0.0   \n",
      "2024-12-18 16:15:00  0.472727  0.518519  0.759259   0.58         0.0   \n",
      "2024-12-18 16:20:00  0.527273  0.648148  0.814815   0.64         0.0   \n",
      "2024-12-18 16:25:00  0.581818  0.611111  0.870370   0.70         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:05:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:10:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:15:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:20:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 16:05:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:10:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:15:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:20:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:25:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_Bin_91  close_Bin_92  \\\n",
      "2017-09-25 12:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:05:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:10:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:15:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:20:00         0.0  ...           0.0           0.0   \n",
      "...                         ...  ...           ...           ...   \n",
      "2024-12-18 16:05:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:10:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:15:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:20:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:25:00         0.0  ...           0.0           0.0   \n",
      "\n",
      "                     close_Bin_93  close_Bin_94  close_Bin_95  close_Bin_96  \\\n",
      "2017-09-25 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:05:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:10:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:15:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:20:00           0.0           0.0           0.0           0.0   \n",
      "...                           ...           ...           ...           ...   \n",
      "2024-12-18 16:05:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:10:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:15:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:20:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:25:00           0.0           0.0           0.0           0.0   \n",
      "\n",
      "                     close_Bin_97  close_Bin_98  close_Bin_99  close_target  \n",
      "2017-09-25 12:00:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:05:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:10:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:15:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:20:00           0.0           0.0           1.0          1.00  \n",
      "...                           ...           ...           ...           ...  \n",
      "2024-12-18 16:05:00           0.0           0.0           0.0          0.54  \n",
      "2024-12-18 16:10:00           0.0           0.0           0.0          0.50  \n",
      "2024-12-18 16:15:00           0.0           0.0           0.0          0.58  \n",
      "2024-12-18 16:20:00           0.0           0.0           0.0          0.64  \n",
      "2024-12-18 16:25:00           0.0           0.0           0.0          0.70  \n",
      "\n",
      "[754915 rows x 405 columns]\n",
      "Loaded model from experiment 5 for fine-tuning.\n",
      "Experiment 6: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.6804, Train Acc: 0.5804 | Val Loss: 0.6805, Val Acc: 0.5797\n",
      "Epoch 2/10 | Train Loss: 0.6804, Train Acc: 0.5804 | Val Loss: 0.6807, Val Acc: 0.5797\n",
      "Epoch 3/10 | Train Loss: 0.6804, Train Acc: 0.5804 | Val Loss: 0.6808, Val Acc: 0.5797\n",
      "Epoch 4/10 | Train Loss: 0.6805, Train Acc: 0.5804 | Val Loss: 0.6805, Val Acc: 0.5797\n",
      "Epoch 5/10 | Train Loss: 0.6803, Train Acc: 0.5804 | Val Loss: 0.6805, Val Acc: 0.5797\n",
      "Epoch 6/10 | Train Loss: 0.6803, Train Acc: 0.5804 | Val Loss: 0.6804, Val Acc: 0.5797\n",
      "Epoch 7/10 | Train Loss: 0.6803, Train Acc: 0.5804 | Val Loss: 0.6804, Val Acc: 0.5797\n",
      "Epoch 8/10 | Train Loss: 0.6803, Train Acc: 0.5804 | Val Loss: 0.6807, Val Acc: 0.5797\n",
      "Epoch 9/10 | Train Loss: 0.6803, Train Acc: 0.5804 | Val Loss: 0.6804, Val Acc: 0.5797\n",
      "Epoch 10/10 | Train Loss: 0.6803, Train Acc: 0.5804 | Val Loss: 0.6805, Val Acc: 0.5797\n",
      "Saved model for experiment 6.\n",
      "Experiment 6: Final Validation Accuracy: 0.5797\n",
      "Experiment 6: Test Accuracy: 0.5705\n",
      "Experiment 7: 데이터 구간 [175000:425000]\n",
      "                         open      high       low  close  open_Bin_0  \\\n",
      "2017-09-25 12:00:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:05:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:10:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:15:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:20:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "...                       ...       ...       ...    ...         ...   \n",
      "2024-12-18 16:05:00  0.200000  0.500000  0.462963   0.54         0.0   \n",
      "2024-12-18 16:10:00  0.490909  0.685185  0.648148   0.50         0.0   \n",
      "2024-12-18 16:15:00  0.472727  0.518519  0.759259   0.58         0.0   \n",
      "2024-12-18 16:20:00  0.527273  0.648148  0.814815   0.64         0.0   \n",
      "2024-12-18 16:25:00  0.581818  0.611111  0.870370   0.70         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:05:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:10:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:15:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:20:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 16:05:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:10:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:15:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:20:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:25:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_Bin_91  close_Bin_92  \\\n",
      "2017-09-25 12:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:05:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:10:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:15:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:20:00         0.0  ...           0.0           0.0   \n",
      "...                         ...  ...           ...           ...   \n",
      "2024-12-18 16:05:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:10:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:15:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:20:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:25:00         0.0  ...           0.0           0.0   \n",
      "\n",
      "                     close_Bin_93  close_Bin_94  close_Bin_95  close_Bin_96  \\\n",
      "2017-09-25 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:05:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:10:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:15:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:20:00           0.0           0.0           0.0           0.0   \n",
      "...                           ...           ...           ...           ...   \n",
      "2024-12-18 16:05:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:10:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:15:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:20:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:25:00           0.0           0.0           0.0           0.0   \n",
      "\n",
      "                     close_Bin_97  close_Bin_98  close_Bin_99  close_target  \n",
      "2017-09-25 12:00:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:05:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:10:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:15:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:20:00           0.0           0.0           1.0          1.00  \n",
      "...                           ...           ...           ...           ...  \n",
      "2024-12-18 16:05:00           0.0           0.0           0.0          0.54  \n",
      "2024-12-18 16:10:00           0.0           0.0           0.0          0.50  \n",
      "2024-12-18 16:15:00           0.0           0.0           0.0          0.58  \n",
      "2024-12-18 16:20:00           0.0           0.0           0.0          0.64  \n",
      "2024-12-18 16:25:00           0.0           0.0           0.0          0.70  \n",
      "\n",
      "[754915 rows x 405 columns]\n",
      "Loaded model from experiment 6 for fine-tuning.\n",
      "Experiment 7: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.6808, Train Acc: 0.5794 | Val Loss: 0.6837, Val Acc: 0.5705\n",
      "Epoch 2/10 | Train Loss: 0.6807, Train Acc: 0.5794 | Val Loss: 0.6839, Val Acc: 0.5705\n",
      "Epoch 3/10 | Train Loss: 0.6807, Train Acc: 0.5794 | Val Loss: 0.6832, Val Acc: 0.5705\n",
      "Epoch 4/10 | Train Loss: 0.6807, Train Acc: 0.5794 | Val Loss: 0.6833, Val Acc: 0.5705\n",
      "Epoch 5/10 | Train Loss: 0.6808, Train Acc: 0.5794 | Val Loss: 0.6833, Val Acc: 0.5705\n",
      "Epoch 6/10 | Train Loss: 0.6807, Train Acc: 0.5794 | Val Loss: 0.6839, Val Acc: 0.5705\n",
      "Epoch 7/10 | Train Loss: 0.6807, Train Acc: 0.5794 | Val Loss: 0.6833, Val Acc: 0.5705\n",
      "Epoch 8/10 | Train Loss: 0.6806, Train Acc: 0.5794 | Val Loss: 0.6832, Val Acc: 0.5705\n",
      "Epoch 9/10 | Train Loss: 0.6806, Train Acc: 0.5794 | Val Loss: 0.6837, Val Acc: 0.5705\n",
      "Epoch 10/10 | Train Loss: 0.6806, Train Acc: 0.5794 | Val Loss: 0.6835, Val Acc: 0.5705\n",
      "Saved model for experiment 7.\n",
      "Experiment 7: Final Validation Accuracy: 0.5705\n",
      "Experiment 7: Test Accuracy: 0.5718\n",
      "Experiment 8: 데이터 구간 [200000:450000]\n",
      "                         open      high       low  close  open_Bin_0  \\\n",
      "2017-09-25 12:00:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:05:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:10:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:15:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:20:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "...                       ...       ...       ...    ...         ...   \n",
      "2024-12-18 16:05:00  0.200000  0.500000  0.462963   0.54         0.0   \n",
      "2024-12-18 16:10:00  0.490909  0.685185  0.648148   0.50         0.0   \n",
      "2024-12-18 16:15:00  0.472727  0.518519  0.759259   0.58         0.0   \n",
      "2024-12-18 16:20:00  0.527273  0.648148  0.814815   0.64         0.0   \n",
      "2024-12-18 16:25:00  0.581818  0.611111  0.870370   0.70         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:05:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:10:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:15:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:20:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 16:05:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:10:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:15:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:20:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:25:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_Bin_91  close_Bin_92  \\\n",
      "2017-09-25 12:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:05:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:10:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:15:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:20:00         0.0  ...           0.0           0.0   \n",
      "...                         ...  ...           ...           ...   \n",
      "2024-12-18 16:05:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:10:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:15:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:20:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:25:00         0.0  ...           0.0           0.0   \n",
      "\n",
      "                     close_Bin_93  close_Bin_94  close_Bin_95  close_Bin_96  \\\n",
      "2017-09-25 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:05:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:10:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:15:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:20:00           0.0           0.0           0.0           0.0   \n",
      "...                           ...           ...           ...           ...   \n",
      "2024-12-18 16:05:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:10:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:15:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:20:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:25:00           0.0           0.0           0.0           0.0   \n",
      "\n",
      "                     close_Bin_97  close_Bin_98  close_Bin_99  close_target  \n",
      "2017-09-25 12:00:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:05:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:10:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:15:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:20:00           0.0           0.0           1.0          1.00  \n",
      "...                           ...           ...           ...           ...  \n",
      "2024-12-18 16:05:00           0.0           0.0           0.0          0.54  \n",
      "2024-12-18 16:10:00           0.0           0.0           0.0          0.50  \n",
      "2024-12-18 16:15:00           0.0           0.0           0.0          0.58  \n",
      "2024-12-18 16:20:00           0.0           0.0           0.0          0.64  \n",
      "2024-12-18 16:25:00           0.0           0.0           0.0          0.70  \n",
      "\n",
      "[754915 rows x 405 columns]\n",
      "Loaded model from experiment 7 for fine-tuning.\n",
      "Experiment 8: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.6811, Train Acc: 0.5782 | Val Loss: 0.6833, Val Acc: 0.5718\n",
      "Epoch 2/10 | Train Loss: 0.6811, Train Acc: 0.5782 | Val Loss: 0.6830, Val Acc: 0.5718\n",
      "Epoch 3/10 | Train Loss: 0.6811, Train Acc: 0.5782 | Val Loss: 0.6829, Val Acc: 0.5718\n",
      "Epoch 4/10 | Train Loss: 0.6812, Train Acc: 0.5782 | Val Loss: 0.6829, Val Acc: 0.5718\n",
      "Epoch 5/10 | Train Loss: 0.6811, Train Acc: 0.5782 | Val Loss: 0.6829, Val Acc: 0.5718\n",
      "Epoch 6/10 | Train Loss: 0.6812, Train Acc: 0.5782 | Val Loss: 0.6832, Val Acc: 0.5718\n",
      "Epoch 7/10 | Train Loss: 0.6810, Train Acc: 0.5782 | Val Loss: 0.6831, Val Acc: 0.5718\n",
      "Epoch 8/10 | Train Loss: 0.6810, Train Acc: 0.5782 | Val Loss: 0.6830, Val Acc: 0.5718\n",
      "Epoch 9/10 | Train Loss: 0.6810, Train Acc: 0.5782 | Val Loss: 0.6828, Val Acc: 0.5718\n",
      "Epoch 10/10 | Train Loss: 0.6810, Train Acc: 0.5782 | Val Loss: 0.6829, Val Acc: 0.5718\n",
      "Saved model for experiment 8.\n",
      "Experiment 8: Final Validation Accuracy: 0.5718\n",
      "Experiment 8: Test Accuracy: 0.5741\n",
      "Experiment 9: 데이터 구간 [225000:475000]\n",
      "                         open      high       low  close  open_Bin_0  \\\n",
      "2017-09-25 12:00:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:05:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:10:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:15:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:20:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "...                       ...       ...       ...    ...         ...   \n",
      "2024-12-18 16:05:00  0.200000  0.500000  0.462963   0.54         0.0   \n",
      "2024-12-18 16:10:00  0.490909  0.685185  0.648148   0.50         0.0   \n",
      "2024-12-18 16:15:00  0.472727  0.518519  0.759259   0.58         0.0   \n",
      "2024-12-18 16:20:00  0.527273  0.648148  0.814815   0.64         0.0   \n",
      "2024-12-18 16:25:00  0.581818  0.611111  0.870370   0.70         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:05:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:10:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:15:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:20:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 16:05:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:10:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:15:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:20:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:25:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_Bin_91  close_Bin_92  \\\n",
      "2017-09-25 12:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:05:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:10:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:15:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:20:00         0.0  ...           0.0           0.0   \n",
      "...                         ...  ...           ...           ...   \n",
      "2024-12-18 16:05:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:10:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:15:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:20:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:25:00         0.0  ...           0.0           0.0   \n",
      "\n",
      "                     close_Bin_93  close_Bin_94  close_Bin_95  close_Bin_96  \\\n",
      "2017-09-25 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:05:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:10:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:15:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:20:00           0.0           0.0           0.0           0.0   \n",
      "...                           ...           ...           ...           ...   \n",
      "2024-12-18 16:05:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:10:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:15:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:20:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:25:00           0.0           0.0           0.0           0.0   \n",
      "\n",
      "                     close_Bin_97  close_Bin_98  close_Bin_99  close_target  \n",
      "2017-09-25 12:00:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:05:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:10:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:15:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:20:00           0.0           0.0           1.0          1.00  \n",
      "...                           ...           ...           ...           ...  \n",
      "2024-12-18 16:05:00           0.0           0.0           0.0          0.54  \n",
      "2024-12-18 16:10:00           0.0           0.0           0.0          0.50  \n",
      "2024-12-18 16:15:00           0.0           0.0           0.0          0.58  \n",
      "2024-12-18 16:20:00           0.0           0.0           0.0          0.64  \n",
      "2024-12-18 16:25:00           0.0           0.0           0.0          0.70  \n",
      "\n",
      "[754915 rows x 405 columns]\n",
      "Loaded model from experiment 8 for fine-tuning.\n",
      "Experiment 9: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.6818, Train Acc: 0.5759 | Val Loss: 0.6822, Val Acc: 0.5741\n",
      "Epoch 2/10 | Train Loss: 0.6817, Train Acc: 0.5759 | Val Loss: 0.6824, Val Acc: 0.5741\n",
      "Epoch 3/10 | Train Loss: 0.6818, Train Acc: 0.5759 | Val Loss: 0.6821, Val Acc: 0.5741\n",
      "Epoch 4/10 | Train Loss: 0.6817, Train Acc: 0.5759 | Val Loss: 0.6821, Val Acc: 0.5741\n",
      "Epoch 5/10 | Train Loss: 0.6818, Train Acc: 0.5759 | Val Loss: 0.6821, Val Acc: 0.5741\n",
      "Epoch 6/10 | Train Loss: 0.6818, Train Acc: 0.5759 | Val Loss: 0.6821, Val Acc: 0.5741\n",
      "Epoch 7/10 | Train Loss: 0.6816, Train Acc: 0.5759 | Val Loss: 0.6824, Val Acc: 0.5741\n",
      "Epoch 8/10 | Train Loss: 0.6817, Train Acc: 0.5759 | Val Loss: 0.6823, Val Acc: 0.5741\n",
      "Epoch 9/10 | Train Loss: 0.6817, Train Acc: 0.5759 | Val Loss: 0.6822, Val Acc: 0.5741\n",
      "Epoch 10/10 | Train Loss: 0.6817, Train Acc: 0.5759 | Val Loss: 0.6822, Val Acc: 0.5741\n",
      "Saved model for experiment 9.\n",
      "Experiment 9: Final Validation Accuracy: 0.5741\n",
      "Experiment 9: Test Accuracy: 0.5785\n",
      "Experiment 10: 데이터 구간 [250000:500000]\n",
      "                         open      high       low  close  open_Bin_0  \\\n",
      "2017-09-25 12:00:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:05:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:10:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:15:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:20:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "...                       ...       ...       ...    ...         ...   \n",
      "2024-12-18 16:05:00  0.200000  0.500000  0.462963   0.54         0.0   \n",
      "2024-12-18 16:10:00  0.490909  0.685185  0.648148   0.50         0.0   \n",
      "2024-12-18 16:15:00  0.472727  0.518519  0.759259   0.58         0.0   \n",
      "2024-12-18 16:20:00  0.527273  0.648148  0.814815   0.64         0.0   \n",
      "2024-12-18 16:25:00  0.581818  0.611111  0.870370   0.70         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:05:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:10:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:15:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:20:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 16:05:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:10:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:15:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:20:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:25:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_Bin_91  close_Bin_92  \\\n",
      "2017-09-25 12:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:05:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:10:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:15:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:20:00         0.0  ...           0.0           0.0   \n",
      "...                         ...  ...           ...           ...   \n",
      "2024-12-18 16:05:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:10:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:15:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:20:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:25:00         0.0  ...           0.0           0.0   \n",
      "\n",
      "                     close_Bin_93  close_Bin_94  close_Bin_95  close_Bin_96  \\\n",
      "2017-09-25 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:05:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:10:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:15:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:20:00           0.0           0.0           0.0           0.0   \n",
      "...                           ...           ...           ...           ...   \n",
      "2024-12-18 16:05:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:10:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:15:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:20:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:25:00           0.0           0.0           0.0           0.0   \n",
      "\n",
      "                     close_Bin_97  close_Bin_98  close_Bin_99  close_target  \n",
      "2017-09-25 12:00:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:05:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:10:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:15:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:20:00           0.0           0.0           1.0          1.00  \n",
      "...                           ...           ...           ...           ...  \n",
      "2024-12-18 16:05:00           0.0           0.0           0.0          0.54  \n",
      "2024-12-18 16:10:00           0.0           0.0           0.0          0.50  \n",
      "2024-12-18 16:15:00           0.0           0.0           0.0          0.58  \n",
      "2024-12-18 16:20:00           0.0           0.0           0.0          0.64  \n",
      "2024-12-18 16:25:00           0.0           0.0           0.0          0.70  \n",
      "\n",
      "[754915 rows x 405 columns]\n",
      "Loaded model from experiment 9 for fine-tuning.\n",
      "Experiment 10: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.6825, Train Acc: 0.5733 | Val Loss: 0.6812, Val Acc: 0.5785\n",
      "Epoch 2/10 | Train Loss: 0.6825, Train Acc: 0.5733 | Val Loss: 0.6814, Val Acc: 0.5785\n",
      "Epoch 3/10 | Train Loss: 0.6825, Train Acc: 0.5733 | Val Loss: 0.6808, Val Acc: 0.5785\n",
      "Epoch 4/10 | Train Loss: 0.6825, Train Acc: 0.5733 | Val Loss: 0.6808, Val Acc: 0.5785\n",
      "Epoch 5/10 | Train Loss: 0.6826, Train Acc: 0.5733 | Val Loss: 0.6809, Val Acc: 0.5785\n",
      "Epoch 6/10 | Train Loss: 0.6825, Train Acc: 0.5733 | Val Loss: 0.6813, Val Acc: 0.5785\n",
      "Epoch 7/10 | Train Loss: 0.6825, Train Acc: 0.5733 | Val Loss: 0.6810, Val Acc: 0.5785\n",
      "Epoch 8/10 | Train Loss: 0.6825, Train Acc: 0.5733 | Val Loss: 0.6809, Val Acc: 0.5785\n",
      "Epoch 9/10 | Train Loss: 0.6825, Train Acc: 0.5733 | Val Loss: 0.6811, Val Acc: 0.5785\n",
      "Epoch 10/10 | Train Loss: 0.6824, Train Acc: 0.5733 | Val Loss: 0.6808, Val Acc: 0.5785\n",
      "Saved model for experiment 10.\n",
      "Experiment 10: Final Validation Accuracy: 0.5785\n",
      "Experiment 10: Test Accuracy: 0.5819\n",
      "Experiment 11: 데이터 구간 [275000:525000]\n",
      "                         open      high       low  close  open_Bin_0  \\\n",
      "2017-09-25 12:00:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:05:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:10:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:15:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:20:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "...                       ...       ...       ...    ...         ...   \n",
      "2024-12-18 16:05:00  0.200000  0.500000  0.462963   0.54         0.0   \n",
      "2024-12-18 16:10:00  0.490909  0.685185  0.648148   0.50         0.0   \n",
      "2024-12-18 16:15:00  0.472727  0.518519  0.759259   0.58         0.0   \n",
      "2024-12-18 16:20:00  0.527273  0.648148  0.814815   0.64         0.0   \n",
      "2024-12-18 16:25:00  0.581818  0.611111  0.870370   0.70         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:05:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:10:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:15:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:20:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 16:05:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:10:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:15:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:20:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:25:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_Bin_91  close_Bin_92  \\\n",
      "2017-09-25 12:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:05:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:10:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:15:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:20:00         0.0  ...           0.0           0.0   \n",
      "...                         ...  ...           ...           ...   \n",
      "2024-12-18 16:05:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:10:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:15:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:20:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:25:00         0.0  ...           0.0           0.0   \n",
      "\n",
      "                     close_Bin_93  close_Bin_94  close_Bin_95  close_Bin_96  \\\n",
      "2017-09-25 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:05:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:10:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:15:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:20:00           0.0           0.0           0.0           0.0   \n",
      "...                           ...           ...           ...           ...   \n",
      "2024-12-18 16:05:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:10:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:15:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:20:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:25:00           0.0           0.0           0.0           0.0   \n",
      "\n",
      "                     close_Bin_97  close_Bin_98  close_Bin_99  close_target  \n",
      "2017-09-25 12:00:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:05:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:10:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:15:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:20:00           0.0           0.0           1.0          1.00  \n",
      "...                           ...           ...           ...           ...  \n",
      "2024-12-18 16:05:00           0.0           0.0           0.0          0.54  \n",
      "2024-12-18 16:10:00           0.0           0.0           0.0          0.50  \n",
      "2024-12-18 16:15:00           0.0           0.0           0.0          0.58  \n",
      "2024-12-18 16:20:00           0.0           0.0           0.0          0.64  \n",
      "2024-12-18 16:25:00           0.0           0.0           0.0          0.70  \n",
      "\n",
      "[754915 rows x 405 columns]\n",
      "Loaded model from experiment 10 for fine-tuning.\n",
      "Experiment 11: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.6823, Train Acc: 0.5743 | Val Loss: 0.6799, Val Acc: 0.5819\n",
      "Epoch 2/10 | Train Loss: 0.6823, Train Acc: 0.5743 | Val Loss: 0.6798, Val Acc: 0.5819\n",
      "Epoch 3/10 | Train Loss: 0.6823, Train Acc: 0.5743 | Val Loss: 0.6798, Val Acc: 0.5819\n",
      "Epoch 4/10 | Train Loss: 0.6822, Train Acc: 0.5743 | Val Loss: 0.6797, Val Acc: 0.5819\n",
      "Epoch 5/10 | Train Loss: 0.6822, Train Acc: 0.5743 | Val Loss: 0.6797, Val Acc: 0.5819\n",
      "Epoch 6/10 | Train Loss: 0.6823, Train Acc: 0.5743 | Val Loss: 0.6801, Val Acc: 0.5819\n",
      "Epoch 7/10 | Train Loss: 0.6822, Train Acc: 0.5743 | Val Loss: 0.6800, Val Acc: 0.5819\n",
      "Epoch 8/10 | Train Loss: 0.6822, Train Acc: 0.5743 | Val Loss: 0.6797, Val Acc: 0.5819\n",
      "Epoch 9/10 | Train Loss: 0.6822, Train Acc: 0.5743 | Val Loss: 0.6797, Val Acc: 0.5819\n",
      "Epoch 10/10 | Train Loss: 0.6822, Train Acc: 0.5743 | Val Loss: 0.6797, Val Acc: 0.5819\n",
      "Saved model for experiment 11.\n",
      "Experiment 11: Final Validation Accuracy: 0.5819\n",
      "Experiment 11: Test Accuracy: 0.5820\n",
      "Experiment 12: 데이터 구간 [300000:550000]\n",
      "                         open      high       low  close  open_Bin_0  \\\n",
      "2017-09-25 12:00:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:05:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:10:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:15:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:20:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "...                       ...       ...       ...    ...         ...   \n",
      "2024-12-18 16:05:00  0.200000  0.500000  0.462963   0.54         0.0   \n",
      "2024-12-18 16:10:00  0.490909  0.685185  0.648148   0.50         0.0   \n",
      "2024-12-18 16:15:00  0.472727  0.518519  0.759259   0.58         0.0   \n",
      "2024-12-18 16:20:00  0.527273  0.648148  0.814815   0.64         0.0   \n",
      "2024-12-18 16:25:00  0.581818  0.611111  0.870370   0.70         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:05:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:10:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:15:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:20:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 16:05:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:10:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:15:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:20:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:25:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_Bin_91  close_Bin_92  \\\n",
      "2017-09-25 12:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:05:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:10:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:15:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:20:00         0.0  ...           0.0           0.0   \n",
      "...                         ...  ...           ...           ...   \n",
      "2024-12-18 16:05:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:10:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:15:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:20:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:25:00         0.0  ...           0.0           0.0   \n",
      "\n",
      "                     close_Bin_93  close_Bin_94  close_Bin_95  close_Bin_96  \\\n",
      "2017-09-25 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:05:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:10:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:15:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:20:00           0.0           0.0           0.0           0.0   \n",
      "...                           ...           ...           ...           ...   \n",
      "2024-12-18 16:05:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:10:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:15:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:20:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:25:00           0.0           0.0           0.0           0.0   \n",
      "\n",
      "                     close_Bin_97  close_Bin_98  close_Bin_99  close_target  \n",
      "2017-09-25 12:00:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:05:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:10:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:15:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:20:00           0.0           0.0           1.0          1.00  \n",
      "...                           ...           ...           ...           ...  \n",
      "2024-12-18 16:05:00           0.0           0.0           0.0          0.54  \n",
      "2024-12-18 16:10:00           0.0           0.0           0.0          0.50  \n",
      "2024-12-18 16:15:00           0.0           0.0           0.0          0.58  \n",
      "2024-12-18 16:20:00           0.0           0.0           0.0          0.64  \n",
      "2024-12-18 16:25:00           0.0           0.0           0.0          0.70  \n",
      "\n",
      "[754915 rows x 405 columns]\n",
      "Loaded model from experiment 11 for fine-tuning.\n",
      "Experiment 12: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.6823, Train Acc: 0.5742 | Val Loss: 0.6805, Val Acc: 0.5820\n",
      "Epoch 2/10 | Train Loss: 0.6822, Train Acc: 0.5742 | Val Loss: 0.6797, Val Acc: 0.5820\n",
      "Epoch 3/10 | Train Loss: 0.6823, Train Acc: 0.5742 | Val Loss: 0.6797, Val Acc: 0.5820\n",
      "Epoch 4/10 | Train Loss: 0.6822, Train Acc: 0.5742 | Val Loss: 0.6797, Val Acc: 0.5820\n",
      "Epoch 5/10 | Train Loss: 0.6823, Train Acc: 0.5742 | Val Loss: 0.6800, Val Acc: 0.5820\n",
      "Epoch 6/10 | Train Loss: 0.6822, Train Acc: 0.5742 | Val Loss: 0.6798, Val Acc: 0.5820\n",
      "Epoch 7/10 | Train Loss: 0.6822, Train Acc: 0.5742 | Val Loss: 0.6797, Val Acc: 0.5820\n",
      "Epoch 8/10 | Train Loss: 0.6822, Train Acc: 0.5742 | Val Loss: 0.6798, Val Acc: 0.5820\n",
      "Epoch 9/10 | Train Loss: 0.6821, Train Acc: 0.5742 | Val Loss: 0.6797, Val Acc: 0.5820\n",
      "Epoch 10/10 | Train Loss: 0.6822, Train Acc: 0.5742 | Val Loss: 0.6797, Val Acc: 0.5820\n",
      "Saved model for experiment 12.\n",
      "Experiment 12: Final Validation Accuracy: 0.5820\n",
      "Experiment 12: Test Accuracy: 0.6021\n",
      "Experiment 13: 데이터 구간 [325000:575000]\n",
      "                         open      high       low  close  open_Bin_0  \\\n",
      "2017-09-25 12:00:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:05:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:10:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:15:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:20:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "...                       ...       ...       ...    ...         ...   \n",
      "2024-12-18 16:05:00  0.200000  0.500000  0.462963   0.54         0.0   \n",
      "2024-12-18 16:10:00  0.490909  0.685185  0.648148   0.50         0.0   \n",
      "2024-12-18 16:15:00  0.472727  0.518519  0.759259   0.58         0.0   \n",
      "2024-12-18 16:20:00  0.527273  0.648148  0.814815   0.64         0.0   \n",
      "2024-12-18 16:25:00  0.581818  0.611111  0.870370   0.70         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:05:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:10:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:15:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:20:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 16:05:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:10:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:15:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:20:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:25:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_Bin_91  close_Bin_92  \\\n",
      "2017-09-25 12:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:05:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:10:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:15:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:20:00         0.0  ...           0.0           0.0   \n",
      "...                         ...  ...           ...           ...   \n",
      "2024-12-18 16:05:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:10:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:15:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:20:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:25:00         0.0  ...           0.0           0.0   \n",
      "\n",
      "                     close_Bin_93  close_Bin_94  close_Bin_95  close_Bin_96  \\\n",
      "2017-09-25 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:05:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:10:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:15:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:20:00           0.0           0.0           0.0           0.0   \n",
      "...                           ...           ...           ...           ...   \n",
      "2024-12-18 16:05:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:10:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:15:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:20:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:25:00           0.0           0.0           0.0           0.0   \n",
      "\n",
      "                     close_Bin_97  close_Bin_98  close_Bin_99  close_target  \n",
      "2017-09-25 12:00:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:05:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:10:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:15:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:20:00           0.0           0.0           1.0          1.00  \n",
      "...                           ...           ...           ...           ...  \n",
      "2024-12-18 16:05:00           0.0           0.0           0.0          0.54  \n",
      "2024-12-18 16:10:00           0.0           0.0           0.0          0.50  \n",
      "2024-12-18 16:15:00           0.0           0.0           0.0          0.58  \n",
      "2024-12-18 16:20:00           0.0           0.0           0.0          0.64  \n",
      "2024-12-18 16:25:00           0.0           0.0           0.0          0.70  \n",
      "\n",
      "[754915 rows x 405 columns]\n",
      "Loaded model from experiment 12 for fine-tuning.\n",
      "Experiment 13: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.6818, Train Acc: 0.5754 | Val Loss: 0.6735, Val Acc: 0.6021\n",
      "Epoch 2/10 | Train Loss: 0.6818, Train Acc: 0.5754 | Val Loss: 0.6727, Val Acc: 0.6021\n",
      "Epoch 3/10 | Train Loss: 0.6819, Train Acc: 0.5754 | Val Loss: 0.6728, Val Acc: 0.6021\n",
      "Epoch 4/10 | Train Loss: 0.6819, Train Acc: 0.5754 | Val Loss: 0.6739, Val Acc: 0.6021\n",
      "Epoch 5/10 | Train Loss: 0.6819, Train Acc: 0.5754 | Val Loss: 0.6726, Val Acc: 0.6021\n",
      "Epoch 6/10 | Train Loss: 0.6818, Train Acc: 0.5754 | Val Loss: 0.6733, Val Acc: 0.6021\n",
      "Epoch 7/10 | Train Loss: 0.6819, Train Acc: 0.5754 | Val Loss: 0.6732, Val Acc: 0.6021\n",
      "Epoch 8/10 | Train Loss: 0.6819, Train Acc: 0.5754 | Val Loss: 0.6729, Val Acc: 0.6021\n",
      "Epoch 9/10 | Train Loss: 0.6819, Train Acc: 0.5754 | Val Loss: 0.6741, Val Acc: 0.6021\n",
      "Epoch 10/10 | Train Loss: 0.6818, Train Acc: 0.5754 | Val Loss: 0.6732, Val Acc: 0.6021\n",
      "Saved model for experiment 13.\n",
      "Experiment 13: Final Validation Accuracy: 0.6021\n",
      "Experiment 13: Test Accuracy: 0.6071\n",
      "Experiment 14: 데이터 구간 [350000:600000]\n",
      "                         open      high       low  close  open_Bin_0  \\\n",
      "2017-09-25 12:00:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:05:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:10:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:15:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:20:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "...                       ...       ...       ...    ...         ...   \n",
      "2024-12-18 16:05:00  0.200000  0.500000  0.462963   0.54         0.0   \n",
      "2024-12-18 16:10:00  0.490909  0.685185  0.648148   0.50         0.0   \n",
      "2024-12-18 16:15:00  0.472727  0.518519  0.759259   0.58         0.0   \n",
      "2024-12-18 16:20:00  0.527273  0.648148  0.814815   0.64         0.0   \n",
      "2024-12-18 16:25:00  0.581818  0.611111  0.870370   0.70         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:05:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:10:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:15:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:20:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 16:05:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:10:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:15:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:20:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:25:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_Bin_91  close_Bin_92  \\\n",
      "2017-09-25 12:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:05:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:10:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:15:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:20:00         0.0  ...           0.0           0.0   \n",
      "...                         ...  ...           ...           ...   \n",
      "2024-12-18 16:05:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:10:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:15:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:20:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:25:00         0.0  ...           0.0           0.0   \n",
      "\n",
      "                     close_Bin_93  close_Bin_94  close_Bin_95  close_Bin_96  \\\n",
      "2017-09-25 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:05:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:10:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:15:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:20:00           0.0           0.0           0.0           0.0   \n",
      "...                           ...           ...           ...           ...   \n",
      "2024-12-18 16:05:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:10:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:15:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:20:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:25:00           0.0           0.0           0.0           0.0   \n",
      "\n",
      "                     close_Bin_97  close_Bin_98  close_Bin_99  close_target  \n",
      "2017-09-25 12:00:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:05:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:10:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:15:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:20:00           0.0           0.0           1.0          1.00  \n",
      "...                           ...           ...           ...           ...  \n",
      "2024-12-18 16:05:00           0.0           0.0           0.0          0.54  \n",
      "2024-12-18 16:10:00           0.0           0.0           0.0          0.50  \n",
      "2024-12-18 16:15:00           0.0           0.0           0.0          0.58  \n",
      "2024-12-18 16:20:00           0.0           0.0           0.0          0.64  \n",
      "2024-12-18 16:25:00           0.0           0.0           0.0          0.70  \n",
      "\n",
      "[754915 rows x 405 columns]\n",
      "Loaded model from experiment 13 for fine-tuning.\n",
      "Experiment 14: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.6804, Train Acc: 0.5801 | Val Loss: 0.6732, Val Acc: 0.6071\n",
      "Epoch 2/10 | Train Loss: 0.6804, Train Acc: 0.5801 | Val Loss: 0.6715, Val Acc: 0.6071\n",
      "Epoch 3/10 | Train Loss: 0.6805, Train Acc: 0.5801 | Val Loss: 0.6705, Val Acc: 0.6071\n",
      "Epoch 4/10 | Train Loss: 0.6804, Train Acc: 0.5801 | Val Loss: 0.6713, Val Acc: 0.6071\n",
      "Epoch 5/10 | Train Loss: 0.6804, Train Acc: 0.5801 | Val Loss: 0.6713, Val Acc: 0.6071\n",
      "Epoch 6/10 | Train Loss: 0.6804, Train Acc: 0.5801 | Val Loss: 0.6710, Val Acc: 0.6071\n",
      "Epoch 7/10 | Train Loss: 0.6804, Train Acc: 0.5801 | Val Loss: 0.6714, Val Acc: 0.6071\n",
      "Epoch 8/10 | Train Loss: 0.6804, Train Acc: 0.5801 | Val Loss: 0.6710, Val Acc: 0.6071\n",
      "Epoch 9/10 | Train Loss: 0.6804, Train Acc: 0.5801 | Val Loss: 0.6711, Val Acc: 0.6071\n",
      "Epoch 10/10 | Train Loss: 0.6804, Train Acc: 0.5801 | Val Loss: 0.6713, Val Acc: 0.6071\n",
      "Saved model for experiment 14.\n",
      "Experiment 14: Final Validation Accuracy: 0.6071\n",
      "Experiment 14: Test Accuracy: 0.6274\n",
      "Experiment 15: 데이터 구간 [375000:625000]\n",
      "                         open      high       low  close  open_Bin_0  \\\n",
      "2017-09-25 12:00:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:05:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:10:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:15:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "2017-09-25 12:20:00  1.000000  1.000000  1.000000   1.00         0.0   \n",
      "...                       ...       ...       ...    ...         ...   \n",
      "2024-12-18 16:05:00  0.200000  0.500000  0.462963   0.54         0.0   \n",
      "2024-12-18 16:10:00  0.490909  0.685185  0.648148   0.50         0.0   \n",
      "2024-12-18 16:15:00  0.472727  0.518519  0.759259   0.58         0.0   \n",
      "2024-12-18 16:20:00  0.527273  0.648148  0.814815   0.64         0.0   \n",
      "2024-12-18 16:25:00  0.581818  0.611111  0.870370   0.70         0.0   \n",
      "\n",
      "                     open_Bin_1  open_Bin_2  open_Bin_3  open_Bin_4  \\\n",
      "2017-09-25 12:00:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:05:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:10:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:15:00         0.0         0.0         0.0         0.0   \n",
      "2017-09-25 12:20:00         0.0         0.0         0.0         0.0   \n",
      "...                         ...         ...         ...         ...   \n",
      "2024-12-18 16:05:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:10:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:15:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:20:00         0.0         0.0         0.0         0.0   \n",
      "2024-12-18 16:25:00         0.0         0.0         0.0         0.0   \n",
      "\n",
      "                     open_Bin_5  ...  close_Bin_91  close_Bin_92  \\\n",
      "2017-09-25 12:00:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:05:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:10:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:15:00         0.0  ...           0.0           0.0   \n",
      "2017-09-25 12:20:00         0.0  ...           0.0           0.0   \n",
      "...                         ...  ...           ...           ...   \n",
      "2024-12-18 16:05:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:10:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:15:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:20:00         0.0  ...           0.0           0.0   \n",
      "2024-12-18 16:25:00         0.0  ...           0.0           0.0   \n",
      "\n",
      "                     close_Bin_93  close_Bin_94  close_Bin_95  close_Bin_96  \\\n",
      "2017-09-25 12:00:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:05:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:10:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:15:00           0.0           0.0           0.0           0.0   \n",
      "2017-09-25 12:20:00           0.0           0.0           0.0           0.0   \n",
      "...                           ...           ...           ...           ...   \n",
      "2024-12-18 16:05:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:10:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:15:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:20:00           0.0           0.0           0.0           0.0   \n",
      "2024-12-18 16:25:00           0.0           0.0           0.0           0.0   \n",
      "\n",
      "                     close_Bin_97  close_Bin_98  close_Bin_99  close_target  \n",
      "2017-09-25 12:00:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:05:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:10:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:15:00           0.0           0.0           1.0          1.00  \n",
      "2017-09-25 12:20:00           0.0           0.0           1.0          1.00  \n",
      "...                           ...           ...           ...           ...  \n",
      "2024-12-18 16:05:00           0.0           0.0           0.0          0.54  \n",
      "2024-12-18 16:10:00           0.0           0.0           0.0          0.50  \n",
      "2024-12-18 16:15:00           0.0           0.0           0.0          0.58  \n",
      "2024-12-18 16:20:00           0.0           0.0           0.0          0.64  \n",
      "2024-12-18 16:25:00           0.0           0.0           0.0          0.70  \n",
      "\n",
      "[754915 rows x 405 columns]\n",
      "Loaded model from experiment 14 for fine-tuning.\n",
      "Experiment 15: Training with lr=0.0001 (Fine-Tuning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\User\\anaconda3\\envs\\py39coin\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.6793, Train Acc: 0.5835 | Val Loss: 0.6639, Val Acc: 0.6274\n",
      "Epoch 2/10 | Train Loss: 0.6793, Train Acc: 0.5835 | Val Loss: 0.6644, Val Acc: 0.6274\n",
      "Epoch 3/10 | Train Loss: 0.6793, Train Acc: 0.5835 | Val Loss: 0.6648, Val Acc: 0.6274\n",
      "Epoch 4/10 | Train Loss: 0.6793, Train Acc: 0.5835 | Val Loss: 0.6648, Val Acc: 0.6274\n",
      "Epoch 5/10 | Train Loss: 0.6793, Train Acc: 0.5835 | Val Loss: 0.6639, Val Acc: 0.6274\n",
      "Epoch 6/10 | Train Loss: 0.6792, Train Acc: 0.5835 | Val Loss: 0.6635, Val Acc: 0.6274\n",
      "Epoch 7/10 | Train Loss: 0.6792, Train Acc: 0.5835 | Val Loss: 0.6630, Val Acc: 0.6274\n",
      "Epoch 8/10 | Train Loss: 0.6792, Train Acc: 0.5835 | Val Loss: 0.6637, Val Acc: 0.6274\n",
      "Epoch 9/10 | Train Loss: 0.6792, Train Acc: 0.5835 | Val Loss: 0.6629, Val Acc: 0.6274\n",
      "Epoch 10/10 | Train Loss: 0.6792, Train Acc: 0.5835 | Val Loss: 0.6633, Val Acc: 0.6274\n",
      "Saved model for experiment 15.\n",
      "Experiment 15: Final Validation Accuracy: 0.6274\n",
      "Experiment 15: Test Accuracy: 0.6484\n",
      "\n",
      "Final Average Validation Accuracy: 0.5844\n",
      "Final Average Test Accuracy: 0.5881\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from math import sqrt\n",
    "\n",
    "# rolling minmax scaling 함수 (window=24)\n",
    "def rolling_minmax_scale(series, window=24):\n",
    "    roll_min = series.rolling(window=window, min_periods=window).min()\n",
    "    roll_max = series.rolling(window=window, min_periods=window).max()\n",
    "    scaled = (series - roll_min) / ((roll_max - roll_min) + 1e-8)\n",
    "    scaled = scaled.replace([np.inf, -np.inf], np.nan)\n",
    "    scaled = scaled.fillna(1.0)\n",
    "    return scaled.clip(upper=1.0)\n",
    "\n",
    "# binning 및 one-hot 인코딩 함수 (결과를 정수 0,1로)\n",
    "def bin_and_encode(data, features, bins=100, drop_original=True):\n",
    "    for feature in features:\n",
    "        # pd.cut에 bins로 지정된 구간을 사용 (여기서는 자동으로 최소~최대 값)\n",
    "        data[f'{feature}_Bin'] = pd.cut(data[feature], bins=bins, labels=False)\n",
    "        one_hot = pd.get_dummies(data[f'{feature}_Bin'], prefix=f'{feature}_Bin').astype(np.int32)\n",
    "        expected_columns = [f'{feature}_Bin_{i}' for i in range(bins)]\n",
    "        one_hot = one_hot.reindex(columns=expected_columns, fill_value=0)\n",
    "        data = pd.concat([data, one_hot], axis=1)\n",
    "        if drop_original:\n",
    "            data.drop(columns=[f'{feature}_Bin'], inplace=True)\n",
    "    # 개별 열 단위로 float32로 변환\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    for col in numeric_cols:\n",
    "        data[col] = data[col].astype(np.float32)\n",
    "    return data\n",
    "\n",
    "####################################\n",
    "# Dataset 및 모델 정의 (나머지 구성은 동일)\n",
    "####################################\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, input_data, target_data, lookback=24):\n",
    "        self.input_data = input_data.values\n",
    "        self.target_data = target_data.values\n",
    "        self.lookback = lookback\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_data) - self.lookback\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.input_data[idx: idx + self.lookback, :]\n",
    "        y = self.target_data[idx + self.lookback, 0]\n",
    "        y_prev = self.target_data[idx + self.lookback - 1, 0]\n",
    "        y_target = 1 if y > y_prev else 0\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y_target, dtype=torch.long)\n",
    "\n",
    "class EncoderOnlyTransformerCustom(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=512, num_layers=6, nhead=8, ffn_dim=2048, num_classes=2, max_seq_len=24):\n",
    "        super(EncoderOnlyTransformerCustom, self).__init__()\n",
    "        self.token_embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embedding_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=nhead, dim_feedforward=ffn_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x = self.token_embedding(x)\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        pos_emb = self.position_embedding(positions)\n",
    "        x = x + pos_emb\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.transformer_encoder(x)\n",
    "        return self.fc(x[-1, :, :])\n",
    "\n",
    "####################################\n",
    "# 데이터 로드 및 Variant 1 실행: OHLC 원본 데이터로 실험\n",
    "####################################\n",
    "# CSV 파일 로드 – OHLC 4개 데이터만 사용\n",
    "data = pd.read_csv(\"ETH_upbit_KRW_min5.csv\", index_col=0)\n",
    "data = data[['open', 'high', 'low', 'close']]\n",
    "data.index = pd.to_datetime(data.index)\n",
    "\n",
    "# 각 OHLC 열에 대해 rolling minmax scaling (window=24)\n",
    "ohlc_features = ['open', 'high', 'low', 'close']\n",
    "for feature in ohlc_features:\n",
    "    data[feature] = rolling_minmax_scale(data[feature], window=24)\n",
    "\n",
    "# One-hot 인코딩: 각 OHLC 열은 100구간으로 나눔\n",
    "data = bin_and_encode(data, ohlc_features, bins=100, drop_original=True)\n",
    "\n",
    "# 타깃은 원본 close 값 사용 (실험 목적)\n",
    "data['close_target'] = data['close']\n",
    "data = data.dropna()\n",
    "\n",
    "# 최종 입력은 _Bin_ 접미사 열들만 사용\n",
    "final_input_columns = [col for col in data.columns if '_Bin_' in col]\n",
    "final_target_column = ['close_target']\n",
    "\n",
    "data_input = data[final_input_columns]\n",
    "data_target = data[final_target_column]\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return total_loss / len(data_loader), correct / total\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, lr, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_state = None\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, device)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = model.state_dict()\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(data, num_experiments=16, lookback=24, num_epochs=10):\n",
    "    final_input_columns = [col for col in data.columns if '_Bin_' in col]\n",
    "    target_cols = ['close_target']\n",
    "    \n",
    "    data_input = data[final_input_columns]\n",
    "    data_target = data[target_cols]\n",
    "    \n",
    "    data_input = data_input.apply(pd.to_numeric).astype(np.float32)\n",
    "    data_target = data_target.apply(pd.to_numeric).astype(np.float32)\n",
    "    \n",
    "    step_size = 25000\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    val_acc_list = []\n",
    "    test_acc_list = []\n",
    "    \n",
    "    for exp in range(num_experiments):\n",
    "        train_start = exp * step_size\n",
    "        train_end = train_start + step_size * 8\n",
    "        val_end = train_end + step_size\n",
    "        test_end = val_end + step_size\n",
    "        if test_end > len(data_input):\n",
    "            break\n",
    "        print(f\"Experiment {exp}: 데이터 구간 [{train_start}:{test_end}]\")\n",
    "        print(data)\n",
    "        \n",
    "        train_input = data_input.iloc[train_start:train_end]\n",
    "        train_target = data_target.iloc[train_start:train_end]\n",
    "        val_input = data_input.iloc[train_end:val_end]\n",
    "        val_target = data_target.iloc[train_end:val_end]\n",
    "        test_input = data_input.iloc[val_end:test_end]\n",
    "        test_target = data_target.iloc[val_end:test_end]\n",
    "        \n",
    "        train_dataset = TimeSeriesDataset(train_input, train_target, lookback=lookback)\n",
    "        val_dataset = TimeSeriesDataset(val_input, val_target, lookback=lookback)\n",
    "        test_dataset = TimeSeriesDataset(test_input, test_target, lookback=lookback)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        lr = 1e-4\n",
    "        input_dim = data_input.shape[1]\n",
    "        model = EncoderOnlyTransformerCustom(input_dim=input_dim, embedding_dim=512, num_layers=6, nhead=8,\n",
    "                                               ffn_dim=2048, num_classes=2, max_seq_len=lookback).to(device)\n",
    "        model_path = f\"model_experiment_{exp}.pth\"\n",
    "        if exp > 0:\n",
    "            try:\n",
    "                model.load_state_dict(torch.load(f\"model_experiment_{exp - 1}.pth\"))\n",
    "                print(f\"Loaded model from experiment {exp - 1} for fine-tuning.\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Model file for experiment {exp - 1} not found. Starting fresh training.\")\n",
    "        \n",
    "        print(f\"Experiment {exp}: Training with lr={lr} (Fine-Tuning)\")\n",
    "        model = train_model(model, train_loader, val_loader, num_epochs, lr, device)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Saved model for experiment {exp}.\")\n",
    "        \n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, device)\n",
    "        val_acc_list.append(val_acc)\n",
    "        print(f\"Experiment {exp}: Final Validation Accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "        test_loss, test_acc = evaluate_model(model, test_loader, device)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(f\"Experiment {exp}: Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    if val_acc_list:\n",
    "        avg_val_acc = sum(val_acc_list) / len(val_acc_list)\n",
    "        avg_test_acc = sum(test_acc_list) / len(test_acc_list)\n",
    "        print(f\"\\nFinal Average Validation Accuracy: {avg_val_acc:.4f}\")\n",
    "        print(f\"Final Average Test Accuracy: {avg_test_acc:.4f}\")\n",
    "    else:\n",
    "        print(\"실험이 한 번도 실행되지 않았습니다.\")\n",
    "\n",
    "\n",
    "train_and_evaluate(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from math import sqrt\n",
    "\n",
    "# rolling minmax scaling 함수 (window=24)\n",
    "def rolling_minmax_scale(series, window=24):\n",
    "    roll_min = series.rolling(window=window, min_periods=window).min()\n",
    "    roll_max = series.rolling(window=window, min_periods=window).max()\n",
    "    scaled = (series - roll_min) / ((roll_max - roll_min) + 1e-8)\n",
    "    scaled = scaled.replace([np.inf, -np.inf], np.nan)\n",
    "    scaled = scaled.fillna(1.0)\n",
    "    return scaled.clip(upper=1.0)\n",
    "\n",
    "# binning 및 one-hot 인코딩 함수 (결과를 정수 0,1로)\n",
    "def bin_and_encode(data, features, bins=100, drop_original=True):\n",
    "    for feature in features:\n",
    "        # pd.cut에 bins로 지정된 구간을 사용 (여기서는 자동으로 최소~최대 값)\n",
    "        data[f'{feature}_Bin'] = pd.cut(data[feature], bins=bins, labels=False)\n",
    "        one_hot = pd.get_dummies(data[f'{feature}_Bin'], prefix=f'{feature}_Bin').astype(np.int32)\n",
    "        expected_columns = [f'{feature}_Bin_{i}' for i in range(bins)]\n",
    "        one_hot = one_hot.reindex(columns=expected_columns, fill_value=0)\n",
    "        data = pd.concat([data, one_hot], axis=1)\n",
    "        if drop_original:\n",
    "            data.drop(columns=[f'{feature}_Bin'], inplace=True)\n",
    "    # 개별 열 단위로 float32로 변환\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    for col in numeric_cols:\n",
    "        data[col] = data[col].astype(np.float32)\n",
    "    return data\n",
    "\n",
    "####################################\n",
    "# Dataset 및 모델 정의 (나머지 구성은 동일)\n",
    "####################################\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, input_data, target_data, lookback=24):\n",
    "        self.input_data = input_data.values\n",
    "        self.target_data = target_data.values\n",
    "        self.lookback = lookback\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_data) - self.lookback\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.input_data[idx: idx + self.lookback, :]\n",
    "        y = self.target_data[idx + self.lookback, 0]\n",
    "        y_prev = self.target_data[idx + self.lookback - 1, 0]\n",
    "        y_target = 1 if y > y_prev else 0\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y_target, dtype=torch.long)\n",
    "\n",
    "class EncoderOnlyTransformerCustom(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=512, num_layers=6, nhead=8, ffn_dim=2048, num_classes=2, max_seq_len=24):\n",
    "        super(EncoderOnlyTransformerCustom, self).__init__()\n",
    "        self.token_embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embedding_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=nhead, dim_feedforward=ffn_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x = self.token_embedding(x)\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        pos_emb = self.position_embedding(positions)\n",
    "        x = x + pos_emb\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.transformer_encoder(x)\n",
    "        return self.fc(x[-1, :, :])\n",
    "\n",
    "####################################\n",
    "# 데이터 로드 및 Variant 1 실행: OHLC 원본 데이터로 실험\n",
    "####################################\n",
    "# CSV 파일 로드 – OHLC 4개 데이터만 사용\n",
    "data = pd.read_csv(\"ETH_upbit_KRW_min5_0309.csv\", index_col=0)\n",
    "data = data[['open', 'high', 'low', 'close']]\n",
    "data.index = pd.to_datetime(data.index)\n",
    "\n",
    "# 각 OHLC 열에 대해 rolling minmax scaling (window=24)\n",
    "ohlc_features = ['open', 'high', 'low', 'close']\n",
    "for feature in ohlc_features:\n",
    "    data[feature] = rolling_minmax_scale(data[feature], window=24)\n",
    "\n",
    "# One-hot 인코딩: 각 OHLC 열은 100구간으로 나눔\n",
    "data = bin_and_encode(data, ohlc_features, bins=100, drop_original=True)\n",
    "\n",
    "# 타깃은 원본 close 값 사용 (실험 목적)\n",
    "data['close_target'] = data['close']\n",
    "data = data.dropna()\n",
    "\n",
    "# 최종 입력은 _Bin_ 접미사 열들만 사용\n",
    "final_input_columns = [col for col in data.columns if '_Bin_' in col]\n",
    "final_target_column = ['close_target']\n",
    "\n",
    "data_input = data[final_input_columns]\n",
    "data_target = data[final_target_column]\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return total_loss / len(data_loader), correct / total\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, lr, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_state = None\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, device)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = model.state_dict()\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(data, num_experiments=16, lookback=24, num_epochs=10):\n",
    "    final_input_columns = [col for col in data.columns if '_Bin_' in col]\n",
    "    target_cols = ['close_target']\n",
    "    \n",
    "    data_input = data[final_input_columns]\n",
    "    data_target = data[target_cols]\n",
    "    \n",
    "    data_input = data_input.apply(pd.to_numeric).astype(np.float32)\n",
    "    data_target = data_target.apply(pd.to_numeric).astype(np.float32)\n",
    "    \n",
    "    step_size = 31200\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    val_acc_list = []\n",
    "    test_acc_list = []\n",
    "    \n",
    "    for exp in range(num_experiments):\n",
    "        train_start = exp * step_size\n",
    "        train_end = train_start + step_size * 8\n",
    "        val_end = train_end + step_size\n",
    "        test_end = val_end + step_size\n",
    "        if test_end > len(data_input):\n",
    "            break\n",
    "        print(f\"Experiment {exp}: 데이터 구간 [{train_start}:{test_end}]\")\n",
    "        print(data)\n",
    "        \n",
    "        train_input = data_input.iloc[train_start:train_end]\n",
    "        train_target = data_target.iloc[train_start:train_end]\n",
    "        val_input = data_input.iloc[train_end:val_end]\n",
    "        val_target = data_target.iloc[train_end:val_end]\n",
    "        test_input = data_input.iloc[val_end:test_end]\n",
    "        test_target = data_target.iloc[val_end:test_end]\n",
    "        \n",
    "        train_dataset = TimeSeriesDataset(train_input, train_target, lookback=lookback)\n",
    "        val_dataset = TimeSeriesDataset(val_input, val_target, lookback=lookback)\n",
    "        test_dataset = TimeSeriesDataset(test_input, test_target, lookback=lookback)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        lr = 1e-4\n",
    "        input_dim = data_input.shape[1]\n",
    "        model = EncoderOnlyTransformerCustom(input_dim=input_dim, embedding_dim=512, num_layers=6, nhead=8,\n",
    "                                               ffn_dim=2048, num_classes=2, max_seq_len=lookback).to(device)\n",
    "        model_path = f\"model_experiment_{exp}.pth\"\n",
    "        if exp > 0:\n",
    "            try:\n",
    "                model.load_state_dict(torch.load(f\"model_experiment_{exp - 1}.pth\"))\n",
    "                print(f\"Loaded model from experiment {exp - 1} for fine-tuning.\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Model file for experiment {exp - 1} not found. Starting fresh training.\")\n",
    "        \n",
    "        print(f\"Experiment {exp}: Training with lr={lr} (Fine-Tuning)\")\n",
    "        model = train_model(model, train_loader, val_loader, num_epochs, lr, device)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Saved model for experiment {exp}.\")\n",
    "        \n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, device)\n",
    "        val_acc_list.append(val_acc)\n",
    "        print(f\"Experiment {exp}: Final Validation Accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "        test_loss, test_acc = evaluate_model(model, test_loader, device)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(f\"Experiment {exp}: Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    if val_acc_list:\n",
    "        avg_val_acc = sum(val_acc_list) / len(val_acc_list)\n",
    "        avg_test_acc = sum(test_acc_list) / len(test_acc_list)\n",
    "        print(f\"\\nFinal Average Validation Accuracy: {avg_val_acc:.4f}\")\n",
    "        print(f\"Final Average Test Accuracy: {avg_test_acc:.4f}\")\n",
    "    else:\n",
    "        print(\"실험이 한 번도 실행되지 않았습니다.\")\n",
    "\n",
    "\n",
    "train_and_evaluate(data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39coin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
